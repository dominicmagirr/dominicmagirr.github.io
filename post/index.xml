<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Dominic Magirr</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 15 Oct 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Trouble with tau</title>
      <link>/post/trouble-with-tau/</link>
      <pubDate>Fri, 15 Oct 2021 00:00:00 +0000</pubDate>
      <guid>/post/trouble-with-tau/</guid>
      <description>


&lt;p&gt;This post is to express some minor frustration with some papers I’ve read recently evaluating the performance of restricted mean survival time as a summary measure in oncology studies.&lt;/p&gt;
&lt;p&gt;I should say that I’m not a saint when it comes to designing simulation studies. Consciously and/or unconsciously, it’s tempting to give our favourite methods an easier ride.&lt;/p&gt;
&lt;p&gt;Nevertheless, a couple of things bother me, and they’re related to each other. One is the choice of censoring distribution, and the other is the choice of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; in the definition of restricted mean survival time (I’ll explain).&lt;/p&gt;
&lt;div id=&#34;restricted-mean-survival-time&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Restricted mean survival time&lt;/h2&gt;
&lt;p&gt;The restricted mean survival time up until month &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; is the average amount of time patients are alive for out of the first &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; months:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{RMST}(\tau):=E\left\lbrace \text{min}(\tau, T)\right\rbrace\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It’s sometimes defined as the area under the survival curve &lt;span class=&#34;math inline&#34;&gt;\(\text{RMST}(\tau):=\int_{0}^{\tau}S(t)dt\)&lt;/span&gt;. Is it obvious that that’s the same thing? Maybe it should be, but not to me right now…&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
 E\left\lbrace \text{min}(\tau, T)\right\rbrace &amp;amp; = \int_{0}^{\tau}tf(t)dt + \tau\int_{\tau}^{\infty}f(t)dt\\
   &amp;amp; = \left[ tF(t)  \right]_0^{\tau} - \int_{0}^{\tau}F(t)dt +\tau S(\tau)\\
   &amp;amp;= \tau\left\lbrace 1 - S(\tau)  \right\rbrace - \int_{0}^{\tau}\left\lbrace 1 - S(t)\right\rbrace dt +\tau S(\tau) \\
   &amp;amp;= \int_{0}^{\tau}S(t)dt
\end{aligned}
\]&lt;/span&gt;
As a summary measure for treatment effect, one could use a difference &lt;span class=&#34;math inline&#34;&gt;\(\text{RMST}_E(\tau)-\text{RMST}_C(\tau)\)&lt;/span&gt;, or a ratio &lt;span class=&#34;math inline&#34;&gt;\(\text{RMST}_E(\tau)/\text{RMST}_C(\tau)\)&lt;/span&gt; contrasting the experimental and control arms. One could perform estimation by plugging in the Kaplan Meier estimates, &lt;span class=&#34;math inline&#34;&gt;\(\hat{S}_E(t)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{S}_C(t)\)&lt;/span&gt;, for example.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-choose-tau&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to choose &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;?&lt;/h2&gt;
&lt;p&gt;A very basic consideration is that everything later than &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; will be ignored. This suggests that taking &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; as large as possible might generally (not always) be the most powerful strategy.&lt;/p&gt;
&lt;div id=&#34;censoring-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Censoring distributions&lt;/h3&gt;
&lt;p&gt;This brings me on to censoring distributions, and comparisons with the log-rank test / Cox model. With the log-rank test, every event contributes to the test statistic. So whatever the choice of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;, if there are many events after &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;, then this often (not always!) means that RMST will tend to lose power compared to log-rank / Cox. So if you’re conducting a simulation study and you would like to show the benefits of RMST, what’s a good censoring distribution?&lt;/p&gt;
&lt;p&gt;Well, something that looks like this…&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/censoring_dist.PNG&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I see this again and again. What this graph is showing is that there is some reasonably high (e.g. 30%) number of patients who’s censoring time is equal to the length of the study. In other words these 30% of patients are recruited immediately upon the study starting. In this situation there’s no problem with pushing &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; right up to the length of the study, and RMST would use all of the data in the same way as log-rank / Cox.&lt;/p&gt;
&lt;p&gt;The problem of course is that recruitment never looks like this! If a more realistic censoring distribution were used where recruitment starts slowly and increases gradually, then this would create a problem for pushing &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; right up to the length of the study, because things would become unstable when only a few patients are at risk. One would be forced to bring &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; earlier, and one would see power loss (under certain situations) compared to the log-rank test.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparisons-based-on-real-trial-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparisons based on real trial data&lt;/h3&gt;
&lt;p&gt;Simulation studies can be manipulated to favour one method or another. Why not take a large selection of real studies and see which method performed best? Yes, an excellent idea from &lt;a href=&#34;https://journals.sagepub.com/doi/full/10.1177/1740774520940256&#34;&gt;Horiguchi and colleagues&lt;/a&gt;. And to be fair, not only an idea, they’ve actually gone to the trouble of doing it. I still don’t think the comparison is totally fair though. They define &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; as “the minimum of the maximum observed times from two groups”. To see what this really means, I took a look at Kaplan-Meier curves from their first study (alphabetically), and drew (very badly) a line where &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; is. I don’t think anyone could report a difference in RMST at this &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; and keep a straight face. It’s clearly ridiculous. Regardless of &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.13237&#34;&gt;whether or not the asymptotics hold well enough to give a valid test&lt;/a&gt; (they probably don’t), you’d simply have to bring &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; forward.&lt;/p&gt;
&lt;p&gt;It won’t look this silly for every study of course, but that’s difficult to predict. As a general strategy, I think one would need to be more conservative.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/first_km.PNG&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From &lt;em&gt;Agarwala, Sanjiv S., et al. Journal of Clinical Oncology 35.8 (2017): 885.&lt;/em&gt; (&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5455684/&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5455684/&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;I don’t want to dismiss restricted mean survival time as a useful summary measure. In some situations I’m sure it is a good idea. In some situations a log-rank test / Cox model is a bad idea. All I’m saying is that power comparisons should be a bit fairer, or at least, we should look at the details more closely. Like I said, I’m not immune to this kind of bias.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A stratified log-rank test does not control alpha for a marginal null hypothesis and that&#39;s ok</title>
      <link>/post/a-stratified-log-rank-test-does-not-control-alpha-for-a-marginal-null-hypothesis-and-that-s-ok/</link>
      <pubDate>Thu, 07 Oct 2021 00:00:00 +0000</pubDate>
      <guid>/post/a-stratified-log-rank-test-does-not-control-alpha-for-a-marginal-null-hypothesis-and-that-s-ok/</guid>
      <description>


&lt;p&gt;Recently, I’ve become interested in the robustness of standard statistical methods for RCTs under model misspecification. It seems to be a hot topic at the moment. I’d like to believe this is not such a big problem. I don’t think it is a big problem. But I’m trying to challenge my beliefs nevertheless.&lt;/p&gt;
&lt;div id=&#34;stratified-log-rank-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stratified log-rank test&lt;/h2&gt;
&lt;p&gt;The null hypothesis that is generally considered when performing an unstratified log-rank test is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_{0}: S_{E}(t) = S_{C}(t)\text{ for all }t&amp;gt;0\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(S_{E}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_{C}\)&lt;/span&gt; denote the survival distributions on the experimental and control arms, respectively, in the full trial population. Personally, I prefer one-sided testing and a null hypothesis&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\tilde{H}_{0}: S_{E}(t) \leq S_{C}(t)\text{ for all }t&amp;gt;0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;but this is unconventional (strictly speaking, it’s harder to reject &lt;span class=&#34;math inline&#34;&gt;\(\tilde{H}_{0}\)&lt;/span&gt; than &lt;span class=&#34;math inline&#34;&gt;\(H_{0}\)&lt;/span&gt;, but some might argue that &lt;span class=&#34;math inline&#34;&gt;\(\tilde{H}_{0}\)&lt;/span&gt; is still too easy to reject and we need a bigger null hypothesis space – a subject for another post). In any case, under some standard assumptions, an unstratified log-rank test controls &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; when testing &lt;span class=&#34;math inline&#34;&gt;\(H_{0}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\tilde{H}_{0}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The null hypothesis that is generally considered when performing a stratified log-rank test is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_{0,S}: S_{E,i}(t) = S_{C,i}(t)\text{ for all }t&amp;gt;0\text{ and for all strata }i\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(S_{E,i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_{C,i}\)&lt;/span&gt; denote the survival distributions on the experimental and control arms, respectively, in the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th stratum. Again, I would prefer a one-sided version&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\tilde{H}_{0,S}: S_{E,i}(t) \leq S_{C,i}(t)\text{ for all }t&amp;gt;0\text{ and for all strata }i.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A stratified log-rank test controls &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; when testing &lt;span class=&#34;math inline&#34;&gt;\(H_{0,S}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\tilde{H}_{0,S}\)&lt;/span&gt;, but a question some people might ask is: does a stratified log-rank test control &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; when thought of as a test of &lt;span class=&#34;math inline&#34;&gt;\(H_{0}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\tilde{H}_{0}\)&lt;/span&gt;? (I’m not entirely convinced this is a sensible question, but let’s leave that aside).&lt;/p&gt;
&lt;p&gt;The answer to the question is no. Below is a (rather extreme) example where the population consists of two equally sized strata. Under the control treatment, the first stratum has much poorer survival than the second stratum. The experimental treatment is beneficial in the first stratum and harmful in the second stratum. However, overall, marginally, the survival distributions on the two arms are practically the same (experimental very fractionally worse).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../post/2021-10-07-a-stratified-log-rank-test-does-not-control-alpha-for-a-marginal-null-hypothesis-and-that-s-ok_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now consider a trial in this population, where 1000 patients are recruited at a uniform rate for 12 months, and patients are subsequently followed-up for a further 12 months, making the total trial length 24 months, at which point all the survival times for all surviving patients are censored.&lt;/p&gt;
&lt;p&gt;I simulated this trial 10,000 times, and the percentage of times that the stratified log-rank test (actually I used a stratified Cox model with treatment as the only covariate – all code below) produced a Z-value for the treatment term that was below the 2.5% quantile of the normal distribution (low values of Z favouring the experimental arm) was 38%. The percentage of times for the unstratified Cox model was 1%.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-this-a-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Is this a problem?&lt;/h2&gt;
&lt;p&gt;No I don’t think so. Firstly, there is no problem for the null hypotheses &lt;span class=&#34;math inline&#34;&gt;\(H_{0,S}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\tilde{H}_{0,S}\)&lt;/span&gt;. Secondly, this is a very extreme violation of the assumptions of the stratified Cox model. Thirdly, consider the point estimates for the hazard ratios in the two strata in the subset of simulated data sets where the stratified log-rank test rejects and the unstratified log-rank test does not. These are highlighted in blue in the figure below. In these cases we would observe a point estimate between 0.5 and 0.7 in the first stratum, and a point estimate between 1.1 and 1.6 in the second stratum.&lt;/p&gt;
&lt;p&gt;So I see this as more of a curiosity than a practical problem. It’s interesting to compare with logistic regression, where it’s been shown (&lt;a href=&#34;https://biostats.bepress.com/jhubiostat/paper281/&#34;&gt;here&lt;/a&gt;) that a stratified test controls &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; not only for the stratified null but also for the overall trial population null (see also &lt;a href=&#34;https://arxiv.org/pdf/2107.07278.pdf&#34;&gt;here&lt;/a&gt;). The same goes for continuous data analyzed by ANCOVA which I think is quite well known.&lt;/p&gt;
&lt;p&gt;As a final comment, it’s important to keep in mind the efficiency gains from using a stratified test in more typical situations where its assumptions are reasonable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(survival)
library(tidyverse)
## survival probability for a 2-piece exponential
surv_2 &amp;lt;- function(t, 
                   lambda_1, 
                   lambda_2, 
                   t_star){
  
  p_1 &amp;lt;- exp(-lambda_1 * t)
  p_2 &amp;lt;- exp(-lambda_1 * t_star) * exp(-lambda_2 * pmax(0, (t - t_star)))
  
  (t &amp;lt; t_star) * p_1 + (t &amp;gt;= t_star) * p_2
  
}

## function to plot the three survival distributions
plot_strat &amp;lt;- function(t_end = 24, 
                       t_star_1 = 9,
                       t_star_0 = 9,
                       strat_1_rate_c_1 = log(2) / 4,
                       strat_1_rate_c_2 = log(2) / 4,
                       strat_1_rate_e_1 = log(2) / 4 * 0.6,
                       strat_1_rate_e_2 = log(2) / 4 * 0.6,
                       strat_0_rate_c_1 = log(2) / 15,
                       strat_0_rate_c_2 = log(2) / 15,
                       strat_0_rate_e_1 = log(2) / 15 * 1.1,
                       strat_0_rate_e_2 = log(2) / 15 * 1,
                       prop_strat_1 = 0.1){
  
  t_seq &amp;lt;- seq(0, t_end, length.out = 100)
  
  s_c_1 &amp;lt;- surv_2(t_seq, lambda_1 = strat_1_rate_c_1, lambda_2 = strat_1_rate_c_2, t_star = t_star_1)
  s_e_1 &amp;lt;- surv_2(t_seq, lambda_1 = strat_1_rate_e_1, lambda_2 = strat_1_rate_e_2, t_star = t_star_1)
  s_c_0 &amp;lt;- surv_2(t_seq, lambda_1 = strat_0_rate_c_1, lambda_2 = strat_0_rate_c_2, t_star = t_star_0)
  s_e_0 &amp;lt;- surv_2(t_seq, lambda_1 = strat_0_rate_e_1, lambda_2 = strat_0_rate_e_2, t_star = t_star_0)
  
  par(mfrow = c(1,3))
  
  plot(t_seq, s_c_1, type = &amp;quot;l&amp;quot;, ylim = c(0,1), xlab = &amp;quot;time&amp;quot;, ylab = &amp;quot;Surv&amp;quot;, 
       main = paste(&amp;quot;First stratum (&amp;quot;, prop_strat_1,&amp;quot;)&amp;quot;))
  points(t_seq, s_e_1, type = &amp;quot;l&amp;quot;, col = 2)
  legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Control&amp;quot;, &amp;quot;Experimental&amp;quot;), lty = c(1,1), col = c(1,2))
  
  plot(t_seq, s_c_0, type = &amp;quot;l&amp;quot;,  ylim = c(0,1), col = 1, lty = 1, xlab = &amp;quot;time&amp;quot;, ylab = &amp;quot;Surv&amp;quot;, 
       main = paste(&amp;quot;Second stratum (&amp;quot;, 1 - prop_strat_1,&amp;quot;)&amp;quot;))
  points(t_seq, s_e_0, type = &amp;quot;l&amp;quot;, col = 2, lty = 1)
  legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Control&amp;quot;, &amp;quot;Experimental&amp;quot;), lty = c(1,1), col = c(1,2))
  
  plot(t_seq, prop_strat_1 * s_c_1 + (1 - prop_strat_1) * s_c_0, type = &amp;quot;l&amp;quot;, ylim = c(0,1), xlab = &amp;quot;time&amp;quot;, ylab = &amp;quot;Surv&amp;quot;, main = &amp;quot;Overall&amp;quot;)
  points(t_seq, prop_strat_1 * s_e_1 + (1 - prop_strat_1) * s_e_0, type = &amp;quot;l&amp;quot;, col = 2)
  legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Control&amp;quot;, &amp;quot;Experimental&amp;quot;), lty = c(1,1), col = c(1,2))
}

## plot the three survival distributions
plot_strat(t_end = 24, 
           t_star_1 = 9,
           t_star_0 = 6,
           strat_1_rate_c_1 = log(2) / 4,
           strat_1_rate_c_2 = log(2) / 4,
           strat_1_rate_e_1 = log(2) / 4 * 0.6,
           strat_1_rate_e_2 = log(2) / 4 * 0.6,
           strat_0_rate_c_1 = log(2) / 15,
           strat_0_rate_c_2 = log(2) / 15,
           strat_0_rate_e_1 = log(2) / 15 * 2.3,
           strat_0_rate_e_2 = log(2) / 15 * 0.9,
           prop_strat_1 = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2021-10-07-a-stratified-log-rank-test-does-not-control-alpha-for-a-marginal-null-hypothesis-and-that-s-ok_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## function for simulating from a piece-wise exponential distribution
t_piecewise_exp &amp;lt;- function(n = 10, 
                            change_points = c(6, 12),
                            lambdas = c(log(2) / 9, log(2) / 9, log(2) / 9)){
  
  t_lim &amp;lt;- matrix(rep(c(diff(c(0, change_points)), Inf), each = n), nrow = n)
  t_sep &amp;lt;- do.call(cbind, purrr::map(lambdas, rexp, n = n))
  which_cells &amp;lt;- t(apply(t_sep &amp;lt; t_lim, 1, function(x){
    rep(c(T,F), c(min(which(x)), length(x) - min(which(x))))
  } ))
  rowSums(pmin(t_sep, t_lim) * which_cells)
}

## function to simulate trial data (uncensored)
sim_t_uncensored &amp;lt;- function(model,
                             recruitment){
  
  rec_0 &amp;lt;- recruitment$r_period * runif(recruitment$n_0) ^ (1 / recruitment$k)
  rec_1 &amp;lt;- recruitment$r_period * runif(recruitment$n_1) ^ (1 / recruitment$k)
  
  time_0 &amp;lt;- t_piecewise_exp(recruitment$n_0, model$change_points, model$lambdas_0)
  time_1 &amp;lt;- t_piecewise_exp(recruitment$n_1, model$change_points, model$lambdas_1)
  
  data.frame(time = c(time_0, time_1),
             rec = c(rec_0, rec_1),
             group = rep(c(&amp;quot;control&amp;quot;, &amp;quot;experimental&amp;quot;), c(recruitment$n_0, recruitment$n_1)))
  
}

## function to apply a data cut off to uncensored data set
apply_dco &amp;lt;- function(df,
                      dco = NULL,
                      events = NULL){
  
  if (is.null(dco) &amp;amp;&amp;amp; is.null(events)) stop(&amp;quot;Must specify either dco or events&amp;quot;)
  
  df$cal_time &amp;lt;- df$time + df$rec
  
  if (is.null(dco)){
    dco &amp;lt;- sort(df$cal_time)[events]
  }
  
  df_dco &amp;lt;- df[df$rec &amp;lt; dco, ]
  df_dco$event &amp;lt;-  df_dco$cal_time &amp;lt;= dco
  df_dco$time &amp;lt;- pmin(df_dco$time, dco - df_dco$rec)
  df_dco$dco &amp;lt;- dco
  
  df_dco
  
}

## function to simulate and analyze a single trial
sim_1_trial &amp;lt;- function(dummy, 
                        model_1, ## strata 1
                        model_0, ## strata 0
                        recruitment_1,
                        recruitment_0){
  
  
  df_uncensored_1 &amp;lt;- sim_t_uncensored(model_1, recruitment_1)
  df_uncensored_0 &amp;lt;- sim_t_uncensored(model_0, recruitment_0)
  
  df_final_1 &amp;lt;- apply_dco(df_uncensored_1, dco = 24)
  df_final_0 &amp;lt;- apply_dco(df_uncensored_0, dco = 24)
  
  df_final_1$cov &amp;lt;- &amp;quot;1&amp;quot;
  df_final_0$cov &amp;lt;- &amp;quot;0&amp;quot;
  
  df_final &amp;lt;- rbind(df_final_1, df_final_0)
  
  cox_str &amp;lt;-   summary(coxph(Surv(time, event) ~ group + strata(cov), data = df_final))
  cox_unstr &amp;lt;- summary(coxph(Surv(time, event) ~ group, data = df_final))
  
  cox_0 &amp;lt;- summary(coxph(Surv(time, event) ~ group, data = df_final %&amp;gt;% filter(cov == &amp;quot;0&amp;quot;)))
  cox_1 &amp;lt;- summary(coxph(Surv(time, event) ~ group, data = df_final %&amp;gt;% filter(cov == &amp;quot;1&amp;quot;)))

  
  data.frame(z_str = cox_str$coefficients[1,&amp;quot;z&amp;quot;],
             z_unstr = cox_unstr$coefficients[1,&amp;quot;z&amp;quot;],
             hr_1 = cox_1$coefficients[1,&amp;quot;exp(coef)&amp;quot;],
             hr_0 = cox_0$coefficients[1,&amp;quot;exp(coef)&amp;quot;])
  
  
}


## specify recruitment
recruitment_1 = list(n_0 = 250, n_1 = 250, r_period = 12, k = 1)
recruitment_0 = list(n_0 = 250, n_1 = 250, r_period = 12, k = 1)

## specify models
model_1 = list(change_points = c(9),
               lambdas_0 = c(log(2) / 4, log(2) / 4),
               lambdas_1 = c(log(2) / 4 * 0.6, log(2) / 4 * 0.6))


model_0 = list(change_points = c(6),
               lambdas_0 = c(log(2) / 15, log(2) / 15),
               lambdas_1 = c(log(2) / 15 * 2.3, log(2) / 15 * 0.9))

set.seed(352)
res &amp;lt;- purrr::map_df(1:10000, 
                     sim_1_trial, 
                     model_1 = model_1, 
                     model_0 = model_0, 
                     recruitment_1 = recruitment_1, 
                     recruitment_0 = recruitment_0)

## rejections from stratified test
mean(res$z_str &amp;lt; qnorm(0.025))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3774&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## rejections from unstratified test
mean(res$z_unstr &amp;lt; qnorm(0.025))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0111&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## plot hazard ratios by strata
res &amp;lt;- res %&amp;gt;% mutate(only_stratified_rejects = z_str &amp;lt; qnorm(0.025) &amp;amp; z_unstr &amp;gt; qnorm(0.025))
ggplot(res, aes(x = hr_0, y = hr_1, colour = only_stratified_rejects)) + 
  geom_point() +
  xlab(&amp;quot;estimated HR second strata&amp;quot;) + 
  ylab(&amp;quot;estimated HR first strata&amp;quot;) +
  scale_x_continuous(limits = c(0.9, 2.2)) +
  scale_y_continuous(limits = c(0.4, 0.8)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2021-10-07-a-stratified-log-rank-test-does-not-control-alpha-for-a-marginal-null-hypothesis-and-that-s-ok_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multiple imputation without a specialist R package</title>
      <link>/post/multiple-imputation-without-a-specialist-r-package/</link>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
      <guid>/post/multiple-imputation-without-a-specialist-r-package/</guid>
      <description>


&lt;p&gt;If you’re interested in doing multiple imputation in R, it’s best to use a specialist package. There are many good options out there, including &lt;code&gt;mice&lt;/code&gt; (&lt;a href=&#34;https://www.jstatsoft.org/article/view/v045i03&#34; class=&#34;uri&#34;&gt;https://www.jstatsoft.org/article/view/v045i03&lt;/a&gt;), &lt;code&gt;smcfcs&lt;/code&gt; (&lt;a href=&#34;https://cran.r-project.org/web/packages/smcfcs/vignettes/smcfcs-vignette.html&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/smcfcs/vignettes/smcfcs-vignette.html&lt;/a&gt;), &lt;code&gt;norm2&lt;/code&gt; (&lt;a href=&#34;https://usermanual.wiki/Document/norm2UserGuide.911613350/view&#34; class=&#34;uri&#34;&gt;https://usermanual.wiki/Document/norm2UserGuide.911613350/view&lt;/a&gt;), and many others (&lt;a href=&#34;https://cran.r-project.org/web/views/MissingData.html&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/views/MissingData.html&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The aim of this post is for me to deepen my understanding of multiple imputation and not forget it. More specifically, I want a middle way which is somewhere between a black box command (leaves me unsatisfied) and full gory details (ahhhhh!).&lt;/p&gt;
&lt;p&gt;In terms of the methodological approach of using multiple imputation for sensitivity analysis, I’ll follow the excellent paper by Cro et al. (&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8569&#34; class=&#34;uri&#34;&gt;https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8569&lt;/a&gt;). The paper comes with a Stata package (&lt;a href=&#34;https://journals.sagepub.com/doi/pdf/10.1177/1536867X1601600211&#34; class=&#34;uri&#34;&gt;https://journals.sagepub.com/doi/pdf/10.1177/1536867X1601600211&lt;/a&gt;) which was based on original SAS Macros by James Roger (&lt;a href=&#34;https://www.lshtm.ac.uk/research/centres-projects-groups/missing-data#dia-missing-data&#34; class=&#34;uri&#34;&gt;https://www.lshtm.ac.uk/research/centres-projects-groups/missing-data#dia-missing-data&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The key tool I’ll be using in R is the &lt;code&gt;brms&lt;/code&gt; package (&lt;a href=&#34;https://cran.r-project.org/web/packages/brms/index.html&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/brms/index.html&lt;/a&gt;). The brilliant thing about this is that I can understand each step conceptually, without knowing too much about the details.&lt;/p&gt;
&lt;p&gt;This is probably an inefficient way to do this though. There could well be errors too.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;data-set&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data set&lt;/h2&gt;
&lt;p&gt;I’ll use the “low dropout” data set from Mallinckrodt et al. (&lt;a href=&#34;https://journals.sagepub.com/doi/pdf/10.1177/2168479013501310&#34; class=&#34;uri&#34;&gt;https://journals.sagepub.com/doi/pdf/10.1177/2168479013501310&lt;/a&gt;) which is available via &lt;a href=&#34;https://www.lshtm.ac.uk/research/centres-projects-groups/missing-data#dia-missing-data&#34; class=&#34;uri&#34;&gt;https://www.lshtm.ac.uk/research/centres-projects-groups/missing-data#dia-missing-data&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;low_data &amp;lt;- haven::read_sas(&amp;quot;low1.sas7bdat&amp;quot;)
head(low_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   PATIENT POOLINV trt   basval  week change
##     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1    1005 101     2         16     1     -3
## 2    1005 101     2         16     2     -5
## 3    1005 101     2         16     4    -10
## 4    1005 101     2         16     6    -11
## 5    1005 101     2         16     8    -13
## 6    1006 101     2         17     1     -1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This data set is in long format. It consists of patient ID, treatment (1 or 2), treatment centre (&lt;code&gt;POOLINV&lt;/code&gt;), baseline value, timepoint (&lt;code&gt;week&lt;/code&gt;), and change from baseline (the outcome variable).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I won’t describe the context here, or do any exploratory analysis – see the Mallinckrodt et al paper for this (&lt;a href=&#34;https://journals.sagepub.com/doi/pdf/10.1177/2168479013501310&#34; class=&#34;uri&#34;&gt;https://journals.sagepub.com/doi/pdf/10.1177/2168479013501310&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For my analysis, I’ll first convert it to a wide format, with a separate variable for the outcome at each timepoint (week 1, 2, 4, 6 and 8).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;low_data_wide &amp;lt;- low_data %&amp;gt;%
  pivot_wider(names_from = week, 
              names_prefix = &amp;quot;week&amp;quot;,
              values_from = change)

head(low_data_wide)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 9
##   PATIENT POOLINV trt   basval week1 week2 week4 week6 week8
##     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1    1005 101     2         16    -3    -5   -10   -11   -13
## 2    1006 101     2         17    -1    -2    -6   -10   -12
## 3    1008 101     1         32    -6   -12   -17   -20   -22
## 4    1011 101     1         18    -1    -5    -8    NA    NA
## 5    1012 101     1         22    -6    -9   -13   -16   -17
## 6    1015 101     2         29    -6   -14   -14   -20   -25&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see now that there are some missing values.&lt;/p&gt;
&lt;p&gt;At this point I’ll also split the data set by treatment group. This is because I’ll use a separate imputation model for each treatment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;low_data_1 &amp;lt;- low_data_wide %&amp;gt;% dplyr::filter(trt == &amp;quot;1&amp;quot;)
low_data_2 &amp;lt;- low_data_wide %&amp;gt;% dplyr::filter(trt == &amp;quot;2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to fit the imputation model, which is basically a Bayesian MMRM, using &lt;code&gt;brms&lt;/code&gt;. The covariarates in each model are baseline (&lt;code&gt;basval&lt;/code&gt;) and centre (&lt;code&gt;POOLINV&lt;/code&gt;), which both get crossed with timepoint. And the covariance matrices are unstructured.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_1 &amp;lt;- brm(data = low_data_1, 
             family = gaussian,
             bf(mvbind(week1, week2, week4, week6, week8) | mi() ~ basval + POOLINV) + 
             set_rescor(TRUE),
             refresh = 0)

fit_2 &amp;lt;- brm(data = low_data_2, 
             family = gaussian,
             bf(mvbind(week1, week2, week4, week6, week8) | mi() ~ basval + POOLINV) + 
             set_rescor(TRUE),
             refresh = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can take a quick look at the summary of the first model fit to reassure ourselves it is giving us the right kind of thing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit_1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian, gaussian, gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity
##          mu = identity; sigma = identity
##          mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: week1 | mi() ~ basval + POOLINV 
##          week2 | mi() ~ basval + POOLINV 
##          week4 | mi() ~ basval + POOLINV 
##          week6 | mi() ~ basval + POOLINV 
##          week8 | mi() ~ basval + POOLINV 
##    Data: low_data_1 (Number of observations: 100) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## week1_Intercept      1.60      1.57    -1.51     4.71 1.00     4000
## week2_Intercept     -2.20      2.35    -6.71     2.43 1.00     3363
## week4_Intercept     -0.25      2.56    -5.21     4.67 1.00     2839
## week6_Intercept      0.25      2.94    -5.53     5.99 1.00     2658
## week8_Intercept      5.11      3.18    -1.24    11.45 1.00     3234
## week1_basval        -0.19      0.07    -0.33    -0.05 1.00     4603
## week1_POOLINV121    -0.08      0.76    -1.60     1.41 1.00     3794
## week1_POOLINV131     0.09      0.65    -1.19     1.42 1.00     3684
## week1_POOLINV141     0.78      0.69    -0.60     2.10 1.00     3695
## week2_basval        -0.13      0.10    -0.33     0.07 1.00     4158
## week2_POOLINV121     1.12      1.14    -1.17     3.29 1.00     2923
## week2_POOLINV131    -0.89      0.99    -2.79     1.07 1.00     2865
## week2_POOLINV141     0.59      1.03    -1.41     2.61 1.00     3027
## week4_basval        -0.34      0.11    -0.56    -0.12 1.00     3469
## week4_POOLINV121     0.82      1.22    -1.56     3.15 1.00     2511
## week4_POOLINV131    -1.74      1.08    -3.84     0.40 1.00     2620
## week4_POOLINV141     0.24      1.13    -1.97     2.42 1.00     2610
## week6_basval        -0.45      0.13    -0.70    -0.19 1.00     3247
## week6_POOLINV121     0.29      1.42    -2.48     3.11 1.00     2632
## week6_POOLINV131    -0.75      1.24    -3.17     1.66 1.00     2343
## week6_POOLINV141    -0.25      1.27    -2.64     2.26 1.00     2458
## week8_basval        -0.70      0.14    -0.97    -0.42 1.00     3638
## week8_POOLINV121    -0.50      1.56    -3.53     2.56 1.00     3161
## week8_POOLINV131    -2.67      1.36    -5.25    -0.00 1.00     2597
## week8_POOLINV141    -1.22      1.39    -3.84     1.49 1.00     2932
##                  Tail_ESS
## week1_Intercept      3397
## week2_Intercept      2979
## week4_Intercept      2889
## week6_Intercept      2744
## week8_Intercept      3078
## week1_basval         3552
## week1_POOLINV121     3029
## week1_POOLINV131     3199
## week1_POOLINV141     3162
## week2_basval         3157
## week2_POOLINV121     3272
## week2_POOLINV131     3013
## week2_POOLINV141     3075
## week4_basval         3213
## week4_POOLINV121     2567
## week4_POOLINV131     3273
## week4_POOLINV141     2821
## week6_basval         2879
## week6_POOLINV121     2651
## week6_POOLINV131     2558
## week6_POOLINV141     2869
## week8_basval         3229
## week8_POOLINV121     2997
## week8_POOLINV131     2840
## week8_POOLINV141     3062
## 
## Family Specific Parameters: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_week1     2.50      0.18     2.18     2.89 1.00     5461     3139
## sigma_week2     3.73      0.27     3.25     4.30 1.00     3838     3248
## sigma_week4     4.05      0.29     3.54     4.66 1.00     3645     3259
## sigma_week6     4.53      0.34     3.93     5.26 1.00     3573     2732
## sigma_week8     4.88      0.38     4.19     5.71 1.00     4169     3132
## 
## Residual Correlations: 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## rescor(week1,week2)     0.53      0.07     0.38     0.67 1.00     3978
## rescor(week1,week4)     0.42      0.08     0.24     0.56 1.00     3800
## rescor(week2,week4)     0.71      0.05     0.60     0.80 1.00     3129
## rescor(week1,week6)     0.32      0.09     0.14     0.49 1.00     3557
## rescor(week2,week6)     0.53      0.07     0.37     0.66 1.00     3500
## rescor(week4,week6)     0.76      0.04     0.67     0.84 1.00     3506
## rescor(week1,week8)     0.17      0.10    -0.03     0.36 1.00     3875
## rescor(week2,week8)     0.33      0.09     0.14     0.50 1.00     3791
## rescor(week4,week8)     0.55      0.07     0.39     0.68 1.00     3528
## rescor(week6,week8)     0.85      0.03     0.78     0.90 1.00     3724
##                     Tail_ESS
## rescor(week1,week2)     3313
## rescor(week1,week4)     3299
## rescor(week2,week4)     3356
## rescor(week1,week6)     3151
## rescor(week2,week6)     2833
## rescor(week4,week6)     2767
## rescor(week1,week8)     3453
## rescor(week2,week8)     3145
## rescor(week4,week8)     3300
## rescor(week6,week8)     3391
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I’ll extract the posterior samples from the MCMC.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_1 &amp;lt;- posterior_samples(fit_1)
dim(p_1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4000   58&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that we have 4000 samples from 58 “parameters”. Actually, only the first 40 are what I would think of as model parameters, e.g., (printing the head of columns 1,2,3)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(p_1[,c(1,2,3)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   b_week1_Intercept b_week2_Intercept b_week4_Intercept
## 1         0.5154750        -5.1858678         -3.870133
## 2         1.6749945        -3.3279521         -2.459063
## 3         3.1583047        -0.7224206          4.742484
## 4         3.4688916         1.9246520          2.719808
## 5         4.2199799        -0.7346923          5.604683
## 6         0.4128793        -4.0833588         -2.098488&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the rest are samples from the predictive distribution of the missing data, e.g., (printing the head of columns 51,52,53)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(p_1[,c(51,52,53)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Ymi_week8[9] Ymi_week8[18] Ymi_week8[20]
## 1    -4.059760    -13.807886     -5.103871
## 2    -3.287829     -6.579197    -10.596638
## 3    -2.680057      1.211975    -10.349368
## 4    -1.487953     -8.258051     -5.253324
## 5    -4.687085    -11.235663    -10.320470
## 6    -1.184016     -5.524158     -5.053344&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see this corresponds to missing data on the week 8 variable in rows 9,18, and 20, respectively, of the original data set (&lt;code&gt;low_data_1&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;That’s the beautiful thing about MCMC. I can sample from the predictive distribution of the missing data “for free”. To create mulitple imputed data sets, all I need to do is extract samples from the MCMC chain and use them to fill in the missing data in the original data set.&lt;/p&gt;
&lt;p&gt;To actually do this, I need to introduce a small piece of ugly code (sorry!). Essentially, I’m extracting the location of the missing data in the original data set via the column names of the MCMC object.&lt;/p&gt;
&lt;p&gt;I’m creating 5 imputed data sets because that’s all I’m prepared to type (one could easily take more and create a function to avoid typing). As I have 4000 (correlated) samples from the posterior, and need to create 5 imputed data sets, I’m picking out rows 800, 1600, 2400, 3200 and 4000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### create 5 copies of the data set (TRT 1), plus original (labelled 0).
low_data_1_0 &amp;lt;- low_data_1 %&amp;gt;% mutate(.id = PATIENT, .imp = 0)
low_data_1_1 &amp;lt;- low_data_1 %&amp;gt;% mutate(.id = PATIENT, .imp = 1)
low_data_1_2 &amp;lt;- low_data_1 %&amp;gt;% mutate(.id = PATIENT, .imp = 2)
low_data_1_3 &amp;lt;- low_data_1 %&amp;gt;% mutate(.id = PATIENT, .imp = 3)
low_data_1_4 &amp;lt;- low_data_1 %&amp;gt;% mutate(.id = PATIENT, .imp = 4)
low_data_1_5 &amp;lt;- low_data_1 %&amp;gt;% mutate(.id = PATIENT, .imp = 5)

### extract the positions of NAs in data set that need to be replaced.
s_1_1 &amp;lt;- str_split(names(p_1), &amp;quot;Ymi_&amp;quot;, simplify = TRUE)
s_2_1 &amp;lt;- str_split(s_1_1[,2], &amp;quot;\\[&amp;quot;, simplify = TRUE)
col_i_1 &amp;lt;- s_2_1[,1]
row_i_1 &amp;lt;- as.numeric(str_split(s_2_1[,2], &amp;quot;\\]&amp;quot;, simplify = TRUE)[,1])


### replace NAs with values from MCMC chain
for (i in seq_along(col_i_1)){
  
  if(col_i_1[i] != &amp;quot;&amp;quot;){
    
    low_data_1_1[row_i_1[i], col_i_1[i]] &amp;lt;- as.numeric(p_1[800, i])    
    low_data_1_2[row_i_1[i], col_i_1[i]] &amp;lt;- as.numeric(p_1[1600, i])  
    low_data_1_3[row_i_1[i], col_i_1[i]] &amp;lt;- as.numeric(p_1[2400, i])  
    low_data_1_4[row_i_1[i], col_i_1[i]] &amp;lt;- as.numeric(p_1[3200, i])  
    low_data_1_5[row_i_1[i], col_i_1[i]] &amp;lt;- as.numeric(p_1[4000, i])  
    
  }
  
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we look at the first two rows of the original data set, followed by the first two rows of the first imputed data set, we can see that the missing values have been filled in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(low_data_1_0, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 11
##   PATIENT POOLINV trt   basval week1 week2 week4 week6 week8   .id  .imp
##     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1    1008 101     1         32    -6   -12   -17   -20   -22  1008     0
## 2    1011 101     1         18    -1    -5    -8    NA    NA  1011     0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(low_data_1_1, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 11
##   PATIENT POOLINV trt   basval week1 week2 week4 week6 week8   .id  .imp
##     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1    1008 101     1         32    -6   -12   -17 -20   -22    1008     1
## 2    1011 101     1         18    -1    -5    -8 -15.4 -17.4  1011     1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I now need to do exactly the same thing for the treatment 2 data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_2 &amp;lt;- posterior_samples(fit_2)

### create 5 copies of the data set (TRT 1), plus original (labelled 0).
low_data_2_0 &amp;lt;- low_data_2 %&amp;gt;% mutate(.id = PATIENT, .imp = 0)
low_data_2_1 &amp;lt;- low_data_2 %&amp;gt;% mutate(.id = PATIENT, .imp = 1)
low_data_2_2 &amp;lt;- low_data_2 %&amp;gt;% mutate(.id = PATIENT, .imp = 2)
low_data_2_3 &amp;lt;- low_data_2 %&amp;gt;% mutate(.id = PATIENT, .imp = 3)
low_data_2_4 &amp;lt;- low_data_2 %&amp;gt;% mutate(.id = PATIENT, .imp = 4)
low_data_2_5 &amp;lt;- low_data_2 %&amp;gt;% mutate(.id = PATIENT, .imp = 5)

### extract the positions of NAs in data set that need to be replaced.
s_1_2 &amp;lt;- str_split(names(p_2), &amp;quot;Ymi_&amp;quot;, simplify = TRUE)
s_2_2 &amp;lt;- str_split(s_1_2[,2], &amp;quot;\\[&amp;quot;, simplify = TRUE)
col_i_2 &amp;lt;- s_2_2[,1]
row_i_2 &amp;lt;- as.numeric(str_split(s_2_2[,2], &amp;quot;\\]&amp;quot;, simplify = TRUE)[,1])


### replace NAs with values from MCMC chain
for (i in seq_along(col_i_2)){
  
  if(col_i_2[i] != &amp;quot;&amp;quot;){
    
    low_data_2_1[row_i_2[i], col_i_2[i]] &amp;lt;- as.numeric(p_2[800, i])  
    low_data_2_2[row_i_2[i], col_i_2[i]] &amp;lt;- as.numeric(p_2[1600, i]) 
    low_data_2_3[row_i_2[i], col_i_2[i]] &amp;lt;- as.numeric(p_2[2400, i]) 
    low_data_2_4[row_i_2[i], col_i_2[i]] &amp;lt;- as.numeric(p_2[3200, i]) 
    low_data_2_5[row_i_2[i], col_i_2[i]] &amp;lt;- as.numeric(p_2[4000, i]) 
    
  }
  
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then I can stack all of the imputed data sets together. Note that I created a label &lt;code&gt;.imp&lt;/code&gt; for each imputed data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;low_data_mi &amp;lt;- rbind(low_data_1_0,
                     low_data_2_0,
                     low_data_1_1,
                     low_data_2_1,
                     low_data_1_2,
                     low_data_2_2,
                     low_data_1_3,
                     low_data_2_3,
                     low_data_1_4,
                     low_data_2_4,
                     low_data_1_5,
                     low_data_2_5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final step is to fit an analysis model (ANCOVA: week 8 vs treatment, baseline and centre) to each imputed data set, and then combine the results using Rubin’s Rules (&lt;a href=&#34;https://bookdown.org/mwheymans/bookmi/rubins-rules.html&#34; class=&#34;uri&#34;&gt;https://bookdown.org/mwheymans/bookmi/rubins-rules.html&lt;/a&gt;). I claimed in the title that I wouldn’t use a specialist multiple imputation package, but I’ll borrow some code from &lt;code&gt;mice&lt;/code&gt; for this step (that’s why I created the &lt;code&gt;.id&lt;/code&gt; and &lt;code&gt;.imp&lt;/code&gt; columns above – &lt;code&gt;mice&lt;/code&gt; recognizes these):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mice)
fit_mi &amp;lt;- with(as.mids(low_data_mi), lm(week8 ~ trt + POOLINV + basval))
summary(pool(fit_mi))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               estimate  std.error  statistic       df      p.value
## (Intercept)  1.7477586 1.89808520  0.9208009 178.3657 3.583981e-01
## trt2        -1.8635641 0.72553680 -2.5685315 103.8203 1.163510e-02
## POOLINV121  -0.6095234 1.07376910 -0.5676484 153.3716 5.711037e-01
## POOLINV131  -2.1373912 0.94122937 -2.2708505 126.8167 2.484221e-02
## POOLINV141  -0.1464228 0.96397585 -0.1518946 114.2162 8.795380e-01
## basval      -0.5532151 0.08369395 -6.6099764 186.9722 3.901515e-10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the estimated treatment effect is around -1.8 with standard error around 0.7. Comparing this with Table 4 in the Mallinckrodt et al paper, we can see that it’s in the right ballpark, bearing in mind slight differences in the model and the low number of imputations.&lt;/p&gt;
&lt;p&gt;Up until now I’ve only performed imputations under a missing-at-random assumption. Given that we could just fit an MMRM model directly, there wasn’t much point in going to the trouble of multiple imputation. However, the advantage of multiple imputation is that it’s easy to perform certain types of sensitivity analysis.&lt;/p&gt;
&lt;p&gt;For example, to perform &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;-based sensitivity analysis (see Cro et al.), I may, as an example, add a fixed amount &lt;span class=&#34;math inline&#34;&gt;\(\delta = 2\)&lt;/span&gt; to the imputations on the experimental treatment arm. To do this, I’ll literally copy-paste by code above, but adding delta:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;delta &amp;lt;- 2

### replace NAs with values from MCMC chain
for (i in seq_along(col_i_2)){
  
  if(col_i_2[i] != &amp;quot;&amp;quot;){
    
    low_data_2_1[row_i_2[i], col_i_2[i]] &amp;lt;- as.numeric(p_2[800, i])  + delta 
    low_data_2_2[row_i_2[i], col_i_2[i]] &amp;lt;- as.numeric(p_2[1600, i]) + delta 
    low_data_2_3[row_i_2[i], col_i_2[i]] &amp;lt;- as.numeric(p_2[2400, i]) + delta 
    low_data_2_4[row_i_2[i], col_i_2[i]] &amp;lt;- as.numeric(p_2[3200, i]) + delta 
    low_data_2_5[row_i_2[i], col_i_2[i]] &amp;lt;- as.numeric(p_2[4000, i]) + delta 
    
  }
  
  
}

low_data_mi &amp;lt;- rbind(low_data_1_0,
                     low_data_2_0,
                     low_data_1_1,
                     low_data_2_1,
                     low_data_1_2,
                     low_data_2_2,
                     low_data_1_3,
                     low_data_2_3,
                     low_data_1_4,
                     low_data_2_4,
                     low_data_1_5,
                     low_data_2_5)

fit_mi &amp;lt;- with(as.mids(low_data_mi), lm(week8 ~ trt + POOLINV + basval))
summary(pool(fit_mi))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               estimate  std.error   statistic       df      p.value
## (Intercept)  1.3996455 1.90747537  0.73376861 178.5648 4.640521e-01
## trt2        -1.7184241 0.72878966 -2.35791501 104.6129 2.023692e-02
## POOLINV121  -0.6247259 1.07890860 -0.57903510 153.8860 5.634121e-01
## POOLINV131  -2.1714417 0.94558853 -2.29639171 127.5395 2.328443e-02
## POOLINV141  -0.0533222 0.96836491 -0.05506416 114.9899 9.561829e-01
## basval      -0.5368745 0.08411448 -6.38266384 187.0423 1.336286e-09&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that this reduces the treatment effect somewhat. The standard error has not changed much.&lt;/p&gt;
&lt;p&gt;An alternative to &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;-based sensitivity analysis is reference-based sensitivity analysis, where more qualitative types of assumption are made about the missing data. See Cro et al. for a disscussion of pros and cons of the two approaches. My very personal opinion is that I prefer the &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;-based approach. But maybe that’s influenced by its computational simplicity. There’s no way I could do an exercise like this for a reference-based approach in 100 lines of code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Mallinckrodt, Craig, et al. “Recent developments in the prevention and treatment of missing data.” Therapeutic innovation &amp;amp; regulatory science 48.1 (2014): 68-80.&lt;/p&gt;
&lt;p&gt;Cro, Suzie, et al. “Sensitivity analysis for clinical trials with missing continuous outcome data using controlled multiple imputation: a practical guide.” Statistics in medicine 39.21 (2020): 2815-2842.&lt;/p&gt;
&lt;p&gt;Cro, Suzie, et al. “Reference-based sensitivity analysis via multiple imputation for longitudinal trials with protocol deviation.” The Stata Journal 16.2 (2016): 443-463.&lt;/p&gt;
&lt;p&gt;Bürkner, Paul-Christian. “brms: An R package for Bayesian multilevel models using Stan.” Journal of statistical software 80.1 (2017): 1-28.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;helpful-links&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Helpful links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://thestatsgeek.com/category/missing-data/&#34; class=&#34;uri&#34;&gt;https://thestatsgeek.com/category/missing-data/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://discourse.mc-stan.org/t/unstructured-error-covariance-matrix-in-a-multilevel-growth-model/21792/4&#34; class=&#34;uri&#34;&gt;https://discourse.mc-stan.org/t/unstructured-error-covariance-matrix-in-a-multilevel-growth-model/21792/4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.lshtm.ac.uk/research/centres-projects-groups/missing-data#dia-missing-data&#34; class=&#34;uri&#34;&gt;https://www.lshtm.ac.uk/research/centres-projects-groups/missing-data#dia-missing-data&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Landmark/Milestone analysis under a Royston-Parmar flexible parametric survival model using the R package flexsurv</title>
      <link>/post/landmark-milestone-analysis-under-a-royston-parmar-flexible-survival-model-using-the-r-package-flexsurv/</link>
      <pubDate>Sat, 22 May 2021 00:00:00 +0000</pubDate>
      <guid>/post/landmark-milestone-analysis-under-a-royston-parmar-flexible-survival-model-using-the-r-package-flexsurv/</guid>
      <description>


&lt;p&gt;The aim of this post is to demonstrate a landmark/milestone analysis of RCT time-to-event data with a Royston-Parmar flexible parametric survival model. The original reference is:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Royston P, Parmar M (2002). “Flexible Parametric Proportional-Hazards and Proportional-Odds Models for Censored Survival Data, with Application to Prognostic Modelling and
Estimation of Treatment Effects.” Statistics in Medicine, 21(1), 2175–2197. &lt;a href=&#34;doi:10.1002/&#34; class=&#34;uri&#34;&gt;doi:10.1002/&lt;/a&gt;
sim.1203&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This model has been expertly coded and documented by Chris Jackson in the R package &lt;strong&gt;flexsurv&lt;/strong&gt; (&lt;a href=&#34;https://www.jstatsoft.org/article/view/v070i08&#34; class=&#34;uri&#34;&gt;https://www.jstatsoft.org/article/view/v070i08&lt;/a&gt;). In this post I’ll be making a big meal out of the same material in an effort to increase my own understanding.&lt;/p&gt;
&lt;p&gt;I need a dataset, so I’ll re-use one from a &lt;a href=&#34;../post/adjusting-for-covariates-under-non-proportional-hazards&#34;&gt;previous blogpost&lt;/a&gt; – this is
&lt;a href=&#34;https://www.nature.com/articles/s41591-018-0134-3&#34;&gt;publically available data&lt;/a&gt; from the OAK RCT comparing a checkpoint inhibitor (MPDL3280A) versus chemotherapy (Docetaxel).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(survival)
library(dplyr)
library(flexsurv)
dat_oak &amp;lt;- readxl::read_excel(&amp;quot;41591_2018_134_MOESM3_ESM.xlsx&amp;quot;,
                              sheet = 3) %&amp;gt;% 
  select(PtID, ECOGGR, OS, OS.CNSR, TRT01P) %&amp;gt;%
  mutate(OS.EVENT = -1 * (OS.CNSR - 1))

head(dat_oak)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##    PtID ECOGGR    OS OS.CNSR TRT01P    OS.EVENT
##   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;
## 1   318      1  5.19       0 Docetaxel        1
## 2  1088      0  4.83       0 Docetaxel        1
## 3   345      1  1.94       0 Docetaxel        1
## 4   588      0 24.6        1 Docetaxel        0
## 5   306      1  5.98       0 MPDL3280A        1
## 6   718      1 19.2        1 Docetaxel        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ECOG grade is a prognostic covariate. Initially I’m going to ignore this covariate, but I’ll come back to it later in the post.&lt;/p&gt;
&lt;p&gt;I’ll start by plotting the survival curves using the &lt;code&gt;survminer&lt;/code&gt; package (&lt;a href=&#34;https://cran.r-project.org/web/packages/survminer/index.html&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/survminer/index.html&lt;/a&gt;)…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;km_oak &amp;lt;- survfit(Surv(OS, OS.EVENT) ~ TRT01P,
                  data = dat_oak)


survminer::ggsurvplot(km_oak, 
                      data = dat_oak, 
                      risk.table = TRUE, 
                      break.x.by = 6,
                      legend.title = &amp;quot;&amp;quot;,
                      xlab = &amp;quot;Time (months)&amp;quot;,
                      ylab = &amp;quot;Overall survival&amp;quot;,
                      risk.table.fontsize = 4,
                      legend = c(0.8,0.8))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2021-05-22-landmark-milestone-analysis-under-a-royston-parmar-flexible-survival-model-using-the-r-package-flexsurv_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;../post/flexsurv-2&#34;&gt;another previous post&lt;/a&gt; I discussed using &lt;code&gt;flexsurv::flexsurvreg&lt;/code&gt; to fit a Weibull model. In brief, we can describe this model as two parallel straight lines on the log cumumlative hazard scale (versus log time).&lt;/p&gt;
&lt;p&gt;Here, I’ll fit a Weibull model to the OAK data and plot the log cumulative hazard versus log time, on top of the non-parametric equivalent. Note that I’m creating functions here because I’ll want to make this graph repeatedly as we add flexibility to the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;###########################
### Non-parametric analysis
###########################

# Vector of time in months (for plotting):
t_seq &amp;lt;- seq(1, 24, length.out = 100)
# Extract km estimates
km_sum &amp;lt;- summary(km_oak, times = t_seq, extend = TRUE)

# Function to plot km estimates (log-cumhaz scale)
plot_non_param &amp;lt;- function(){
  
  plot(log(t_seq), log(km_sum$cumhaz[1:100]), type = &amp;quot;l&amp;quot;, 
       xlab = &amp;quot;log(time)&amp;quot;, ylab = &amp;quot;log cumulative hazard&amp;quot;)
  points(log(t_seq), log(km_sum$cumhaz[101:200]), type = &amp;quot;l&amp;quot;, col = 2)
  legend(&amp;quot;bottomright&amp;quot;, c(&amp;quot;Docetaxel&amp;quot;, &amp;quot;MPDL3280A&amp;quot;),
         lty = c(1,1), col = 1:2)       
  
}

#########################
### Weibull regression
#########################
fit_weibull &amp;lt;- flexsurvreg(Surv(OS, OS.EVENT) ~ TRT01P,
                           data = dat_oak,
                           dist = &amp;quot;weibull&amp;quot;)


#Function to add parametric estimate of log-cumulative hazard
#to the KM-based plot
add_model_estimate &amp;lt;- function(fit){
  
  ### Add Docetaxel estimate
  summary(fit, 
          newdata = data.frame(TRT01P = &amp;quot;Docetaxel&amp;quot;),
          t = t_seq, type = &amp;quot;cumhaz&amp;quot;)[[1]][,&amp;quot;est&amp;quot;] %&amp;gt;% 
    log() %&amp;gt;% 
    points(x = log(t_seq), type = &amp;quot;l&amp;quot;, lty = 2)
  
  ### Add MPDL3280A estimate
  summary(fit, 
          newdata = data.frame(TRT01P = &amp;quot;MPDL3280A&amp;quot;),
          t = t_seq, type = &amp;quot;cumhaz&amp;quot;)[[1]][,&amp;quot;est&amp;quot;] %&amp;gt;% 
    log() %&amp;gt;% 
    points(x = log(t_seq), type = &amp;quot;l&amp;quot;, lty = 2, col = 2)
}


# plot Weibull regression on top of KM estimates
plot_non_param()
add_model_estimate(fit_weibull)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2021-05-22-landmark-milestone-analysis-under-a-royston-parmar-flexible-survival-model-using-the-r-package-flexsurv_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see this is not a good fit at early timepoints. Parallel lines on this scale is the same as saying the hazards are proportional, which is clearly not the case. We therefore need to relax the proportional hazards assumption. One option is to maintain the log-cumulative-hazard-versus-log-time perspective, also maintain &lt;em&gt;straight&lt;/em&gt; lines on this scale, but give each treatment its own slope. In other words, this is a Weibull model but with a different shape parameter per treatment. One way I could achieve this is to split the data set and fit two separate models. I won’t do this though. Instead I’ll use the &lt;code&gt;flexsurv::flexsurvspline&lt;/code&gt; function.
First I’m just going do it. Later I’ll explain the model and syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# spline model with 0 internal knots and where each treatment has its
# own intercept (gamma0) and its own slope (gamma1) 
fit_spline_0_1 &amp;lt;- flexsurvspline(Surv(OS, OS.EVENT) ~ TRT01P + gamma1(TRT01P), 
                                 data = dat_oak,
                                 k = 0,
                                 scale = &amp;quot;hazard&amp;quot;)

plot_non_param()
add_model_estimate(fit_spline_0_1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2021-05-22-landmark-milestone-analysis-under-a-royston-parmar-flexible-survival-model-using-the-r-package-flexsurv_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This perhaps looks a bit better, but still not great. At this point we might be tempted to introduce more complex splines. That is, we move away from modelling the log-cumulative hazard versus log time as straight lines. In my &lt;a href=&#34;../post/flexsurv-2&#34;&gt;previous post&lt;/a&gt; I talked about this, but still in the context of a proportional hazards model. That is, allowing curved lines, but proportional. For completeness, this went something like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# spline model with 1 internal knot and where each treatment has its 
# own intercept (gamma0), but common slope (gamma1) and common gamma2 -- I&amp;#39;ll explain later.
fit_spline_1_0 &amp;lt;- flexsurvspline(Surv(OS, OS.EVENT) ~ TRT01P, 
                                 data = dat_oak,
                                 k = 1,
                                 scale = &amp;quot;hazard&amp;quot;)

plot_non_param()
add_model_estimate(fit_spline_1_0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2021-05-22-landmark-milestone-analysis-under-a-royston-parmar-flexible-survival-model-using-the-r-package-flexsurv_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As mentioned, this doesn’t capture the non-proportional hazards. We need something a bit more flexible…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# spline model with 1 internal knot and where each treatment has its 
# own intercept (gamma0), its own slope (gamma1), but common gamma2 -- I&amp;#39;ll explain later.
fit_spline_1_1 &amp;lt;- flexsurvspline(Surv(OS, OS.EVENT) ~ TRT01P + gamma1(TRT01P), 
                               data = dat_oak,
                               k = 1,
                               scale = &amp;quot;hazard&amp;quot;)

plot_non_param()
add_model_estimate(fit_spline_1_1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2021-05-22-landmark-milestone-analysis-under-a-royston-parmar-flexible-survival-model-using-the-r-package-flexsurv_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is starting to look better. Now to expain a bit more what’s going on. Let’s look at the maximum likelihood estimates from the last model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_spline_1_1$res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                 est        L95%        U95%         se
## gamma0                  -3.12264755 -3.44183503 -2.80346006 0.16285375
## gamma1                   1.73312765  1.33785579  2.12839950 0.20167302
## gamma2                   0.03389207  0.01200609  0.05577806 0.01116653
## TRT01PMPDL3280A         -0.10551890 -0.56579074  0.35475295 0.23483689
## gamma1(TRT01PMPDL3280A) -0.07747115 -0.23538127  0.08043896 0.08056787&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The corresponding model is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log H(t;~Docetaxel) = \gamma_0 + \gamma_1 \times \log(t) + \gamma_2 \times f(\log(t))\]&lt;/span&gt;
and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log H(t;~MPDL3280A) = \gamma_0 + \text{`TRT01PMPDL3280A`} + \gamma_1 \times \log(t) + \text{`gamma1(TRT01PMPDL3280A)`} \times \log(t) + \gamma_2 \times f(\log(t))\]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now the question is: what is &lt;span class=&#34;math inline&#34;&gt;\(f(\log(t))\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;The best way to answer that is probably to plot it, together with the other two basis functions (constant and linear) in this model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_basis &amp;lt;- flexsurv:::basis(knots = fit_spline_1_1$knots, log(1:24))
my_gamma &amp;lt;- fit_spline_1_1$res[c(&amp;quot;gamma0&amp;quot;, &amp;quot;gamma1&amp;quot;, &amp;quot;gamma2&amp;quot;), &amp;quot;est&amp;quot;]

plot(log(1:24),colSums(t(my_basis) * c(0,0,1)) / -50,
     main = &amp;quot;Basis funs (re-scaled to fit on same plot)&amp;quot;,
     xlab = &amp;quot;log(time)&amp;quot;,
     ylab = &amp;quot;Arbitrary scale&amp;quot;,
     type = &amp;quot;l&amp;quot;, ylim = c(-0.5, 1.5))
points(log(1:24),colSums(t(my_basis) * c(0,1,0)) / 3, type = &amp;quot;l&amp;quot;, col = 2)
points(log(1:24),colSums(t(my_basis) * c(1,0,0)) , type = &amp;quot;l&amp;quot;, col = 3)

legend(&amp;quot;bottomright&amp;quot;, c(&amp;quot;const&amp;quot;, &amp;quot;log(t)&amp;quot;, &amp;quot;f(log(t))&amp;quot;),
       lty = c(1,1,1),
       col = 3:1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2021-05-22-landmark-milestone-analysis-under-a-royston-parmar-flexible-survival-model-using-the-r-package-flexsurv_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The formulas are given in the &lt;code&gt;flexsurv&lt;/code&gt; documentation but I won’t repeat it here because I don’t really care. The important thing is that I’m allowing the log-cumulative hazard function to be a linear combination of more complex basis functions in &lt;span class=&#34;math inline&#34;&gt;\(log(t)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Hopefully I’ve done enough examples that you can see what the syntax is doing. Basically, by specifying &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; internal knots, the baseline log-cumulative hazard is a linear combination of &lt;span class=&#34;math inline&#34;&gt;\(k+2\)&lt;/span&gt; basis functions, including a constant function, a linear function, and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; more complex things. By default, if you just write something like &lt;code&gt;formula = Surv(OS, OS.EVENT) ~ TRT01P&lt;/code&gt; then this will give you a separate constant (&lt;code&gt;gamma0&lt;/code&gt;) for each treatment. For the other basis functions, if you want each treatment to have its own version then you must explicitly specify this by adding e.g. &lt;code&gt;gamma1(TRT01P)&lt;/code&gt; into the formula. In principle, I don’t see any reason why you couldn’t fit the following model with a common slope (&lt;code&gt;gamma1&lt;/code&gt;) but separate &lt;code&gt;gamma2&lt;/code&gt;, to give an alternative to &lt;code&gt;fit_spline_1_1&lt;/code&gt; but with the same number of parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# spline model with 1 internal knot and where each treatment has its 
# own intercept (gamma0), its own gamma2, but common slope (gamma1).
fit_spline_1_2 &amp;lt;- flexsurvspline(Surv(OS, OS.EVENT) ~ TRT01P + gamma2(TRT01P), 
                               data = dat_oak,
                               k = 1,
                               scale = &amp;quot;hazard&amp;quot;)

plot_non_param()
add_model_estimate(fit_spline_1_2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2021-05-22-landmark-milestone-analysis-under-a-royston-parmar-flexible-survival-model-using-the-r-package-flexsurv_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As a final flourish for this post, I’ll do the landmark/milestone analysis based on the model &lt;code&gt;fit_spline_1_1&lt;/code&gt;. Let’s pick 18 months as the landmark time. I’ll take the same approach as the &lt;code&gt;flexsurv&lt;/code&gt; package. This simulates from the multivariate normal distribution with mean equal to the maximum likelihood estimates and variance equal to the variance of the maximum likelihood estimates. These random draws are then put through a function outputting the quantity of interest (in our case the difference in survival probabilities at 18 months) and we can then take quantiles to give us an estimate and confidence interval.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## simulate from MVN distribution
sims &amp;lt;- normboot.flexsurvreg(fit_spline_1_1, 
                             B = 1e5, 
                             raw = TRUE)

## give S(18) on docetaxel for each draw 
s_18_docetaxel &amp;lt;-  1 - psurvspline(18, 
                                   gamma = sims[,c(&amp;quot;gamma0&amp;quot;, &amp;quot;gamma1&amp;quot;, &amp;quot;gamma2&amp;quot;)], 
                                   knots = fit_spline_1_1$knots)

## give S(18) on MPDL3280A for each draw 
s_18_MPDL3280A &amp;lt;-  1 - psurvspline(18, 
                                   gamma = cbind(sims[,&amp;quot;gamma0&amp;quot;], 
                                                 sims[,&amp;quot;gamma1&amp;quot;]+sims[,&amp;quot;gamma1(TRT01PMPDL3280A)&amp;quot;], 
                                                 sims[,&amp;quot;gamma2&amp;quot;]),
                                   offset = sims[,&amp;quot;TRT01PMPDL3280A&amp;quot;],
                                   knots = fit_spline_1_1$knots)

## 95% CI and estimate for diff in survival 
## at 18 months (MPDL3280A - docetaxel)
quantile(s_18_MPDL3280A - s_18_docetaxel,
         probs = c(0.025, 0.5, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       2.5%        50%      97.5% 
## 0.05959083 0.12016872 0.17982301&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;p.s.-prognostic-covariate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;p.s. prognostic covariate&lt;/h2&gt;
&lt;p&gt;I promised at one point to discuss the prognostic covariate ECOG status which was available in the data set (in this particular data set it takes values 0 and 1). This post is already longer than I expected, so I won’t dwell on this, but &lt;a href=&#34;https://www.jstatsoft.org/article/view/v070i08&#34;&gt;as explained in the flexsurv tutorial&lt;/a&gt; it’s possible to do a parametric equivalent of a stratified Cox model by using a spline and allowing each ECOG status to have its own &lt;code&gt;gamma0&lt;/code&gt;, &lt;code&gt;gamma1&lt;/code&gt;,…,&lt;code&gt;gamma[k+1]&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make sure ECOGGR is a factor
dat_oak$ECOGGR &amp;lt;- factor(dat_oak$ECOGGR)

# spline model with 2 internal knots and where each ECOG grade has its own 
# complex spline, but where treatment effect within strata is a constant shift
fit_spline_ecog_0 &amp;lt;- flexsurvspline(Surv(OS, OS.EVENT) ~ TRT01P + 
                                      ECOGGR + gamma1(ECOGGR) + gamma2(ECOGGR) + gamma3(ECOGGR), 
                               data = dat_oak,
                               k = 2,
                               scale = &amp;quot;hazard&amp;quot;)

fit_spline_ecog_0$res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                         est       L95%       U95%         se
## gamma0          -4.39991075 -5.1904477 -3.6093738 0.40334259
## gamma1           2.28385223  0.3406304  4.2270740 0.99145792
## gamma2          -0.02574156 -0.3626805  0.3111974 0.17191077
## gamma3           0.11146845 -0.3340140  0.5569509 0.22729116
## TRT01PMPDL3280A -0.33780623 -0.5033559 -0.1722565 0.08446569
## ECOGGR1          1.63784166  0.7727434  2.5029399 0.44138477
## gamma1(ECOGGR1) -0.85035950 -2.8680837  1.1673647 1.02947003
## gamma2(ECOGGR1) -0.03861271 -0.4030762  0.3258508 0.18595417
## gamma3(ECOGGR1)  0.02669129 -0.4635331  0.5169156 0.25011906&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is back to being a proportional hazards model which may not be appropriate. We could then make the treatment effect more flexible, e.g.,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make sure ECOGGR is a factor
dat_oak$ECOGGR &amp;lt;- factor(dat_oak$ECOGGR)

# spline model with 2 internal knots and where each ECOG grade has its own 
# complex spline, but where treatment effect within strata is handled using a 
# constant and linear term only.
fit_spline_ecog_1 &amp;lt;- flexsurvspline(Surv(OS, OS.EVENT) ~ TRT01P + gamma1(TRT01P) +
                                      ECOGGR + gamma1(ECOGGR) + gamma2(ECOGGR) + gamma3(ECOGGR), 
                               data = dat_oak,
                               k = 2,
                               scale = &amp;quot;hazard&amp;quot;)

fit_spline_ecog_1$res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                  est       L95%        U95%         se
## gamma0                  -4.524045821 -5.3443397 -3.70375194 0.41852498
## gamma1                   2.296987159  0.3559155  4.23805878 0.99036086
## gamma2                  -0.030850110 -0.3674476  0.30574735 0.17173655
## gamma3                   0.117629309 -0.3273831  0.56264169 0.22705131
## TRT01PMPDL3280A         -0.085983706 -0.5482385  0.37627105 0.23584859
## ECOGGR1                  1.660089705  0.7948724  2.52530704 0.44144553
## gamma1(TRT01PMPDL3280A) -0.093731710 -0.2545742  0.06711074 0.08206398
## gamma1(ECOGGR1)         -0.799217529 -2.8168457  1.21841060 1.02942102
## gamma2(ECOGGR1)         -0.024330400 -0.3890199  0.34035906 0.18606947
## gamma3(ECOGGR1)          0.006536356 -0.4839968  0.49706953 0.25027663&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It then becomes difficult to summarise the treatment effect succinctly. This is an interesting but complicated topic that I won’t discuss now.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>That&#39;ll do: simulated annealing instead of maths</title>
      <link>/post/that-ll-do-simulated-annealing-instead-of-maths/</link>
      <pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/post/that-ll-do-simulated-annealing-instead-of-maths/</guid>
      <description>


&lt;p&gt;This is a post for future me. Given my steadily declining maths skills, I can see myself needing to use the generalized simulated annealing function &lt;code&gt;GenSA::GenSA&lt;/code&gt; a lot more.&lt;/p&gt;
&lt;p&gt;For example, suppose I’m helping to design a clinical trial and I want to simulate some data from a Gompertz distribution. I know that the Gompertz distribution allows a rapidly decreasing hazard function (that’s why I want to use it), but I’m not familiar with the details, and don’t have the time/motivation right now to do any maths. I want the survival probability at 1 month to be about 80%, at 3 months to be about 75%, and at 12 months to be about 65%. What are the corresponding parameters of the Gompertz distribution?&lt;/p&gt;
&lt;p&gt;From looking at the help pages &lt;code&gt;?flexsurv::pgompertz&lt;/code&gt;, i see that the Gompertz has a shape parameter &lt;code&gt;a&lt;/code&gt; and a rate parameter &lt;code&gt;b&lt;/code&gt;. I write a quick function to calculate the distance between a Gompertz(a,b) distribution and my desired milestone probabilities:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gomp_close &amp;lt;- function(a_b, 
                       t_1, ## timepoint 1
                       t_2, ## timepoint 1
                       t_3, ## timepoint 2
                       p_1, ## p(event by t_1)
                       p_2,
                       p_3){
  
  a &amp;lt;- a_b[1]
  b &amp;lt;- a_b[2]
  
  distance_1 &amp;lt;- flexsurv::pgompertz(t_1, a, b) - p_1
  distance_2 &amp;lt;- flexsurv::pgompertz(t_2, a, b) - p_2
  distance_3 &amp;lt;- flexsurv::pgompertz(t_3, a, b) - p_3
  
  distance_1 ^ 2 + distance_2 ^ 2 + distance_3 ^ 2
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then I let the simulated annealing function find a close enough solution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;result &amp;lt;- GenSA::GenSA(par = c(1, 1), # initial values
                       fn = gomp_close,
                       lower = c(-100, 0), # parameter b is positive, so lower lim 0
                       upper = c(100, 100),
                       t_1 = 1, 
                       t_2 = 3,
                       t_3 = 12,
                       p_1 = 0.2, 
                       p_2 = 0.25, 
                       p_3 = 0.35)

result$par&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.6307335  0.2536741&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just check this looks ok:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(0, 24, length.out = 100)
s &amp;lt;- flexsurv::pgompertz(x, 
                         shape = result$par[1], 
                         rate = result$par[2],
                         lower.tail = FALSE)

plot(x, s, type = &amp;quot;l&amp;quot;, ylim = c(0,1))
points(c(1, 3, 12), c(0.8, 0.75, 0.65))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2021-01-17-that-ll-do-simulated-annealing-instead-of-maths_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adjusting for covariates under non-proportional hazards</title>
      <link>/post/adjusting-for-covariates-under-non-proportional-hazards/</link>
      <pubDate>Sat, 12 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/adjusting-for-covariates-under-non-proportional-hazards/</guid>
      <description>


&lt;p&gt;I’ve written a lot recently about &lt;a href=&#34;../post/non-proportional-hazards-in-immuno-oncology-is-an-old-perspective-needed&#34;&gt;non-proportional hazards in immuno-oncology&lt;/a&gt;. One aspect that I have unfortunately overlooked is covariate adjustment. Perhaps this is because it’s so easy to work with &lt;a href=&#34;../post/extract-patient-level-data-from-a-kaplan-meier-plot&#34;&gt;extracted data from published Kaplan-Meier plots&lt;/a&gt;, where the covariate data is not available. But we know from &lt;a href=&#34;https://www.jstor.org/stable/2531154&#34;&gt;theoretical&lt;/a&gt; and &lt;a href=&#34;https://trialsjournal.biomedcentral.com/articles/10.1186/1745-6215-15-139&#34;&gt;empirical&lt;/a&gt; work that covariate adjustment can lead to big increases in power, and perhaps this is equally important or even more important than the power gains from using a weighted log-rank test to match the anticipated non-proportional hazards. To investigate this, we would need access to immuno-oncology RCT data. In general this is not so easy, but fortunately there’s &lt;a href=&#34;https://www.nature.com/articles/s41591-018-0134-3&#34;&gt;this article&lt;/a&gt; containing clinical data from two RCTs (OAK and POPLAR) comparing a checkpoint inhibitor (atezolizumab) versus chemotherapy.&lt;/p&gt;
&lt;p&gt;Gandara, D.R., Paul, S.M., Kowanetz, M. et al. Blood-based tumor mutational burden as a predictor of clinical benefit in non-small-cell lung cancer patients treated with atezolizumab. Nat Med 24, 1441–1448 (2018). &lt;a href=&#34;https://doi.org/10.1038/s41591-018-0134-3&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1038/s41591-018-0134-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A number of covariates are provided but I will focus on baseline ECOG grade, which is a performance status indicator – a “0” means fully active; a “1” means restricted in physically strenuous activity. Generally, you would expect patients with lower ECOG status to have longer survival.&lt;/p&gt;
&lt;p&gt;Let’s have a look at the OAK data first…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(survival)

## Here I&amp;#39;m loading my library locally...
devtools::load_all(&amp;quot;~/swlrt/&amp;quot;)

## ...but can also get it from github
#devtools::install_github(&amp;quot;dominicmagirr/swlrt&amp;quot;)

## read in the clinical data from the OAK study
## create a new column OS.EVENT which is the opposite
## of OS.CNSR

dat_oak &amp;lt;- readxl::read_excel(&amp;quot;41591_2018_134_MOESM3_ESM.xlsx&amp;quot;,
                              sheet = 3) %&amp;gt;% 
  select(PtID, ECOGGR, OS, OS.CNSR, TRT01P) %&amp;gt;%
  mutate(OS.EVENT = -1 * (OS.CNSR - 1))

## take a look at first few rows
head(dat_oak)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##    PtID ECOGGR    OS OS.CNSR TRT01P    OS.EVENT
##   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;
## 1   318      1  5.19       0 Docetaxel        1
## 2  1088      0  4.83       0 Docetaxel        1
## 3   345      1  1.94       0 Docetaxel        1
## 4   588      0 24.6        1 Docetaxel        0
## 5   306      1  5.98       0 MPDL3280A        1
## 6   718      1 19.2        1 Docetaxel        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s standard fare for surival analysis. Let’s first make a Kaplan-Meier plot and fit a Cox model based only on treatment…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;km_oak &amp;lt;- survfit(Surv(OS, OS.EVENT) ~ TRT01P,
                  data = dat_oak)

survminer::ggsurvplot(km_oak, 
                      data = dat_oak, 
                      risk.table = TRUE, 
                      break.x.by = 6,
                      legend.title = &amp;quot;&amp;quot;,
                      xlab = &amp;quot;Time (months)&amp;quot;,
                      ylab = &amp;quot;Overall survival&amp;quot;,
                      risk.table.fontsize = 4,
                      legend = c(0.8,0.8))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2020-09-12-adjusting-for-covariates-under-non-proportional-hazards_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coxph(Surv(OS, OS.EVENT) ~ TRT01P, data = dat_oak)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## coxph(formula = Surv(OS, OS.EVENT) ~ TRT01P, data = dat_oak)
## 
##                     coef exp(coef) se(coef)      z        p
## TRT01PMPDL3280A -0.31667   0.72857  0.08418 -3.762 0.000169
## 
## Likelihood ratio test=14.16  on 1 df, p=0.0001677
## n= 850, number of events= 569&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The z-value is -3.762 (lower is better). This should be very similar to the z-value of the log-rank test. Let’s check using a function I’ve written for my own R package &lt;a href=&#34;https://github.com/dominicmagirr/swlrt&#34;&gt;swlrt&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;swlrt::wlrt(df = dat_oak,
            trt_colname = &amp;quot;TRT01P&amp;quot;,
            time_colname = &amp;quot;OS&amp;quot;,
            event_colname = &amp;quot;OS.EVENT&amp;quot;,
            wlr = &amp;quot;lr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           u      v_u         z o_minus_e_trt
## 1 -44.59584 139.4881 -3.775946     MPDL3280A&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Had the non-proportional hazards been anticipated, a weighted log-rank test could have been pre-specified, for example a &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8186&#34;&gt;modestly-weighted log-rank test&lt;/a&gt; with &lt;span class=&#34;math inline&#34;&gt;\(t^* = 12\)&lt;/span&gt;…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;swlrt::wlrt(df = dat_oak,
            trt_colname = &amp;quot;TRT01P&amp;quot;,
            time_colname = &amp;quot;OS&amp;quot;,
            event_colname = &amp;quot;OS.EVENT&amp;quot;,
            wlr = &amp;quot;mw&amp;quot;,
            t_star = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           u      v_u         z o_minus_e_trt
## 1 -74.88024 370.4699 -3.890368     MPDL3280A&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This would have had a lower z-value (-3.89), as you’d expect given the slightly delayed effect,&lt;/p&gt;
&lt;div id=&#34;but-what-about-an-adjusted-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;But what about an adjusted analysis?&lt;/h3&gt;
&lt;p&gt;Let’s look at a Cox model including ECOG grade as a covariate…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coxph(Surv(OS, OS.EVENT) ~ TRT01P + ECOGGR, data = dat_oak)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## coxph(formula = Surv(OS, OS.EVENT) ~ TRT01P + ECOGGR, data = dat_oak)
## 
##                     coef exp(coef) se(coef)      z        p
## TRT01PMPDL3280A -0.35234   0.70304  0.08445 -4.172 3.02e-05
## ECOGGR           0.63227   1.88187  0.09072  6.970 3.18e-12
## 
## Likelihood ratio test=65.79  on 2 df, p=5.163e-15
## n= 850, number of events= 569&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the z-value is -4.172. So in this example, remembering to adjust for a prognostic covariate appears more important than dealing with the nph issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cant-we-do-both&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Can’t we do both?&lt;/h3&gt;
&lt;p&gt;Yes, it’s possible to do the modestly-weighted log-rank test, stratified by ECOG grade:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;swlrt::swlrt(df = dat_oak,
             trt_colname = &amp;quot;TRT01P&amp;quot;,
             time_colname = &amp;quot;OS&amp;quot;,
             event_colname = &amp;quot;OS.EVENT&amp;quot;,
             strat_colname = &amp;quot;ECOGGR&amp;quot;,
             wlr = &amp;quot;mw&amp;quot;,
             t_star = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $by_strata
##                 u       v_u         z o_minus_e_trt
## ECOGGR0 -15.46822  87.47411 -1.653867     MPDL3280A
## ECOGGR1 -71.10216 307.90543 -4.052044     MPDL3280A
## 
## $z
## [1] -4.353737&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the z-value is lower still (-4.35), which seems like the best of both worlds.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;will-covariate-adjustment-always-improve-the-z-value&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Will covariate adjustment always improve the z-value?&lt;/h3&gt;
&lt;p&gt;No. Let’s go through exactly the same steps for the POPLAR study.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## read in the clinical data from the POPLAR study
## create a new column OS.EVENT which is the opposite
## of OS.CNSR
dat_poplar &amp;lt;- readxl::read_excel(&amp;quot;41591_2018_134_MOESM3_ESM.xlsx&amp;quot;,
                              sheet = 2) %&amp;gt;% 
  select(PtID, ECOGGR, OS, OS.CNSR, TRT01P) %&amp;gt;%
  mutate(OS.EVENT = -1 * (OS.CNSR - 1))


km_poplar &amp;lt;- survfit(Surv(OS, OS.EVENT) ~ TRT01P,
                  data = dat_poplar)

survminer::ggsurvplot(km_poplar, 
                      data = dat_poplar, 
                      risk.table = TRUE, 
                      break.x.by = 6,
                      legend.title = &amp;quot;&amp;quot;,
                      xlab = &amp;quot;Time (months)&amp;quot;,
                      ylab = &amp;quot;Overall survival&amp;quot;,
                      risk.table.fontsize = 4,
                      legend = c(0.8,0.8))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2020-09-12-adjusting-for-covariates-under-non-proportional-hazards_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coxph(Surv(OS, OS.EVENT) ~ TRT01P, data = dat_poplar)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## coxph(formula = Surv(OS, OS.EVENT) ~ TRT01P, data = dat_poplar)
## 
##                    coef exp(coef) se(coef)      z       p
## TRT01PMPDL3280A -0.3928    0.6752   0.1426 -2.754 0.00589
## 
## Likelihood ratio test=7.64  on 1 df, p=0.005724
## n= 287, number of events= 200&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coxph(Surv(OS, OS.EVENT) ~ TRT01P + ECOGGR, data = dat_poplar)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## coxph(formula = Surv(OS, OS.EVENT) ~ TRT01P + ECOGGR, data = dat_poplar)
## 
##                    coef exp(coef) se(coef)      z       p
## TRT01PMPDL3280A -0.3770    0.6859   0.1426 -2.644 0.00819
## ECOGGR           0.4501    1.5684   0.1587  2.836 0.00457
## 
## Likelihood ratio test=16.18  on 2 df, p=0.000306
## n= 287, number of events= 200&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case the z-value from the unadjusted Cox model is -2.75, whereas adjusting for ECOGGR it’s -2.644. Let’s look at the un-stratified and stratified modestly-weighted logrank tests (&lt;span class=&#34;math inline&#34;&gt;\(t^* = 12\)&lt;/span&gt;)…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## unstratified
swlrt::wlrt(df = dat_poplar,
            trt_colname = &amp;quot;TRT01P&amp;quot;,
            time_colname = &amp;quot;OS&amp;quot;,
            event_colname = &amp;quot;OS.EVENT&amp;quot;,
            wlr = &amp;quot;mw&amp;quot;,
            t_star = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           u      v_u         z o_minus_e_trt
## 1 -36.83455 134.8164 -3.172372     MPDL3280A&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## stratified
swlrt::swlrt(df = dat_poplar,
             trt_colname = &amp;quot;TRT01P&amp;quot;,
             time_colname = &amp;quot;OS&amp;quot;,
             event_colname = &amp;quot;OS.EVENT&amp;quot;,
             strat_colname = &amp;quot;ECOGGR&amp;quot;,
             wlr = &amp;quot;mw&amp;quot;,
             t_star = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $by_strata
##                 u       v_u         z o_minus_e_trt
## ECOGGR0 -12.98698  27.30791 -2.485215     MPDL3280A
## ECOGGR1 -20.73634 114.79982 -1.935359     MPDL3280A
## 
## $z
## [1] -2.828926&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Same pattern here, but with lower z-values, as you’d expect given the delayed effect.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-the-different-behaviour-oak-vs-poplar&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why the different behaviour: OAK vs POPLAR?&lt;/h3&gt;
&lt;p&gt;This is partly explained from looking at the KM curves by ECOG grade in the two studies. In OAK there is a very big difference in surival (ECOG 1 vs ECOG 0) with the median more-or-less doubling…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;km_oak_ecog &amp;lt;- survfit(Surv(OS, OS.EVENT) ~ ECOGGR,
                       data = dat_oak)

survminer::ggsurvplot(km_oak_ecog, 
                      data = dat_oak, 
                      risk.table = TRUE, 
                      break.x.by = 6,
                      legend.title = &amp;quot;&amp;quot;,
                      xlab = &amp;quot;Time (months)&amp;quot;,
                      ylab = &amp;quot;Overall survival&amp;quot;,
                      risk.table.fontsize = 4,
                      legend = c(0.8,0.8))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2020-09-12-adjusting-for-covariates-under-non-proportional-hazards_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;…whereas for POPLAR there is still a clear difference but median only increased by about 33%…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;km_poplar_ecog &amp;lt;- survfit(Surv(OS, OS.EVENT) ~ ECOGGR,
                          data = dat_poplar)


survminer::ggsurvplot(km_poplar_ecog, 
                      data = dat_poplar, 
                      risk.table = TRUE, 
                      break.x.by = 6,
                      legend.title = &amp;quot;&amp;quot;,
                      xlab = &amp;quot;Time (months)&amp;quot;,
                      ylab = &amp;quot;Overall survival&amp;quot;,
                      risk.table.fontsize = 4,
                      legend = c(0.8,0.8))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2020-09-12-adjusting-for-covariates-under-non-proportional-hazards_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;This is just a couple of quick examples, but I think it’s safe to say that if you believe you have a strong prognostic covariate and a delayed treatment effect, then a stratified modestly-weighted logrank test is likely to be a good option in terms of type 1 error and power.&lt;/p&gt;
&lt;p&gt;A lot of work has been done on covariate adjustment. Clearly, it’s a complex discussion but I think the consensus is that you are likely to gain more than you lose. A few references I’ve found useful:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0197245697001475&#34; class=&#34;uri&#34;&gt;https://www.sciencedirect.com/science/article/pii/S0197245697001475&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1047279705003248&#34; class=&#34;uri&#34;&gt;https://www.sciencedirect.com/science/article/pii/S1047279705003248&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://trialsjournal.biomedcentral.com/articles/10.1186/1745-6215-15-139&#34; class=&#34;uri&#34;&gt;https://trialsjournal.biomedcentral.com/articles/10.1186/1745-6215-15-139&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1002/sim.5713&#34; class=&#34;uri&#34;&gt;https://onlinelibrary.wiley.com/doi/full/10.1002/sim.5713&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The stratified weighted log-rank test has its limits. It can’t handle continuous covariates, and it’s going to break down when the number of strata becomes too large.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Bayesian approach to non-proportional hazards</title>
      <link>/post/a-bayesian-approach-to-non-proportional-hazards/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/a-bayesian-approach-to-non-proportional-hazards/</guid>
      <description>


&lt;p&gt;In this blogpost I wanted to explore a Bayesian approach to non-proportional hazards. Take this data set as an example (the data is &lt;a href=&#34;https://github.com/dominicmagirr/bayesian_survival&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(survival)
library(brms)
##########################
dat &amp;lt;- read_csv(&amp;quot;IPD_both.csv&amp;quot;) %&amp;gt;% 
  mutate(arm = factor(arm))

km_est&amp;lt;-survfit(Surv(time,event)~arm, data=dat)
p1 &amp;lt;- survminer::ggsurvplot(km_est, 
                            data = dat, 
                            risk.table = TRUE, 
                            break.x.by = 6,
                            legend.labs = c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;),
                            legend.title = &amp;quot;&amp;quot;,
                            xlab = &amp;quot;Time (months)&amp;quot;,
                            ylab = &amp;quot;Overall survival&amp;quot;,
                            risk.table.fontsize = 4,
                            legend = c(0.8,0.8))

p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2020-07-27-a-bayesian-approach-to-non-proportional-hazards_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like there is a trade-off between short-term survival and long-term survival on the two treatments. But is the apparent long term benefit real? How sure are we? That is my main question of interest here: does treatment “1” improve long term survival?&lt;/p&gt;
&lt;p&gt;Probably the best tool for flexible parametric Bayesian survival analysis is now the rstanarm package (&lt;a href=&#34;https://arxiv.org/abs/2002.09633&#34; class=&#34;uri&#34;&gt;https://arxiv.org/abs/2002.09633&lt;/a&gt;). This looks awesome. Unfortunately, I only have access to my work computer at the moment which doesn’t have the latest version installed. Instead I’ll be using the brms package – which is also excellent.&lt;/p&gt;
&lt;p&gt;Before I can fit a piece-wise exponential model (with changepoints every 6 months), I need to use a little trick (the survSplit function) to change my covariates into time-dependent ones – this kind of thing is explained &lt;a href=&#34;https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## change into time-dependent data set 
dat_td &amp;lt;- survSplit(Surv(time, event) ~ arm,
                   data = dat,
                   cut = c(6,12,18,24),
                   episode = &amp;quot;period&amp;quot;) %&amp;gt;% 
  mutate(censored = as.numeric(!event),
         period = factor(period))

## fit Bayesian model
fit1 &amp;lt;- brm(formula = time | cens(censored) + trunc(lb = tstart) ~ arm * period,
            data = dat_td,
            family = exponential(),
            inits = &amp;quot;0&amp;quot;,
            refresh = 0,
            seed = 593)

summary(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: exponential 
##   Links: mu = log 
## Formula: time | cens(censored) + trunc(lb = tstart) ~ arm * period 
##    Data: dat_td (Number of observations: 1813) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept        2.89      0.10     2.69     3.10 1.00     3215     3119
## arm2             0.23      0.15    -0.07     0.53 1.00     2954     3237
## period2          0.11      0.16    -0.20     0.43 1.00     2821     2525
## period3          0.68      0.23     0.24     1.14 1.00     3194     2944
## period4          0.34      0.25    -0.15     0.85 1.00     3074     2596
## period5          0.06      0.32    -0.52     0.72 1.00     3176     2579
## arm2:period2    -0.48      0.23    -0.93    -0.04 1.00     2729     2777
## arm2:period3    -0.92      0.30    -1.53    -0.33 1.00     2949     3065
## arm2:period4    -0.54      0.35    -1.22     0.14 1.00     2968     2572
## arm2:period5    -0.08      0.44    -0.95     0.80 1.00     3404     2814
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To explain where these parameters fit in mathematically, the probability of surviving to time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; on arm &lt;span class=&#34;math inline&#34;&gt;\(j=1,2\)&lt;/span&gt; is…&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{split}
S_j(t) = &amp;amp; \exp\left(\frac{-t}{\exp(\mu_{j,1})}\right), &amp;amp; ~~t \leq 6 \\
= &amp;amp; \exp\left(\frac{-6}{\exp(\mu_{j,1})}\right)\exp\left(\frac{-(t-6)}{\exp(\mu_{j,2})}\right), &amp;amp;~~ 6 &amp;lt; t \leq 12 \\
 =&amp;amp; \exp\left(\frac{-6}{\exp(\mu_{j,1})}\right)\exp\left(\frac{-(12-6)}{\exp(\mu_{j,2})}\right)\exp\left(\frac{-(t-12)}{\exp(\mu_{j,3})}\right), &amp;amp; ~~12 &amp;lt; t \leq 18 \\
  = &amp;amp;...&amp;amp;
\end{split}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{split}
\mu_{1,1} = &amp;amp; \text{Intercept}\\
\mu_{1,2} = &amp;amp; \text{Intercept} + \text{period2}\\
\mu_{1,3} = &amp;amp; \text{Intercept} + \text{period3}\\
... &amp;amp; 
\end{split}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{split}
\mu_{2,1} = &amp;amp; \text{Intercept} + \text{arm2}\\
\mu_{2,2} = &amp;amp; \text{Intercept} + \text{arm2} + \text{period2} + \text{arm2:period2}\\
\mu_{2,3} = &amp;amp; \text{Intercept} + \text{arm2} + \text{period3} + \text{arm2:period3}\\
... &amp;amp; 
\end{split}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To generate posterior samples of these survival probabilities, I need to take the posterior samples of the model parameters, and then perform these transformations. Apologies for the ugly piece of code here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## function to turn posterior samples 
## of model parameters (&amp;quot;ps = posterior_samples(fit1)&amp;quot;)
## into posterior samples of S(t)
get_s &amp;lt;- function(t, ps, arm = &amp;quot;1&amp;quot;, changepoints = c(6,12,18,24)){
  
  
  ### Extract the scale parameters from posterior samples:
  log_scales_1 &amp;lt;- matrix(NA, 
                         nrow = length(ps[[1]]),
                         ncol = length(changepoints) + 1)
  log_scales_2 &amp;lt;- matrix(NA, 
                         nrow = length(ps[[1]]),
                         ncol = length(changepoints) + 1)
  
  log_scales_1[,1] &amp;lt;- ps$b_Intercept
  log_scales_2[,1] &amp;lt;- ps$b_Intercept + ps$b_arm2
  
  for (i in (1 + seq_along(changepoints))){
    log_scales_1[,i] &amp;lt;- ps$b_Intercept + ps[[paste0(&amp;quot;b_period&amp;quot;,i)]]
    log_scales_2[,i] &amp;lt;- ps$b_Intercept + ps[[paste0(&amp;quot;b_period&amp;quot;,i)]] +
      ps$b_arm2 + ps[[paste0(&amp;quot;b_arm2:period&amp;quot;,i)]]
  }
  
  scales_1 &amp;lt;- exp(log_scales_1)
  scales_2 &amp;lt;- exp(log_scales_2)
  
  ### Piece-wise exponential survival function:
  changepoints_Inf &amp;lt;- c(changepoints, Inf)
  
  if(arm == 1){
    p &amp;lt;- exp(-min(t, changepoints[1]) / scales_1[,1])
    for (i in which(changepoints &amp;lt; t)){
      p &amp;lt;- p * exp(-(min(t, changepoints_Inf[i + 1]) - changepoints[i]) / scales_1[,i + 1])
    }
    return(p)
  }
  else {
    p &amp;lt;- exp(-min(t, changepoints[1]) / scales_2[,1])
    for (i in which(changepoints &amp;lt; t)){
      p &amp;lt;- p * exp(-(min(t, changepoints_Inf[i + 1]) - changepoints[i]) / scales_2[,i + 1])
    }
    return(p)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, I can take the posterior samples for the survival probabilities at each month, calculate the posterior means, and see how well this matches the K-M plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ps &amp;lt;- posterior_samples(fit1)
t_seq &amp;lt;- seq(0, 36, 1)

s_1 &amp;lt;- purrr::map(t_seq, get_s, ps = ps)
s_2 &amp;lt;- purrr::map(t_seq, get_s, ps = ps, arm = &amp;quot;2&amp;quot;)

df_sims &amp;lt;- data.frame(time = t_seq,
                      mean_1 = purrr::map_dbl(s_1, mean),
                      mean_2 = purrr::map_dbl(s_2, mean))

p1$plot + 
  geom_line(data = df_sims,
                    mapping = aes(x = time, y = mean_1), colour = &amp;quot;red&amp;quot;) +
  geom_line(data = df_sims,
            mapping = aes(x = time, y = mean_2), colour = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2020-07-27-a-bayesian-approach-to-non-proportional-hazards_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now suppose I’m interested in the difference in survival probabilities at 24 months, &lt;span class=&#34;math inline&#34;&gt;\(S_1(24) - S_2(24)\)&lt;/span&gt;. I can make a 95% credible interval:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff_24 &amp;lt;- get_s(24, ps) - get_s(24, ps, arm = &amp;quot;2&amp;quot;)
quantile(diff_24, probs = c(0.025, 0.975)) %&amp;gt;% round(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  2.5% 97.5% 
##  0.01  0.16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But I hadn’t pre-specified 24 months. I might just as well have been interested in the difference at 12,18 or 30 months:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff_12 &amp;lt;- get_s(12, ps) - get_s(12, ps, arm = &amp;quot;2&amp;quot;)
quantile(diff_12, probs = c(0.025, 0.975)) %&amp;gt;% round(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  2.5% 97.5% 
## -0.07  0.08&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff_18 &amp;lt;- get_s(18, ps) - get_s(18, ps, arm = &amp;quot;2&amp;quot;)
quantile(diff_18, probs = c(0.025, 0.975)) %&amp;gt;% round(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  2.5% 97.5% 
##  0.00  0.15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff_30 &amp;lt;- get_s(30, ps) - get_s(30, ps, arm = &amp;quot;2&amp;quot;)
quantile(diff_30, probs = c(0.025, 0.975)) %&amp;gt;% round(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  2.5% 97.5% 
## -0.03  0.13&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How should we interpret these 95% credible intervals, where 2 out of 4 just about exclude 0? Borderline convincing? But hang on… when I view &lt;span class=&#34;math inline&#34;&gt;\((-0.07,0.08)\times (0,0.15)\times (0.01,0.16) \times(-0.03, 0.13)\)&lt;/span&gt; as a credible region for the differences at 12,18,24 and 30 months, this has far less than 95% posterior probability:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(diff_12 &amp;gt; -0.07 &amp;amp; diff_12 &amp;lt; 0.08 &amp;amp;
     diff_18 &amp;gt; 0 &amp;amp; diff_18 &amp;lt; 0.15 &amp;amp; 
     diff_24 &amp;gt; 0.01 &amp;amp; diff_24 &amp;lt; 0.16 &amp;amp;
     diff_30 &amp;gt; -0.03 &amp;amp; diff_30 &amp;lt; 0.13)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.86025&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get a 95% credible region, I have to expand the individual credible intervals a bit (via trial and error)…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(diff_12, probs = c(0.008, 0.992)) %&amp;gt;% round(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  0.8% 99.2% 
## -0.08  0.10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(diff_18, probs = c(0.008, 0.992)) %&amp;gt;% round(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  0.8% 99.2% 
## -0.02  0.17&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(diff_24, probs = c(0.008, 0.992)) %&amp;gt;% round(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  0.8% 99.2% 
## -0.01  0.18&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(diff_30, probs = c(0.008, 0.992)) %&amp;gt;% round(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  0.8% 99.2% 
## -0.04  0.15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(diff_12 &amp;gt; -0.08 &amp;amp; diff_12 &amp;lt; 0.10 &amp;amp;
     diff_18 &amp;gt; -0.02 &amp;amp; diff_18 &amp;lt; 0.17 &amp;amp; 
     diff_24 &amp;gt; -0.01 &amp;amp; diff_24 &amp;lt; 0.18 &amp;amp;
     diff_30 &amp;gt; -0.04 &amp;amp; diff_30 &amp;lt; 0.15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9495&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How should we interpret this 95% credible region? None of timepoints quite manage to exclude zero. Borderline unconvincing?&lt;/p&gt;
&lt;p&gt;Another perspective is that treatment “1” is efficacious if there is at least one timepoint where the survival probability is higher than on treatment “2”. The probability that this is the case is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(diff_12 &amp;gt; 0 | diff_18 &amp;gt; 0 | diff_24 &amp;gt; 0 | diff_30 &amp;gt; 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.99625&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well over a 99% chance. Highly convincing evidence!&lt;/p&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Maybe I’m overthinking things here, but for me fitting a nice Bayesian model is only half the job done. We also need a good way to describe the (multivariate) posterior distribution. Of course, all three of these interpretations are valid given the prior distribution and model assumptions (I skipped over discussing the prior distribution here). But are these three summaries not superficially quite similar, yet yielding slightly different (perhaps importantly so) conclusions? Are we really prepared to explain these differences to our clinical colleagues, patients, regulators, payers? If not, is this still intellectually superior to a frequentist analysis? I don’t know the answers to these questions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Non-proportional hazards in immuno-oncology: is an old perspective needed?</title>
      <link>/post/non-proportional-hazards-in-immuno-oncology-is-an-old-perspective-needed/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/non-proportional-hazards-in-immuno-oncology-is-an-old-perspective-needed/</guid>
      <description>


&lt;p&gt;In my opinion, many phase III trials in immuno-oncology are 10–20 % larger than they need (ought) to be.&lt;/p&gt;
&lt;p&gt;This is because the method we use for the primary analysis doesn’t match what we know about how these drugs work.&lt;/p&gt;
&lt;p&gt;Fixing this doesn’t require anything fancy, just old-school stats from the 1960s.&lt;/p&gt;
&lt;p&gt;In this &lt;a href=&#34;https://arxiv.org/abs/2007.04767&#34;&gt;new preprint&lt;/a&gt; I try to explain how I think it should be done.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Shiny app for enhancing published Kaplan-Meier plots</title>
      <link>/post/shiny-app-for-enhancing-published-kaplan-meier-plots/</link>
      <pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/shiny-app-for-enhancing-published-kaplan-meier-plots/</guid>
      <description>


&lt;p&gt;Building on &lt;a href=&#34;../post/extract-patient-level-data-from-a-kaplan-meier-plot&#34;&gt;my last post,&lt;/a&gt; I decided to build a &lt;a href=&#34;https://dominicmagirr.shinyapps.io/enhanceKM/&#34;&gt;shiny app&lt;/a&gt; to make it much easier to extract (approximate) patient-level data from a Kaplan-Meier curve. &lt;a href=&#34;https://youtu.be/sT31vTbK90A&#34;&gt;Video here&lt;/a&gt; and &lt;a href=&#34;https://github.com/dominicmagirr/enhanceKM&#34;&gt;code here.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Extract patient-level data from a Kaplan-Meier plot</title>
      <link>/post/extract-patient-level-data-from-a-kaplan-meier-plot/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/extract-patient-level-data-from-a-kaplan-meier-plot/</guid>
      <description>


&lt;p&gt;I made a &lt;a href=&#34;https://youtu.be/eHuQlIDRXxs&#34;&gt;video&lt;/a&gt; with step-by-step instructions for extracting (approximate) patient-level data from a Kaplan-Meier curve. It’s a useful thing to be able to do, and while there are plenty of references already (see below) I thought perhaps a full guide using completely free software was missing.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Guyot, P., Ades, A., Ouwens, M.J. et al. Enhanced secondary analysis of survival data: reconstructing the data from published Kaplan-Meier survival curves. BMC Med Res Methodol 12, 9 (2012). &lt;a href=&#34;https://doi.org/10.1186/1471-2288-12-9&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1186/1471-2288-12-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wei, Y., &amp;amp; Royston, P. (2017). Reconstructing Time-to-event Data from Published Kaplan–Meier Curves. The Stata Journal, 17(4), 786–802. &lt;a href=&#34;https://doi.org/10.1177/1536867X1801700402&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/1536867X1801700402&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Author: Ankit Rohatgi
Title: WebPlotDigitizer
Website: &lt;a href=&#34;https://automeris.io/WebPlotDigitizer&#34; class=&#34;uri&#34;&gt;https://automeris.io/WebPlotDigitizer&lt;/a&gt;
Version: 4.2
Date: April, 2019
E-Mail: &lt;a href=&#34;mailto:ankitrohatgi@hotmail.com&#34; class=&#34;email&#34;&gt;ankitrohatgi@hotmail.com&lt;/a&gt;
Location: San Francisco, California, USA&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/giabaio/survHE/blob/master/R/digitise.R&#34; class=&#34;uri&#34;&gt;https://github.com/giabaio/survHE/blob/master/R/digitise.R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Satagopan, Jaya M., Alexia Iasonos, and Joseph G. Kanik. “A reconstructed melanoma data set for evaluating differential treatment benefit according to biomarker subgroups.” Data in brief 12 (2017): 667-675.
&lt;a href=&#34;https://doi.org/10.1016/j.dib.2017.05.005&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.dib.2017.05.005&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Is there still a place for significance testing in clinical trials?</title>
      <link>/post/is-there-still-a-place-for-significance-testing-in-clinical-trials/</link>
      <pubDate>Sun, 26 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/is-there-still-a-place-for-significance-testing-in-clinical-trials/</guid>
      <description>


&lt;p&gt;I’ve spent a lot of time thinking about hypothesis tests in clinical trials recently. Periodically, it’s good to question the fundamentals of whether this is a good idea in general. I don’t think my views have changed. I was considering writing a blog post but then I came across &lt;a href=&#34;https://ora.ox.ac.uk/objects/uuid:95a357f2-5ba2-4563-aeb6-ccdfdc031360/download_file?file_format=pdf&amp;amp;safe_filename=Cook_etal_2019_There_is_still_a_place.pdf&amp;amp;type_of_work=Journal+article&#34;&gt;this article&lt;/a&gt; by the editors of the Clinical Trials journal which pretty much sums it up. The article can also be found with DOI: 10.1177/1740774519846504.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>flexsurv 2</title>
      <link>/post/flexsurv-2/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/post/flexsurv-2/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../post/flexsurv&#34;&gt;Previously,&lt;/a&gt; I started discussing the &lt;code&gt;flexsurv&lt;/code&gt; package. I used it to fit a Weibull model. This is implemented as an accelerated failure time model. It is also a proportional hazards model (although, as I found previously, converting between the two is not so straightforward, but it can be done by &lt;code&gt;SurvRegCensCov&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Now let’s compare Weibull regression with Cox regression. Firstly, Weibull regression:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assumes proportional hazards;&lt;/li&gt;
&lt;li&gt;the number of parameters is equal to &lt;span class=&#34;math inline&#34;&gt;\(k + 2\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the number of covariates;&lt;/li&gt;
&lt;li&gt;we can estimate things like the median, &lt;span class=&#34;math inline&#34;&gt;\(P(S&amp;gt;s^*)\)&lt;/span&gt;, etc. from the model…&lt;/li&gt;
&lt;li&gt;but the model might be too restrictive – we won’t estimate these things very well.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cox regression:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assumes proportional hazards;&lt;/li&gt;
&lt;li&gt;there are &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; parameters (one for each covariate);&lt;/li&gt;
&lt;li&gt;we don’t estimate the baseline hazard…&lt;/li&gt;
&lt;li&gt;which means we don’t get estimates for things like the median, &lt;span class=&#34;math inline&#34;&gt;\(P(S&amp;gt;s^*)\)&lt;/span&gt;, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;flexsurv&lt;/code&gt; has a function &lt;code&gt;flexsurvspline&lt;/code&gt; which allows one to bridge the gap between Weibull regression and Cox regresssion.&lt;/p&gt;
&lt;div id=&#34;from-weibull-regression-to-cox-regression.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;From Weibull regression to Cox regression.&lt;/h2&gt;
&lt;p&gt;For a Weibull distribution with shape &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, scale &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, covariates &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, and regression coefficents &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;, the survival probability is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S(t; x) = \exp\left\lbrace - \left( \frac{t}{b \cdot \exp(x^T\gamma)} \right) ^ a\right\rbrace\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since survival is related to the log-cumulative hazard via &lt;span class=&#34;math inline&#34;&gt;\(S=\exp(-H)\)&lt;/span&gt;, this means that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log H(t;x) = a\log t - a\log(b) - a x^T\gamma\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In words, the log-cumulative hazard has a linear relationship with (log-) time, with the intercept depending on the value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. For a given data set, we can check if this is reasonable by looking at non-paramteric estimates, &lt;span class=&#34;math inline&#34;&gt;\(\log \hat{H}(t; x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(survival)
library(flexsurv)
library(ggplot2)

### Non-parametric analysis
fit_km &amp;lt;- survfit(Surv(futime, death) ~ trt,
                  data = myeloid)
t_seq &amp;lt;- seq(1, 2500, length.out = 1000)
km_sum &amp;lt;- summary(fit_km, times = t_seq, extend = TRUE)

### Weibull regression
fit_weibull &amp;lt;- flexsurvreg(Surv(futime, death) ~ trt, 
                           dist = &amp;quot;weibull&amp;quot;, 
                           data = myeloid)

a &amp;lt;- fit_weibull$res[&amp;quot;shape&amp;quot;, &amp;quot;est&amp;quot;]
b &amp;lt;- fit_weibull$res[&amp;quot;scale&amp;quot;, &amp;quot;est&amp;quot;]
trtB &amp;lt;- fit_weibull$res[&amp;quot;trtB&amp;quot;, &amp;quot;est&amp;quot;]


### plot log-cumulative hazard against log-time
df &amp;lt;- data.frame(log_time = log(rep(t_seq, 2)),
                 logcumhaz = log(-log(km_sum$surv)),
                 trt = rep(c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;), each = 1000),
                 logcumhaz_w = c(a * (log(t_seq) - log(b)),
                                 a * (log(t_seq) - log(b) - trtB)))

ggplot(data = df,
       mapping = aes(x = log_time,
                     y = logcumhaz,
                     colour = trt)) +
  geom_line() +
  geom_line(mapping = aes(x = log_time,
                          y = logcumhaz_w,
                          colour = trt),
            linetype = 2) +
  theme_bw() +
  scale_x_continuous(limits = c(2,8)) +
  scale_y_continuous(limits = c(-6,0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2019-11-01-flexsurv-2_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What’s going on here? Well, the first thing to acknowledge is that the hazards only appear to be proportional after about 150 (&lt;span class=&#34;math inline&#34;&gt;\(e^5\)&lt;/span&gt;) days. I’m not sure I would immediately abandon a proportional-hazards model, though, as most of the events happen when the hazards are proportional (only 10-15% of the events happen before day 150), so the right-hand-side of the plot is far more important. Looking to the right then: the relationship between the log-cumulative hazard and log-time is not really linear. The distance between the two lines is roughly the same for the two models (Weibull and non-parametric), suggesting that the Weibull model does ok at estimating the hazard ratio. However, the lack of linearity will lead to poor estimates for the medians, &lt;span class=&#34;math inline&#34;&gt;\(P(S&amp;gt;s^*)\)&lt;/span&gt;, etc., as can be confirmed by plotting the survival curves:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = df,
       mapping = aes(x = exp(log_time),
                     y = exp(-exp(logcumhaz)),
                     colour = trt)) +
  geom_line() +
  geom_line(mapping = aes(x = exp(log_time),
                          y = exp(-exp(logcumhaz_w)),
                          colour = trt),
            linetype = 2) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2019-11-01-flexsurv-2_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To improve the model, given this lack of linearity, it seems quite natural to change from&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log H(t;x) = a\log t - a\log(b) - a x^T\gamma\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log H(t;x) = s(\log t) + x^T\beta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s(\log t)\)&lt;/span&gt; is a natural cubic spline function of (log) time. One can make the model more/less flexible by choosing a large/small number of knots. By default, the knots are placed at quantiles of the uncensored event times. How many knots are required? I don’t really have a good answer for this: one or two. At most, three? In this example, I’m using two inner knots, placed at 33% and 66% of the uncensored event times (indicated by vertical lines):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_spline_2 &amp;lt;- flexsurvspline(Surv(futime, death) ~ trt, 
                               data = myeloid,
                               k = 2,
                               scale = &amp;quot;hazard&amp;quot;)

spline_2_sum &amp;lt;- summary(fit_spline_2, t = t_seq, type = &amp;quot;cumhaz&amp;quot;)

df2 &amp;lt;- cbind(df,
      data.frame(logcumhaz_s2 = log(c(spline_2_sum$`trt=A`[&amp;quot;est&amp;quot;]$est,
                                  spline_2_sum$`trt=B`[&amp;quot;est&amp;quot;]$est))))

ggplot(data = df2,
       mapping = aes(x = log_time,
                     y = logcumhaz,
                     colour = trt)) +
  geom_line() +
  geom_line(mapping = aes(x = log_time,
                          y = logcumhaz_s2,
                          colour = trt),
            linetype = 2) +
  theme_bw() +
  scale_x_continuous(limits = c(2,8)) +
  scale_y_continuous(limits = c(-6,0)) +
  geom_vline(xintercept = fit_spline_2$knots)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2019-11-01-flexsurv-2_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks a lot better, and we can see the improvement in the survival curves:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = df2,
       mapping = aes(x = exp(log_time),
                     y = exp(-exp(logcumhaz)),
                     colour = trt)) +
  geom_line() +
  geom_line(mapping = aes(x = exp(log_time),
                          y = exp(-exp(logcumhaz_s2)),
                          colour = trt),
            linetype = 2) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2019-11-01-flexsurv-2_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;from-cox-regression-to-weibull-regression.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;From Cox regression to Weibull regression.&lt;/h2&gt;
&lt;p&gt;If we start out from Cox regression&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[h(t;x)=h_0(t)\exp(x^T\beta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;this means that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log H(t;x) = \log H_0(t;x) + x^T\beta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We estimate the parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; from the partial likelihood, and don’t estimate &lt;span class=&#34;math inline&#34;&gt;\(\log H_0(t;x)\)&lt;/span&gt;. So &lt;span class=&#34;math inline&#34;&gt;\(\log H_0(t;x)\)&lt;/span&gt; can be anything. However, with the &lt;code&gt;flexsurvspline&lt;/code&gt; function, as long as we use enough knots, &lt;span class=&#34;math inline&#34;&gt;\(s(\log(t))\)&lt;/span&gt; can be more-or-less anything (smooth), so the two methods will give the same information about &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;I can’t really see any reason not to switch from a Cox model to &lt;code&gt;flexsurvspline&lt;/code&gt;. You don’t lose anything in terms of inference on &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, only gain a nice estimate for the baseline hazard. Also, inference is all based on maximum likelihood. No special theory required.&lt;/p&gt;
&lt;p&gt;From the other side, if you start out from Weibull regression, and then realise that Weibull is the wrong model, you don’t have to think too hard about how to choose a better model, you &lt;em&gt;know&lt;/em&gt; that &lt;code&gt;flexsurvspline&lt;/code&gt; will be good (assuming proportional hazards is correct: for non-proportional hazards you may have to think harder).&lt;/p&gt;
&lt;p&gt;What can go wrong? In small sample sizes, I guess there could be issues with over-fitting if too many knots are chosen. But given a decent sample size, I can’t see any problems. I would be interested to see a &lt;span class=&#34;math inline&#34;&gt;\(\log H_0(t;x)\)&lt;/span&gt; that is highly wiggly – doesn’t seem likely in practice.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>flexsurv</title>
      <link>/post/flexsurv/</link>
      <pubDate>Mon, 28 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/flexsurv/</guid>
      <description>


&lt;p&gt;I’m going to write about some of my favourite R packages. I’ll start with &lt;code&gt;flexsurv&lt;/code&gt; (&lt;a href=&#34;https://github.com/chjackson/flexsurv-dev&#34; class=&#34;uri&#34;&gt;https://github.com/chjackson/flexsurv-dev&lt;/a&gt;) by Chris Jackson, which can be used to fit all kinds of parametric models to survival data. It can really do a lot, but I’ll pick out just 2 cool things I like about it:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fit a standard survival model, but where it’s &lt;em&gt;slightly&lt;/em&gt; easier to work out what the parameters mean.&lt;/li&gt;
&lt;li&gt;Fit a proportional hazards model, which is a lot like a Cox model, but where you also model the baseline hazard using a spline.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;consistent-parameter-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Consistent parameter values&lt;/h2&gt;
&lt;p&gt;As mentioned in the tutorial (&lt;a href=&#34;https://www.jstatsoft.org/article/view/v070i08&#34; class=&#34;uri&#34;&gt;https://www.jstatsoft.org/article/view/v070i08&lt;/a&gt;), for simple models &lt;code&gt;flexsurvreg&lt;/code&gt; acts as a wrapper for &lt;code&gt;survival::survreg&lt;/code&gt;, but where the parameters in the output match those of &lt;code&gt;dweibull&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;survival::survreg&lt;/code&gt; I would do, e.g.:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_ovarian &amp;lt;- survival::ovarian
dat_ovarian$rx &amp;lt;- factor(dat_ovarian$rx)

library(survival)
fit_survreg = survreg(Surv(futime, fustat) ~ rx, 
                      dist = &amp;quot;weibull&amp;quot;,
                      data = dat_ovarian)

summary(fit_survreg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## survreg(formula = Surv(futime, fustat) ~ rx, data = dat_ovarian, 
##     dist = &amp;quot;weibull&amp;quot;)
##              Value Std. Error     z      p
## (Intercept)  6.825      0.344 19.84 &amp;lt;2e-16
## rx2          0.559      0.529  1.06   0.29
## Log(scale)  -0.121      0.251 -0.48   0.63
## 
## Scale= 0.886 
## 
## Weibull distribution
## Loglik(model)= -97.4   Loglik(intercept only)= -98
##  Chisq= 1.18 on 1 degrees of freedom, p= 0.28 
## Number of Newton-Raphson Iterations: 5 
## n= 26&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I would look around online for an explanation of the output e.g. (&lt;a href=&#34;https://stats.stackexchange.com/questions/159044/weibull-survival-model-in-r&#34; class=&#34;uri&#34;&gt;https://stats.stackexchange.com/questions/159044/weibull-survival-model-in-r&lt;/a&gt;). There is also an explanation in &lt;code&gt;?survreg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand, using &lt;code&gt;flexsurv&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(flexsurv)
fit_flexsurvreg = flexsurvreg(Surv(futime, fustat) ~ rx, 
                              dist = &amp;quot;weibull&amp;quot;,
                              data = dat_ovarian)

fit_flexsurvreg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## flexsurvreg(formula = Surv(futime, fustat) ~ rx, data = dat_ovarian, 
##     dist = &amp;quot;weibull&amp;quot;)
## 
## Estimates: 
##        data mean  est       L95%      U95%      se        exp(est)
## shape        NA      1.129     0.690     1.848     0.284        NA
## scale        NA    920.128   468.868  1805.704   316.508        NA
## rx2       0.500      0.559    -0.478     1.597     0.529     1.749
##        L95%      U95%    
## shape        NA        NA
## scale        NA        NA
## rx2       0.620     4.936
## 
## N = 26,  Events: 12,  Censored: 14
## Total time at risk: 15588
## Log-likelihood = -97.36415, df = 3
## AIC = 200.7283&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The parameters &lt;code&gt;shape&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt; correspond to &lt;code&gt;dweibull&lt;/code&gt;. So I don’t have to think any further? Not quite: I still have to work out what the estimate of &lt;code&gt;rx2&lt;/code&gt; is doing. I might look at &lt;code&gt;exp(est) = 1.749&lt;/code&gt; and somehow expect this to be a hazard ratio. It’s not. It’s a multiplicative effect on the scale parameter. So when &lt;code&gt;rx = 1&lt;/code&gt; the scale is &lt;code&gt;920.1&lt;/code&gt;, and when &lt;code&gt;rx = 2&lt;/code&gt; the scale is &lt;code&gt;920.1 * 1.749&lt;/code&gt;. The hazard ratio (treatment 2 vs treatment 1) is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} \frac{h_2(x)}{h_1(x)} &amp;amp; = \left( \frac{b_1}{b_2} \right)^a \\
                                      &amp;amp; = \left( \frac{920.1}{920.1 \times 1.749} \right)^{1.129}\\
                                      &amp;amp; = 0.53 \end{align} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the common shape parameter, and &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt; are the scale parameters.&lt;/p&gt;
&lt;p&gt;Once I had started writing this post, I realized that it’s actually not straightforward to make inference on the hazard ratio using &lt;code&gt;flexsurv&lt;/code&gt;. For working out variances/covariances, the &lt;code&gt;survreg&lt;/code&gt; parameterization is indeed better. I looked around for other R packages in this space, and found &lt;code&gt;SurvRegCensCov&lt;/code&gt;, which can do this conversion automatically for you:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(SurvRegCensCov)
ConvertWeibull(fit_survreg)$HR&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            HR        LB       UB
## rx2 0.5318051 0.1683444 1.679989&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For completeness, using &lt;code&gt;flexsurv&lt;/code&gt;, the log-hazard ratio is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} \log\left( \frac{h_2(x)}{h_1(x) }\right) &amp;amp; = a\left\lbrace \log(b_1) - \log(b_2) \right\rbrace \\
                                                         &amp;amp; = -\exp(\log(a)) \times \left\lbrace \log(b_2) - \log(b_1)  \right\rbrace \end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I can extract the terms &lt;span class=&#34;math inline&#34;&gt;\(\alpha:=\log(a)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta:=\log(b_2) - \log(b_1)\)&lt;/span&gt; from the &lt;code&gt;fit_flexsurvreg&lt;/code&gt; object, as well as their (co)variance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alpha &amp;lt;- fit_flexsurvreg$res.t[&amp;quot;shape&amp;quot;, &amp;quot;est&amp;quot;]
beta &amp;lt;- fit_flexsurvreg$res.t[&amp;quot;rx2&amp;quot;, &amp;quot;est&amp;quot;]
cov_alpha_beta &amp;lt;- vcov(fit_flexsurvreg)[c(&amp;quot;shape&amp;quot;, &amp;quot;rx2&amp;quot;), c(&amp;quot;shape&amp;quot;, &amp;quot;rx2&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then work out the variance of the log-hazard ratio using the delta method.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{var}\left\lbrace -\beta\exp(\alpha) \right\rbrace = (-\beta\exp(\alpha), -\exp(\alpha)) \text{Cov}(\alpha, \beta) \left( \begin{array}{c} -\beta\exp(\alpha) \\ -\exp(\alpha) \end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grad &amp;lt;- matrix(c(-beta*exp(alpha),
                 -exp(alpha)), 
               ncol = 1)
var_lhr = t(grad) %*% cov_alpha_beta %*% grad
se_lhr = sqrt(var_lhr)
se_lhr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]
## [1,] 0.5868807&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And to get a 95% confidence interval for the hazard ratio…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_hr = -exp(alpha) * beta
log_hr_upr = log_hr + qnorm(0.975) * se_lhr
log_hr_lwr = log_hr - qnorm(0.975) * se_lhr

data.frame(HR = exp(log_hr),
           LB = exp(log_hr_lwr),
           UB = exp(log_hr_upr))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          HR        LB       UB
## 1 0.5318051 0.1683444 1.679988&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;splines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Splines&lt;/h2&gt;
&lt;p&gt;The second thing I really like about &lt;code&gt;flexsurv&lt;/code&gt; is the proportional hazards model with a spline for the baseline hazard. I’ll explore this in &lt;a href=&#34;../post/flexsurv-2&#34;&gt;another post&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal hurdle models 3</title>
      <link>/post/longitudinal-hurdle-models-3/</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/longitudinal-hurdle-models-3/</guid>
      <description>


&lt;p&gt;In the &lt;a href=&#34;../post/longitudinal-hurdle-models-2&#34;&gt;last&lt;/a&gt; post on longitudinal hurdle models, I had just taken samples from the marginal mean&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} g(\theta; x) &amp;amp; = E(Y \mid  \theta; x) \\
&amp;amp; = \int   E(Y \mid u_{i&amp;#39;}, v_{i&amp;#39;}, \theta; x) f(u_{i&amp;#39;}, v_{i&amp;#39;} \mid \theta, \mathbf{y}) du_{i&amp;#39;}dv_{i&amp;#39;} \\ 
&amp;amp;\approx  L^{-1}\sum_{l = 1}^{L}E(Y \mid u_{i&amp;#39;}^{(l)}, v_{i&amp;#39;}^{(l)}, \theta; x)\\
&amp;amp;=  L^{-1}\sum_{l = 1}^{L}\left\lbrace 1 - \text{logit}^{-1} ( x^T \gamma + u_{i&amp;#39;}^{(l)}) \right\rbrace \exp(x^T\beta + v_{i&amp;#39;}^{(l)} + \frac{\sigma^2}{2}).\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;One of the issues with lognormal data is that it is highly skewed, so the mean can be very large. In a small sample, the sample mean can change a lot based on just 1 or 2 large observations. For this reason I would like to sample from other summary measures of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;samples-from-py-leq-k-mid-theta-x&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Samples from &lt;span class=&#34;math inline&#34;&gt;\(p(Y \leq k \mid \theta; x)\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;This is quite similar to taking samples from the marginal mean.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} r(\theta; x, k) &amp;amp; = p(Y &amp;lt; k \mid  \theta; x) \\
&amp;amp; = \int   p(Y &amp;lt; k \mid u_{i&amp;#39;}, v_{i&amp;#39;}, \theta; x) f(u_{i&amp;#39;}, v_{i&amp;#39;} \mid \theta, \mathbf{y}) du_{i&amp;#39;}dv_{i&amp;#39;} \\ 
&amp;amp;\approx  L^{-1}\sum_{l = 1}^{L}p(Y &amp;lt; k \mid u_{i&amp;#39;}^{(l)}, v_{i&amp;#39;}^{(l)}, \theta; x)\\
&amp;amp;=  L^{-1}\sum_{l = 1}^{L}\ \left[ \pi^{(l)}(x) + \left\lbrace 1 - \pi^{(l)}(x)\right\rbrace \Phi \left\lbrace \frac{\log(k) - x^T\beta - v_{i&amp;#39;}^{(l)}}{\sigma} \right\rbrace \right].\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\pi^{(l)}(x):= \text{logit}^{-1} ( x^T \gamma + u_{i&amp;#39;}^{(l)})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Again, I don’t know of any functions for doing this, so I built my own.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;dominicmagirr/hurlong&amp;quot;)
library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- data.frame(id = NA, time = &amp;quot;2&amp;quot;)
hurlong::marg_pyk_q(k = 0.5,
                    newdata = x, 
                    nsims = 1000, 
                    fit = fit_hurdle)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   id time     2.5%       50%     97.5%
## 1 NA    2 0.421836 0.5018633 0.5792536&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;samples-from-textmediany-mid-theta-x&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Samples from &lt;span class=&#34;math inline&#34;&gt;\(\text{median}(Y \mid \theta; x)\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Finally, I am interested in samples from the marginal median.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} s(\theta; x) &amp;amp; = \text{median}(Y  \mid  \theta; x) \\
&amp;amp; = \int   \text{median}(Y \mid u_{i&amp;#39;}, v_{i&amp;#39;}, \theta; x) f(u_{i&amp;#39;}, v_{i&amp;#39;} \mid \theta, \mathbf{y}) du_{i&amp;#39;}dv_{i&amp;#39;} \\ 
&amp;amp;\approx  L^{-1}\sum_{l = 1}^{L}\text{median}(Y\mid u_{i&amp;#39;}^{(l)}, v_{i&amp;#39;}^{(l)}, \theta; x) \end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To evaluate &lt;span class=&#34;math inline&#34;&gt;\(\text{median}(Y\mid u_{i&amp;#39;}^{(l)}, v_{i&amp;#39;}^{(l)}, \theta; x)\)&lt;/span&gt; at each &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;, I do a search for &lt;span class=&#34;math inline&#34;&gt;\(m^{(l)}\)&lt;/span&gt; such that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(Y &amp;lt; m^{(l)} \mid u_{i&amp;#39;}^{(l)}, v_{i&amp;#39;}^{(l)}, \theta; x) = 0.5.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, I’ve made a function that can do this (for this specific longitudinal hurdle model).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hurlong::marg_med(newdata = x, 
                  nsims = 1000, 
                  ks = exp(seq(log(0.01), 
                               log(100), 
                               length.out = 15)), # where to evaluate p(Y&amp;lt;k)
                  fit = fit_hurdle)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   id time      2.5%       50%     97.5%
## 1 NA    2 0.2332964 0.4894882 0.8856152&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predictions&lt;/h2&gt;
&lt;p&gt;To round off this series on longitudinal hurdle models, I want to show how to simulate draws (and find quantiles) from the posterior predictive distribution for a new observation (&lt;span class=&#34;math inline&#34;&gt;\(\tilde{Y}\)&lt;/span&gt;). Firstly for a patient &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; already in the data set, where we draw from&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(\tilde{y} \mid \mathbf{y} ; x) = f(\tilde{y} \mid u_i, v_i, \theta, \mathbf{y} ; x)f(u_i, v_i, \theta \mid \mathbf{y}) \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(fit_hurdle,
        newdata = data.frame(id = 1, time = &amp;quot;2&amp;quot;),
        robust = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate Est.Error Q2.5    Q97.5
## [1,] 0.2448615 0.3224153    0 2.145083&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and, secondly, for a new patient &lt;span class=&#34;math inline&#34;&gt;\(i&amp;#39;\)&lt;/span&gt;, where we draw from&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(\tilde{y} \mid \mathbf{y} ; x) = f(\tilde{y} \mid u_{i&amp;#39;}, v_{i&amp;#39;}, \theta, \mathbf{y} ; x)f(u_{i&amp;#39;}, v_{i&amp;#39;} \mid \theta,  \mathbf{y}) f(\theta \mid \mathbf{y}) \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(fit_hurdle,
        newdata = data.frame(id = NA, time = &amp;quot;2&amp;quot;),
        allow_new_levels = TRUE,
        sample_new_levels = &amp;quot;gaussian&amp;quot;,
        robust = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate Est.Error Q2.5    Q97.5
## [1,] 0.4614108 0.6840876    0 52.57643&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal hurdle models 2</title>
      <link>/post/longitudinal-hurdle-models-2/</link>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/longitudinal-hurdle-models-2/</guid>
      <description>


&lt;p&gt;In a &lt;a href=&#34;../post/longitudinal-hurdle-models&#34;&gt;previous&lt;/a&gt; post I fit a longitudinal hurdle model using the &lt;code&gt;brms&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)
summary(fit_hurdle)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: hurdle_lognormal 
##   Links: mu = identity; sigma = identity; hu = logit 
## Formula: y ~ time + (1 | q | id) 
##          hu ~ time + (1 | q | id)
##    Data: dat (Number of observations: 800) 
## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;
##          total post-warmup samples = 8000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##                             Estimate Est.Error l-95% CI u-95% CI Rhat
## sd(Intercept)                   1.96      0.16     1.67     2.30 1.00
## sd(hu_Intercept)                2.46      0.30     1.94     3.10 1.00
## cor(Intercept,hu_Intercept)    -0.90      0.04    -0.97    -0.80 1.00
##                             Bulk_ESS Tail_ESS
## sd(Intercept)                   1475     2467
## sd(hu_Intercept)                3265     4837
## cor(Intercept,hu_Intercept)     1949     3570
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept        0.07      0.21    -0.34     0.48 1.01      770     1487
## hu_Intercept    -2.73      0.35    -3.44    -2.07 1.00     1484     3476
## time2           -0.33      0.08    -0.48    -0.18 1.00    11586     5606
## hu_time2         1.27      0.24     0.81     1.73 1.00    11668     5524
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.94      0.03     0.88     1.00 1.00     8823     5708
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’d now like to do some inference on the model, combining its zero and non-zero parts.&lt;/p&gt;
&lt;p&gt;The model is: for observation &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; from patient &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_{i,j} = Z_{i,j}Y^*_{i,j},\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\text{logit}\left\lbrace P(Z_{i,j} = 0) \right\rbrace = x_{i,j}^T\gamma + u_i,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log (Y^*_{i,j})  \sim N(x_{i,j}^T\beta + v_i, ~\sigma^2), \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left( \begin{array}{c}  u_i \\ v_i \\ \end{array} \right) \sim N\left(\left( \begin{array}{c}  0 \\ 0 \\ \end{array} \right), \left( \begin{array}{c c} \sigma_u^2 &amp;amp; \rho \sigma_u\sigma_v \\ \rho \sigma_u \sigma_v &amp;amp; \sigma_v ^ 2\end{array}\right)\right),\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(x_{i,j}^T = (1, t_{i,j})\)&lt;/span&gt;. Also let &lt;span class=&#34;math inline&#34;&gt;\(\theta = (\gamma, \beta, \sigma, \sigma_u, \sigma_v, \rho)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;how-to-use-fitted.brmsfit&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to use &lt;code&gt;fitted.brmsfit&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Having obtained posterior samples from &lt;span class=&#34;math inline&#34;&gt;\((\theta, u_1,\ldots,u_n,v_1,\ldots,v_n)\)&lt;/span&gt;, we &lt;em&gt;might&lt;/em&gt; want to look at samples from:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} h(u_i, v_i, \theta; x) &amp;amp; = E(Y \mid u_i, v_i, \theta; x) \\ &amp;amp;= \left\lbrace 1 - \text{logit}^{-1} ( x^T \gamma + u_i) \right\rbrace \exp(x^T\beta + v_i + \frac{\sigma^2}{2}).\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is some kind of patient-specific expectation of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, conditional on the random effects. If patient &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is already in the model, then we can just take samples directly from the posterior. This can be achieved with the &lt;code&gt;fitted&lt;/code&gt; method:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(fit_hurdle, 
       newdata = data.frame(id = 1, time = &amp;quot;2&amp;quot;), 
       robust = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate Est.Error      Q2.5     Q97.5
## [1,] 0.4034682  0.150417 0.1796645 0.8248473&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will give the median and (2.5, 97.5) quantiles of &lt;span class=&#34;math inline&#34;&gt;\(h(u_1, v_1, \theta ; x)\)&lt;/span&gt; at timepoint “2”.&lt;/p&gt;
&lt;p&gt;Alternatively, we &lt;em&gt;might&lt;/em&gt; be more interested in &lt;span class=&#34;math inline&#34;&gt;\(h(0, 0, \theta ; x)\)&lt;/span&gt;, which in some sense is the patient-specific expectation of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for an “average” patient with random effects fixed at zero. We can get this by setting &lt;code&gt;re_formula = NA&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(fit_hurdle, 
       newdata = data.frame(time = &amp;quot;2&amp;quot;), 
       robust = TRUE, 
       re_formula = NA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate Est.Error      Q2.5    Q97.5
## [1,] 0.9615294 0.2401395 0.5689127 1.569304&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or, we &lt;em&gt;might&lt;/em&gt; be more interested in &lt;span class=&#34;math inline&#34;&gt;\(h(u_{i&amp;#39;}, v_{i&amp;#39;}, \theta ; x)\)&lt;/span&gt; for a new patient &lt;span class=&#34;math inline&#34;&gt;\(i&amp;#39;\)&lt;/span&gt;. Now we need to generate samples from the posterior distribution&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(u_{i&amp;#39;}, v_{i&amp;#39;}, \theta \mid \mathbf{y}) = f(u_{i&amp;#39;}, v_{i&amp;#39;} \mid \theta, \mathbf{y}) f(\theta \mid \mathbf{y})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we can do this by going through our posterior samples &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(k)}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(k = 1,\ldots,K\)&lt;/span&gt; and each time simulating&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left( \begin{array}{c}  u_{i&amp;#39;}^{(k)} \\ v_{i&amp;#39;}^{(k)} \\ \end{array} \right) \sim N\left(\left( \begin{array}{c}  0 \\ 0 \\ \end{array} \right), \left( \begin{array}{c c} (\sigma^{(k)}_u)^2 &amp;amp; \rho \sigma^{(k)}_u\sigma^{(k)}_v \\ \rho \sigma^{(k)}_u \sigma^{(k)}_v &amp;amp; (\sigma^{(k)}_v)^2\end{array}\right)\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The way I expected this to be done is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(fit_hurdle, newdata = data.frame(id = NA, time = &amp;quot;2&amp;quot;), 
       allow_new_levels = TRUE,
       sample_new_levels = &amp;quot;gaussian&amp;quot;,
       robust = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate Est.Error         Q2.5    Q97.5
## [1,] 0.9095811  1.315207 0.0009475099 58.14658&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and this is indeed what’s happening, as can be seen by going through the steps manually&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ps &amp;lt;- posterior_samples(fit_hurdle)

sigma_error &amp;lt;- ps[,&amp;quot;sigma&amp;quot;]
sigma_u &amp;lt;- ps[,&amp;quot;sd_id__hu_Intercept&amp;quot;]
sigma_v &amp;lt;- ps[,&amp;quot;sd_id__Intercept&amp;quot;]
rho &amp;lt;- ps[,&amp;quot;cor_id__Intercept__hu_Intercept&amp;quot;]

n_mcmc &amp;lt;- length(rho)

x &amp;lt;- data.frame(id = NA, time = &amp;quot;2&amp;quot;)

### simulate u_i&amp;#39; and v_i&amp;#39; 
u &amp;lt;- rnorm(n_mcmc, sd = sigma_u)
### include correlation 
v &amp;lt;- rnorm(n_mcmc, 
           mean = u * sigma_v / sigma_u * rho,
           sd = sqrt((1 - rho^2) * sigma_v ^ 2))


### extract draws from xi = x*gamma
xi &amp;lt;- qlogis(fitted(fit_hurdle, 
                    newdata = x, 
                    re_formula = NA, 
                    dpar = &amp;quot;hu&amp;quot;,
                    summary = FALSE))

### extract draws from eta = x*beta
eta &amp;lt;- fitted(fit_hurdle, 
              newdata = x, 
              re_formula = NA, 
              dpar = &amp;quot;mu&amp;quot;,
              summary = FALSE)

ey &amp;lt;- (1 - plogis(xi + u)) * exp(eta + v + sigma_error ^ 2 / 2)

round(quantile(ey, probs = c(0.025, 0.5, 0.975)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   2.5%    50%  97.5% 
##  0.001  0.927 52.369&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;unconditional-expectation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unconditional expectation&lt;/h3&gt;
&lt;p&gt;Instead of a patient-specific expectation, conditional on random effects, we might be more interested in targeting an overall expectation (for a new patient) where we integrate out the random effects. In other words, we want to take samples from&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} g(\theta; x) &amp;amp; = E(Y \mid  \theta; x) \\
&amp;amp; = \int   E(Y \mid u_{i&amp;#39;}, v_{i&amp;#39;}, \theta; x) f(u_{i&amp;#39;}, v_{i&amp;#39;} \mid \theta, \mathbf{y}) du_{i&amp;#39;}dv_{i&amp;#39;} \\ 
&amp;amp;\approx  L^{-1}\sum_{l = 1}^{L}E(Y \mid u_{i&amp;#39;}^{(l)}, v_{i&amp;#39;}^{(l)}, \theta; x)\\
&amp;amp;=  L^{-1}\sum_{l = 1}^{L}\left\lbrace 1 - \text{logit}^{-1} ( x^T \gamma + u_{i&amp;#39;}^{(l)}) \right\rbrace \exp(x^T\beta + v_{i&amp;#39;}^{(l)} + \frac{\sigma^2}{2}).\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;That is, I take &lt;span class=&#34;math inline&#34;&gt;\(g(\theta^{(k)}; x)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(k = 1,\ldots,K\)&lt;/span&gt;. At each &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(l= 1 \ldots,L\)&lt;/span&gt;, I take indpendent draws&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left( \begin{array}{c}  u_{i&amp;#39;}^{(l)} \\ v_{i&amp;#39;}^{(l)} \\ \end{array} \right) \sim N\left(\left( \begin{array}{c}  0 \\ 0 \\ \end{array} \right), \left( \begin{array}{c c} (\sigma^{(k)}_u)^2 &amp;amp; \rho \sigma^{(k)}_u\sigma^{(k)}_v \\ \rho \sigma^{(k)}_u \sigma^{(k)}_v &amp;amp; (\sigma^{(k)}_v)^2\end{array}\right)\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to perform the inner Monte Carlo integration (probably not the best method for this 2-d example). I don’t think it’s possible to do this with &lt;code&gt;brms&lt;/code&gt; so I’ve written my own code (which only works for this specific model).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;dominicmagirr/hurlong&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hurlong::marg_mean_q(newdata = x, 
                     nsims = 1000, 
                     fit = fit_hurdle)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   id time     2.5%      50%    97.5%
## 1 NA    2 3.845286 7.511835 19.58231&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal hurdle models</title>
      <link>/post/longitudinal-hurdle-models/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/longitudinal-hurdle-models/</guid>
      <description>


&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;Recently, I have been modelling data that is longitudinal, contains excess zeros, and where the non-zero data is right-skewed and measured on a continuous scale, rather than being count data.&lt;/p&gt;
&lt;p&gt;I’ll simulate a semi-realistic example data set from a lognormal hurdle model. The “random effects” for the pr(zero) and non-zero parts of the model are negatively correlated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(180)

## 100 patients
id &amp;lt;- 1:100

## 2 timepoints
time &amp;lt;- c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;)

## random effects
u &amp;lt;- rnorm(100, sd = 2)
v &amp;lt;- rnorm(100, mean = -0.95 * u, sd = sqrt((1 - 0.95^2) * 4)) # p(zero) is negatively correlated with Y*

## non-zero data (4 obs per id, at two timepoints)
ystar1 &amp;lt;- exp(rnorm(400, mean = u, sd = 1))
ystar2 &amp;lt;- exp(rnorm(400, mean = -0.5 + u, sd = 1))

## z = 1 if &amp;quot;cross hurdle&amp;quot;, i.e. if not zero
z1 &amp;lt;- rbinom(400, size = 1, prob = 1 - plogis(-2 + v)) # p(cross hurdle) = 1 - p(zero)
z2 &amp;lt;- rbinom(400, size = 1, prob = 1 - plogis(-1 + v))

dat &amp;lt;- data.frame(y = c(z1 * ystar1, z2 * ystar2),
                  time = rep(time, each = 400),
                  id = rep(id, 8))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this data set there are 100 patients and two timepoints. For each patient, at each timepoint, I have simulated 4 independent observations (I’ve only done this to make model convergence a bit easier). The important point is that the data is correlated within patient, and also z (hurdle part) and ystar (non-zero part) are correlated, so that patients who start with a smaller (non-zero) y at the first timepoint are more likely to have y = 0 at the second timepoint. This can be clearly seen in the plot below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
ggplot(data = dat,
                mapping = aes(x = time, y = y, group = id)) +
  geom_point() +
  geom_line() +
  scale_y_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2019-10-16-longitudinal-hurdle-models_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit the model&lt;/h2&gt;
&lt;p&gt;To fit the model I’m using the excellent &lt;code&gt;brms&lt;/code&gt; package (&lt;a href=&#34;https://github.com/paul-buerkner/brms&#34; class=&#34;uri&#34;&gt;https://github.com/paul-buerkner/brms&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Bürkner P. C. (2018). Advanced Bayesian Multilevel Modeling with the R Package brms. The R Journal. 10(1), 395-411. doi.org/10.32614/RJ-2018-017&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)

fit_hurdle &amp;lt;- brm(bf(y ~  time + (1 | q | id),
                     hu ~ time + (1 | q | id)),
                  data = dat,
                  iter = 4000,
                  family = hurdle_lognormal(),
                  refresh = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit_hurdle)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: hurdle_lognormal 
##   Links: mu = identity; sigma = identity; hu = logit 
## Formula: y ~ time + (1 | q | id) 
##          hu ~ time + (1 | q | id)
##    Data: dat (Number of observations: 800) 
## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;
##          total post-warmup samples = 8000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##                             Estimate Est.Error l-95% CI u-95% CI Rhat
## sd(Intercept)                   1.96      0.16     1.69     2.29 1.00
## sd(hu_Intercept)                2.47      0.30     1.93     3.11 1.00
## cor(Intercept,hu_Intercept)    -0.90      0.04    -0.97    -0.81 1.00
##                             Bulk_ESS Tail_ESS
## sd(Intercept)                   1452     2688
## sd(hu_Intercept)                2979     5484
## cor(Intercept,hu_Intercept)     2539     4394
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept        0.07      0.20    -0.34     0.45 1.00      885     1970
## hu_Intercept    -2.72      0.34    -3.43    -2.07 1.00     1662     3089
## time2           -0.33      0.08    -0.49    -0.18 1.00    14798     5985
## hu_time2         1.26      0.24     0.80     1.73 1.00    12452     5627
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.94      0.03     0.88     1.00 1.00    11293     6175
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inference&lt;/h2&gt;
&lt;p&gt;From the output I can see that I have more-or-less recovered the parameters from my model. In practice, I could use this to make inference on the two parts of the model separately. In &lt;a href=&#34;../post/longitudinal-hurdle-models-2&#34;&gt;future&lt;/a&gt; posts I’ll discuss how to make inference/predictions when combining the two parts of the model.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to calculate the log-rank statistic</title>
      <link>/post/log-rank-test/</link>
      <pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/post/log-rank-test/</guid>
      <description>


&lt;p&gt;Suppose we have the following data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- dplyr::tibble(patient_id = as.character(1:12), 
                    treatment = rep(c(&amp;quot;C&amp;quot;, &amp;quot;E&amp;quot;), each = 6),
                    survival_time = survival::Surv(time = c(2,6,8,11,17,24,7,9,13,22,23,25),
                                                   event = c(1,1,1,1,1,0,1,1,1,0,0,0)))

knitr::kable(df)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;patient_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;treatment&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;survival_time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;24+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25+&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let’s arrange the data in increasing order of survival time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_ordered &amp;lt;- dplyr::arrange(df, survival_time[,&amp;quot;time&amp;quot;])
knitr::kable(df_ordered)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;patient_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;treatment&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;survival_time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;24+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25+&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;method-1-scores&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Method 1: scores&lt;/h1&gt;
&lt;p&gt;The first step is to estimate the survival probability from the pooled data using the Nelson-Aalen estimator. Then, to get the log-rank scores, we add 1 to the logarithm of the pooled survival estimate for all observed events. For censored events we do not add 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pooled_fit &amp;lt;- survival::survfit(survival_time ~ 1, data = df_ordered)
df_logrank &amp;lt;- dplyr::mutate(df_ordered, 
                            pooled_s = exp(-cumsum(pooled_fit$n.event / pooled_fit$n.risk)),
                            logrank_score = log(pooled_s) + survival_time[,&amp;quot;status&amp;quot;])

knitr::kable(df_logrank, digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;patient_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;treatment&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;survival_time&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;pooled_s&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logrank_score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.920&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.917&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.840&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.826&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.760&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.726&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.680&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.615&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.600&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.490&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.520&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.347&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.440&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.180&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.361&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22+&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.361&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23+&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.361&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;24+&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.361&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25+&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.361&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.020&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To calculate the logrank score statistic we add up all the scores on the control arm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logrank_score &amp;lt;- dplyr::summarise(dplyr::group_by(df_logrank, treatment), sum(logrank_score))
logrank_score&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   treatment `sum(logrank_score)`
##   &amp;lt;chr&amp;gt;                    &amp;lt;dbl&amp;gt;
## 1 C                         1.66
## 2 E                        -1.66&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;method-2-observed---expected&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Method 2: observed - expected&lt;/h1&gt;
&lt;p&gt;The score statistic is equivalent to the sum of “observed” - “expected” events at each event time. This is implemented in the survival package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logrank_fit &amp;lt;- survival::survdiff(survival_time ~ treatment, data = df)
rbind(logrank_fit$n, logrank_fit$obs - logrank_fit$exp)[2,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## treatment=C treatment=E 
##    1.664105   -1.664105&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
