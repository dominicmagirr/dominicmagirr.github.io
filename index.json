[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a medical statistician interested in the design, analysis and interpretation of clinical trials.\nI originally planned to study medicine. However, after doing some work experience and realizing how difficult that path would be, I decided to do a maths degree. Towards the end of my studies I was fortunate to see a careers talk on Medical Statistics that led me to Lancaster University to do their MSc course \u0026ndash; a fantastic year, and I carried on to do a PhD. My research was on:\n group sequential trials multiple hypothesis testing adaptive designs  I stayed in Lancaster a little longer, and also had a wonderful year at the Medical University of Vienna. This time allowed me to publish some more papers, teach, and start my statistical consultancy career.\nI came back to the UK in 2015 to work at AstraZeneca. For two years, I worked in the early-phase oncology group, as part of clinical teams, contributing to the design of trials, and overseeing the statistical analyses conducted by CRO partners. For a further year, I worked as an internal statistical consultant as part of AstraZeneca\u0026rsquo;s Statistical Innovation team. This was an intense, but rewarding, experience that allowed me to work on trials in many different therapy areas. A highlight was working on new methods for dealing with non-proportional hazards in immuno-oncology. (Preprint here).\nSince January 2019 I have been working at a start-up company Cambridge Cancer Genomics, where I have become very interested in statistical modelling of longitudinal circulating-tumour DNA measurements.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a medical statistician interested in the design, analysis and interpretation of clinical trials.\nI originally planned to study medicine. However, after doing some work experience and realizing how difficult that path would be, I decided to do a maths degree. Towards the end of my studies I was fortunate to see a careers talk on Medical Statistics that led me to Lancaster University to do their MSc course \u0026ndash; a fantastic year, and I carried on to do a PhD.","tags":null,"title":"Dominic Magirr","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":[],"content":" In my last post I had just used the BMA package to do a quick analysis with Bayesian model averaging. The method rests on a lovely piece of theory. Posterior model probabilities are calculated via a simple application of Bayes rule:\n\\[p(M_1 | D) = \\frac{p(D | M_1)p(M_1)}{\\sum_k p(D | M_k)p(M_k)}\\]\nbut it’s a challenge calculating the terms\n\\[p(D | M_k) = \\int p(D | \\theta_k, M_k)p(\\theta_k | M_k) d\\theta_k.\\]\nRafferty (Section 4) gives a top-class explanation for why\n\\[\\log p(D | M_k) \\approx \\log p(D | \\hat{\\theta},M_k) - (d/2)\\log n\\]\nwhere \\(d\\) and \\(n\\) are the number of parameters and “observations”, respectively, in model \\(M_k\\). This is the same as saying\n\\[p(D | M_k) \\approx \\exp\\left\\lbrace -0.5 \\times \\text{BIC}(M_k)\\right\\rbrace,\\]\nwhere \\(BIC\\) is the Bayesian Information Criteria.\nReproduce BMA results manually I find it helpful sometimes to do things the long way round. We started with the survival::veterans data set, changing some covariates from numeric to categorical\nlibrary(survival) library(BMA) data(veteran) veteran$trt \u0026lt;- factor(veteran$trt) veteran$prior \u0026lt;- factor(veteran$prior) Now we can fit a Cox model to each combination of covariates\n## Extract names of covariates x_names \u0026lt;- setdiff(names(veteran), c(\u0026quot;time\u0026quot;, \u0026quot;status\u0026quot;)) ## Find all combinations of covariates xs \u0026lt;- c(\u0026quot;1\u0026quot;, unlist(purrr::map(seq_along(x_names), combn, x = x_names, FUN = function(x) paste(x, collapse = \u0026quot; + \u0026quot;)))) ## Turn into model formulas fs \u0026lt;- paste(\u0026quot;Surv(time, status)\u0026quot;, xs, sep = \u0026quot; ~ \u0026quot;) ## Fit models ms \u0026lt;- purrr::map(fs, function(f) coxph(as.formula(f), data = veteran, method = \u0026quot;breslow\u0026quot;, iter.max = 30)) From the models, we can extract the BICs, turn them into (approximate) marginal probabilites of the data, and then into (approximate) posterior model probabilities (where we assume equal prior probabilities for each model)…\nget_bic \u0026lt;- function(m){ log(m$nevent) * length(coef(m)) - 2 * rev(m$loglik)[1] } bics \u0026lt;- purrr::map_dbl(ms, get_bic) p_ds \u0026lt;- exp(-0.5 * bics) p_ms \u0026lt;- p_ds / (sum(p_ds)) ## calculate cut-off for model inclusion odds_ms \u0026lt;- p_ms / (1 - p_ms) cut_off \u0026lt;- 0.05 * max(odds_ms) / (1 + 0.05 * max(odds_ms)) plot(seq_along(p_ms), p_ms, xlab = \u0026quot;Model index\u0026quot;, ylab = \u0026quot;p(M|D)\u0026quot;) abline(h = cut_off, col = 2) Most of the models have very low posterior probability. Only 6 cross the default threshold for inclusion in the final model (odds ratio less than 20 compared to the best model). I can extract those models and re-standardise:\ninclude_indices \u0026lt;- which(p_ms \u0026gt;= cut_off) ms_include \u0026lt;- ms[include_indices] p_ms_include \u0026lt;- p_ms[include_indices] / sum(p_ms[include_indices]) data.frame(x = xs[include_indices], p = round(p_ms_include,3)) ## x p ## 1 karno 0.154 ## 2 celltype + karno 0.562 ## 3 trt + celltype + karno 0.113 ## 4 celltype + karno + diagtime 0.054 ## 5 celltype + karno + age 0.061 ## 6 celltype + karno + prior 0.056 You can check that this matches the output from the BMA::bic.surv summary.\nMarginal posterior distributions of coefficients This part is messy, to be honest, but I wanted to finish the job. To reproduce the posterior density plots, I start by putting posterior means and variances into a table, with corresponding posterior model probabilities:\npost_df \u0026lt;- purrr::map2_dfr(ms_include, p_ms_include, function(m, p) data.frame(mean = unname(coef(m)), var = unname(diag(vcov(m))), p = p, covariate = names(coef(m)), stringsAsFactors = FALSE)) head(post_df) ## mean var p covariate ## 1 -0.03324294 2.573811e-05 0.1540161 karno ## 2 0.71214811 6.387773e-02 0.5618671 celltypesmallcell ## 3 1.15080136 8.576748e-02 0.5618671 celltypeadeno ## 4 0.32514265 7.655965e-02 0.5618671 celltypelarge ## 5 -0.03090393 2.681814e-05 0.5618671 karno ## 6 0.25731308 4.025205e-02 0.1130516 trt2 I can turn this into a function to find the non-zero densities:\nnon_zero_density \u0026lt;- function(x, select_cov){ df \u0026lt;- post_df[post_df$covariate == select_cov,] ds \u0026lt;- purrr::pmap_dfc(list(p = df$p, m = df$mean, v = df$var), function(p,m,v) p * dnorm(x, m, sqrt(v))) rowSums(ds) } For example (I’ve cheated here by looking ahead for a sensible range of x for trt2)…\nx_trt2 \u0026lt;- seq(-0.5, 1, length.out = 100) nzd_trt2 \u0026lt;- non_zero_density(x = x_trt2, select_cov = \u0026quot;trt2\u0026quot;) plot(x_trt2, nzd_trt2, type = \u0026quot;l\u0026quot;, xlab = \u0026quot;coefficient for trt2\u0026quot;, ylab = \u0026quot;non-zero density\u0026quot;) This is looking good now, compared to last time, but we still need to find the posterior probability that trt2 is excluded from the model, and sort out the y-axis scale (in ?plot.bic.surv it says “The nonzero part of the distribution is scaled so that the maximum height is equal to the probability that the coefficient is nonzero.”)…\nprob_nonzero \u0026lt;- function(select_cov){ df \u0026lt;- post_df[post_df$covariate == select_cov,] sum(df$p) } rescale_nzd \u0026lt;- function(nzd, pnz){ nzd / max(nzd) * pnz } pnz_trt2 \u0026lt;- prob_nonzero(\u0026quot;trt2\u0026quot;) rnzd_trt2 \u0026lt;- rescale_nzd(nzd_trt2, pnz_trt2) plot(x_trt2, rnzd_trt2, type = \u0026quot;l\u0026quot;, xlab = \u0026quot;coefficient for trt2\u0026quot;, ylab = \u0026quot; \u0026quot;, ylim = c(0,1)) points(c(0,0), c(0, 1 - pnz_trt2), type = \u0026#39;l\u0026#39;, lwd = 3) Yep, there we go! I could now repeat this for the other covariates if I wanted.\n  ","date":1572912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572947750,"objectID":"4680c400b3b124f34f12bc917eb1c4e1","permalink":"/post/bma-2/","publishdate":"2019-11-05T00:00:00Z","relpermalink":"/post/bma-2/","section":"post","summary":"In my last post I had just used the BMA package to do a quick analysis with Bayesian model averaging. The method rests on a lovely piece of theory. Posterior model probabilities are calculated via a simple application of Bayes rule:\n\\[p(M_1 | D) = \\frac{p(D | M_1)p(M_1)}{\\sum_k p(D | M_k)p(M_k)}\\]\nbut it’s a challenge calculating the terms\n\\[p(D | M_k) = \\int p(D | \\theta_k, M_k)p(\\theta_k | M_k) d\\theta_k.","tags":["R Packages"],"title":"BMA-2","type":"post"},{"authors":[],"categories":[],"content":" I’ve used the BMA package for Bayesian model averaging a couple of times this year and think it’s great.\n(https://www.r-project.org/doc/Rnews/Rnews_2005-2.pdf)\nThe situation is this: you have a data set with a response variable and multiple explanatory variables (the goal could be prediction or inference). It’s crying out for a regression analysis. But you don’t want to do use a naive variable selection method because you know that’s bad. Is there something smarter you can do, and do quickly?\nExample Let’s take the veteran data set from the survival package. This data comes from a randomized trial of two chemotherapies with a primary endpoint of overall survival. There are several covariates, which we can take a look at…\nlibrary(survival) library(BMA) data(veteran) str(veteran) ## \u0026#39;data.frame\u0026#39;: 137 obs. of 8 variables: ## $ trt : num 1 1 1 1 1 1 1 1 1 1 ... ## $ celltype: Factor w/ 4 levels \u0026quot;squamous\u0026quot;,\u0026quot;smallcell\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ time : num 72 411 228 126 118 10 82 110 314 100 ... ## $ status : num 1 1 1 1 1 1 1 1 1 0 ... ## $ karno : num 60 70 60 60 70 20 40 80 50 70 ... ## $ diagtime: num 7 5 3 9 11 5 10 29 18 6 ... ## $ age : num 69 64 38 63 65 49 69 68 43 70 ... ## $ prior : num 0 10 0 10 10 0 10 0 0 0 ... karno refers to Karnofsky performance score (from 0-100 where higher is better). Something’s gone wrong with the prior treatment variable: this should be a yes/no. Variable trt should also be categorical:\nveteran$trt \u0026lt;- factor(veteran$trt) veteran$prior \u0026lt;- factor(veteran$prior) We can use the bic.surv function to fit a Cox model for every possible combination of the explanatory variables.\ntest.bic.surv\u0026lt;- bic.surv(Surv(time,status) ~ ., data = veteran, factor.type = TRUE) The summary gives us the posterior probability for each model (by default each model is given the same prior probability)\nsummary(test.bic.surv, conditional=FALSE, digits=2) ## ## Call: ## bic.surv.formula(f = Surv(time, status) ~ ., data = veteran, factor.type = TRUE) ## ## ## 6 models were selected ## Best 5 models (cumulative posterior probability = 0.95 ): ## ## p!=0 EV SD model 1 model 2 model 3 ## trt 11.3 ## .2 0.02909 0.1058 . . 0.2573 ## celltype 84.6 ## .smallcell 0.61564 0.3538 0.7121 . 0.8196 ## .adeno 0.97619 0.4965 1.1508 . 1.1477 ## .large 0.28260 0.2830 0.3251 . 0.3930 ## karno 100.0 -0.03135 0.0052 -0.0309 -0.0332 -0.0311 ## diagtime 5.4 0.00018 0.0020 . . . ## age 6.1 -0.00036 0.0026 . . . ## prior 5.6 ## .10 0.00576 0.0542 . . . ## ## nVar 2 1 3 ## BIC -39.3626 -36.7742 -36.1558 ## post prob 0.562 0.154 0.113 ## model 4 model 5 ## trt ## .2 . . ## celltype ## .smallcell 0.7208 0.7264 ## .adeno 1.1643 1.1765 ## .large 0.3215 0.3276 ## karno -0.0318 -0.0311 ## diagtime . . ## age -0.0059 . ## prior ## .10 . 0.1026 ## ## nVar 3 3 ## BIC -34.9292 -34.7553 ## post prob 0.061 0.056 It’s possible to plot the marginal posterior distribution for each covariate (averaged according to posterior model probabilities):\nplot(test.bic.surv) Conclusions  I’ve found these graphs really useful for showing the boss. When you want to show-off potentially interesting relationships, but prevent them (to some extent) from getting carried away. This is an intuitive way to do it.\n A drawback of BMA is that it doesn’t handle missing data. Any row with NAs will be removed.\n In a future post I’ll go more into the details of what BMA is doing.\n    ","date":1572825600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572880751,"objectID":"603d3646c397f5798fde2b3e5d2d4eb5","permalink":"/post/bma/","publishdate":"2019-11-04T00:00:00Z","relpermalink":"/post/bma/","section":"post","summary":"I’ve used the BMA package for Bayesian model averaging a couple of times this year and think it’s great.\n(https://www.r-project.org/doc/Rnews/Rnews_2005-2.pdf)\nThe situation is this: you have a data set with a response variable and multiple explanatory variables (the goal could be prediction or inference). It’s crying out for a regression analysis. But you don’t want to do use a naive variable selection method because you know that’s bad. Is there something smarter you can do, and do quickly?","tags":["R Packages"],"title":"BMA","type":"post"},{"authors":null,"categories":[],"content":" Previously, I started discussing the flexsurv package. I used it to fit a Weibull model. This is implemented as an accelerated failure time model. It is also a proportional hazards model (although, as I found previously, converting between the two is not so straightforward, but it can be done by SurvRegCensCov).\nNow let’s compare Weibull regression with Cox regression. Firstly, Weibull regression:\n assumes proportional hazards; the number of parameters is equal to \\(k + 2\\), where \\(k\\) is the number of covariates; we can estimate things like the median, \\(P(S\u0026gt;s^*)\\), etc. from the model… but the model might be too restrictive – we won’t estimate these things very well.  Cox regression:\n assumes proportional hazards; there are \\(k\\) parameters (one for each covariate); we don’t estimate the baseline hazard… which means we don’t get estimates for things like the median, \\(P(S\u0026gt;s^*)\\), etc.  flexsurv has a function flexsurvspline which allows one to bridge the gap between Weibull regression and Cox regresssion.\nFrom Weibull regression to Cox regression. For a Weibull distribution with shape \\(a\\), scale \\(b\\), covariates \\(x\\), and regression coefficents \\(\\gamma\\), the survival probability is\n\\[S(t; x) = \\exp\\left\\lbrace - \\left( \\frac{t}{b \\cdot \\exp(x^T\\gamma)} \\right) ^ a\\right\\rbrace\\]\nSince survival is related to the log-cumulative hazard via \\(S=\\exp(-H)\\), this means that\n\\[\\log H(t;x) = a\\log t - a\\log(b) - a x^T\\gamma\\]\nIn words, the log-cumulative hazard has a linear relationship with (log-) time, with the intercept depending on the value of \\(x\\). For a given data set, we can check if this is reasonable by looking at non-paramteric estimates, \\(\\log \\hat{H}(t; x)\\).\nlibrary(survival) library(flexsurv) library(ggplot2) ### Non-parametric analysis fit_km \u0026lt;- survfit(Surv(futime, death) ~ trt, data = myeloid) t_seq \u0026lt;- seq(1, 2500, length.out = 1000) km_sum \u0026lt;- summary(fit_km, times = t_seq, extend = TRUE) ### Weibull regression fit_weibull \u0026lt;- flexsurvreg(Surv(futime, death) ~ trt, dist = \u0026quot;weibull\u0026quot;, data = myeloid) a \u0026lt;- fit_weibull$res[\u0026quot;shape\u0026quot;, \u0026quot;est\u0026quot;] b \u0026lt;- fit_weibull$res[\u0026quot;scale\u0026quot;, \u0026quot;est\u0026quot;] trtB \u0026lt;- fit_weibull$res[\u0026quot;trtB\u0026quot;, \u0026quot;est\u0026quot;] ### plot log-cumulative hazard against log-time df \u0026lt;- data.frame(log_time = log(rep(t_seq, 2)), logcumhaz = log(-log(km_sum$surv)), trt = rep(c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;), each = 1000), logcumhaz_w = c(a * (log(t_seq) - log(b)), a * (log(t_seq) - log(b) - trtB))) ggplot(data = df, mapping = aes(x = log_time, y = logcumhaz, colour = trt)) + geom_line() + geom_line(mapping = aes(x = log_time, y = logcumhaz_w, colour = trt), linetype = 2) + theme_bw() + scale_x_continuous(limits = c(2,8)) + scale_y_continuous(limits = c(-6,0)) What’s going on here? Well, the first thing to acknowledge is that the hazards only appear to be proportional after about 150 (\\(e^5\\)) days. I’m not sure I would immediately abandon a proportional-hazards model, though, as most of the events happen when the hazards are proportional (only 10-15% of the events happen before day 150), so the right-hand-side of the plot is far more important. Looking to the right then: the relationship between the log-cumulative hazard and log-time is not really linear. The distance between the two lines is roughly the same for the two models (Weibull and non-parametric), suggesting that the Weibull model does ok at estimating the hazard ratio. However, the lack of linearity will lead to poor estimates for the medians, \\(P(S\u0026gt;s^*)\\), etc., as can be confirmed by plotting the survival curves:\nggplot(data = df, mapping = aes(x = exp(log_time), y = exp(-exp(logcumhaz)), colour = trt)) + geom_line() + geom_line(mapping = aes(x = exp(log_time), y = exp(-exp(logcumhaz_w)), colour = trt), linetype = 2) + theme_bw() To improve the model, given this lack of linearity, it seems quite natural to change from\n\\[\\log H(t;x) = a\\log t - a\\log(b) - a x^T\\gamma\\]\nto\n\\[\\log H(t;x) = s(\\log t) + x^T\\beta\\]\nwhere \\(s(\\log t)\\) is a natural cubic spline function of (log) time. One can make the model more/less flexible by choosing a large/small number of knots. By default, the knots are placed at quantiles of the uncensored event times. How many knots are required? I don’t really have a good answer for this: one or two. At most, three? In this example, I’m using two inner knots, placed at 33% and 66% of the uncensored event times (indicated by vertical lines):\nfit_spline_2 \u0026lt;- flexsurvspline(Surv(futime, death) ~ trt, data = myeloid, k = 2, scale = \u0026quot;hazard\u0026quot;) spline_2_sum \u0026lt;- summary(fit_spline_2, t = t_seq, type = \u0026quot;cumhaz\u0026quot;) df2 \u0026lt;- cbind(df, data.frame(logcumhaz_s2 = log(c(spline_2_sum$`trt=A`[\u0026quot;est\u0026quot;]$est, spline_2_sum$`trt=B`[\u0026quot;est\u0026quot;]$est)))) ggplot(data = df2, mapping = aes(x = log_time, y = logcumhaz, colour = trt)) + geom_line() + geom_line(mapping = aes(x = log_time, y = logcumhaz_s2, colour = trt), linetype = 2) + theme_bw() + scale_x_continuous(limits = c(2,8)) + scale_y_continuous(limits = c(-6,0)) + geom_vline(xintercept = fit_spline_2$knots) This looks a lot better, and we can see the improvement in the survival curves:\nggplot(data = df2, mapping = aes(x = exp(log_time), y = exp(-exp(logcumhaz)), colour = trt)) + geom_line() + geom_line(mapping = aes(x = exp(log_time), y = exp(-exp(logcumhaz_s2)), colour = trt), linetype = 2) + theme_bw()  From Cox regression to Weibull regression. If we start out from Cox regression\n\\[h(t;x)=h_0(t)\\exp(x^T\\beta)\\]\nthis means that\n\\[\\log H(t;x) = \\log H_0(t;x) + x^T\\beta\\]\nWe estimate the parameters \\(\\beta\\) from the partial likelihood, and don’t estimate \\(\\log H_0(t;x)\\). So \\(\\log H_0(t;x)\\) can be anything. However, with the flexsurvspline function, as long as we use enough knots, \\(s(\\log(t))\\) can be more-or-less anything (smooth), so the two methods will give the same information about \\(\\beta\\).\n Conclusions I can’t really see any reason not to switch from a Cox model to flexsurvspline. You don’t lose anything in terms of inference on \\(\\beta\\), only gain a nice estimate for the baseline hazard. Also, inference is all based on maximum likelihood. No special theory required.\nFrom the other side, if you start out from Weibull regression, and then realise that Weibull is the wrong model, you don’t have to think too hard about how to choose a better model, you know that flexsurvspline will be good (assuming proportional hazards is correct: for non-proportional hazards you may have to think harder).\nWhat can go wrong? In small sample sizes, I guess there could be issues with over-fitting if too many knots are chosen. But given a decent sample size, I can’t see any problems. I would be interested to see a \\(\\log H_0(t;x)\\) that is highly wiggly – doesn’t seem likely in practice.\n ","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"4e99202e9d8bdf2882886313032b1cea","permalink":"/post/flexsurv-2/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/post/flexsurv-2/","section":"post","summary":"Previously, I started discussing the flexsurv package. I used it to fit a Weibull model. This is implemented as an accelerated failure time model. It is also a proportional hazards model (although, as I found previously, converting between the two is not so straightforward, but it can be done by SurvRegCensCov).\nNow let’s compare Weibull regression with Cox regression. Firstly, Weibull regression:\n assumes proportional hazards; the number of parameters is equal to \\(k + 2\\), where \\(k\\) is the number of covariates; we can estimate things like the median, \\(P(S\u0026gt;s^*)\\), etc.","tags":["R Packages"],"title":"flexsurv 2","type":"post"},{"authors":null,"categories":[],"content":" I’m going to write about some of my favourite R packages. I’ll start with flexsurv (https://github.com/chjackson/flexsurv-dev) by Chris Jackson, which can be used to fit all kinds of parametric models to survival data. It can really do a lot, but I’ll pick out just 2 cool things I like about it:\nFit a standard survival model, but where it’s slightly easier to work out what the parameters mean. Fit a proportional hazards model, which is a lot like a Cox model, but where you also model the baseline hazard using a spline.  1. Consistent parameter values As mentioned in the tutorial (https://www.jstatsoft.org/article/view/v070i08), for simple models flexsurvreg acts as a wrapper for survival::survreg, but where the parameters in the output match those of dweibull.\nWith survival::survreg I would do, e.g.:\ndat_ovarian \u0026lt;- survival::ovarian dat_ovarian$rx \u0026lt;- factor(dat_ovarian$rx) library(survival) fit_survreg = survreg(Surv(futime, fustat) ~ rx, dist = \u0026quot;weibull\u0026quot;, data = dat_ovarian) summary(fit_survreg) ## ## Call: ## survreg(formula = Surv(futime, fustat) ~ rx, data = dat_ovarian, ## dist = \u0026quot;weibull\u0026quot;) ## Value Std. Error z p ## (Intercept) 6.825 0.344 19.84 \u0026lt;2e-16 ## rx2 0.559 0.529 1.06 0.29 ## Log(scale) -0.121 0.251 -0.48 0.63 ## ## Scale= 0.886 ## ## Weibull distribution ## Loglik(model)= -97.4 Loglik(intercept only)= -98 ## Chisq= 1.18 on 1 degrees of freedom, p= 0.28 ## Number of Newton-Raphson Iterations: 5 ## n= 26 Then I would look around online for an explanation of the output e.g. (https://stats.stackexchange.com/questions/159044/weibull-survival-model-in-r). There is also an explanation in ?survreg.\nOn the other hand, using flexsurv:\nlibrary(flexsurv) fit_flexsurvreg = flexsurvreg(Surv(futime, fustat) ~ rx, dist = \u0026quot;weibull\u0026quot;, data = dat_ovarian) fit_flexsurvreg ## Call: ## flexsurvreg(formula = Surv(futime, fustat) ~ rx, data = dat_ovarian, ## dist = \u0026quot;weibull\u0026quot;) ## ## Estimates: ## data mean est L95% U95% se exp(est) ## shape NA 1.129 0.690 1.848 0.284 NA ## scale NA 920.128 468.868 1805.704 316.508 NA ## rx2 0.500 0.559 -0.478 1.597 0.529 1.749 ## L95% U95% ## shape NA NA ## scale NA NA ## rx2 0.620 4.936 ## ## N = 26, Events: 12, Censored: 14 ## Total time at risk: 15588 ## Log-likelihood = -97.36415, df = 3 ## AIC = 200.7283 The parameters shape and scale correspond to dweibull. So I don’t have to think any further? Not quite: I still have to work out what the estimate of rx2 is doing. I might look at exp(est) = 1.749 and somehow expect this to be a hazard ratio. It’s not. It’s a multiplicative effect on the scale parameter. So when rx = 1 the scale is 920.1, and when rx = 2 the scale is 920.1 * 1.749. The hazard ratio (treatment 2 vs treatment 1) is\n\\[\\begin{align} \\frac{h_2(x)}{h_1(x)} \u0026amp; = \\left( \\frac{b_1}{b_2} \\right)^a \\\\ \u0026amp; = \\left( \\frac{920.1}{920.1 \\times 1.749} \\right)^{1.129}\\\\ \u0026amp; = 0.53 \\end{align} \\]\nwhere \\(a\\) is the common shape parameter, and \\(b_1\\) and \\(b_2\\) are the scale parameters.\nOnce I had started writing this post, I realized that it’s actually not straightforward to make inference on the hazard ratio using flexsurv. For working out variances/covariances, the survreg parameterization is indeed better. I looked around for other R packages in this space, and found SurvRegCensCov, which can do this conversion automatically for you:\nlibrary(SurvRegCensCov) ConvertWeibull(fit_survreg)$HR ## HR LB UB ## rx2 0.5318051 0.1683444 1.679989 For completeness, using flexsurv, the log-hazard ratio is\n\\[\\begin{align} \\log\\left( \\frac{h_2(x)}{h_1(x) }\\right) \u0026amp; = a\\left\\lbrace \\log(b_1) - \\log(b_2) \\right\\rbrace \\\\ \u0026amp; = -\\exp(\\log(a)) \\times \\left\\lbrace \\log(b_2) - \\log(b_1) \\right\\rbrace \\end{align}\\]\nI can extract the terms \\(\\alpha:=\\log(a)\\) and \\(\\beta:=\\log(b_2) - \\log(b_1)\\) from the fit_flexsurvreg object, as well as their (co)variance.\nalpha \u0026lt;- fit_flexsurvreg$res.t[\u0026quot;shape\u0026quot;, \u0026quot;est\u0026quot;] beta \u0026lt;- fit_flexsurvreg$res.t[\u0026quot;rx2\u0026quot;, \u0026quot;est\u0026quot;] cov_alpha_beta \u0026lt;- vcov(fit_flexsurvreg)[c(\u0026quot;shape\u0026quot;, \u0026quot;rx2\u0026quot;), c(\u0026quot;shape\u0026quot;, \u0026quot;rx2\u0026quot;)] Then work out the variance of the log-hazard ratio using the delta method.\n\\[\\text{var}\\left\\lbrace -\\beta\\exp(\\alpha) \\right\\rbrace = (-\\beta\\exp(\\alpha), -\\exp(\\alpha)) \\text{Cov}(\\alpha, \\beta) \\left( \\begin{array}{c} -\\beta\\exp(\\alpha) \\\\ -\\exp(\\alpha) \\end{array}\\right)\\]\ngrad \u0026lt;- matrix(c(-beta*exp(alpha), -exp(alpha)), ncol = 1) var_lhr = t(grad) %*% cov_alpha_beta %*% grad se_lhr = sqrt(var_lhr) se_lhr ## [,1] ## [1,] 0.5868807 And to get a 95% confidence interval for the hazard ratio…\nlog_hr = -exp(alpha) * beta log_hr_upr = log_hr + qnorm(0.975) * se_lhr log_hr_lwr = log_hr - qnorm(0.975) * se_lhr data.frame(HR = exp(log_hr), LB = exp(log_hr_lwr), UB = exp(log_hr_upr)) ## HR LB UB ## 1 0.5318051 0.1683444 1.679988  Splines The second thing I really like about flexsurv is the proportional hazards model with a spline for the baseline hazard. I’ll explore this in another post.\n ","date":1572220800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572220800,"objectID":"e943e878f08e699d8fc8f60da43195a1","permalink":"/post/flexsurv/","publishdate":"2019-10-28T00:00:00Z","relpermalink":"/post/flexsurv/","section":"post","summary":"I’m going to write about some of my favourite R packages. I’ll start with flexsurv (https://github.com/chjackson/flexsurv-dev) by Chris Jackson, which can be used to fit all kinds of parametric models to survival data. It can really do a lot, but I’ll pick out just 2 cool things I like about it:\nFit a standard survival model, but where it’s slightly easier to work out what the parameters mean. Fit a proportional hazards model, which is a lot like a Cox model, but where you also model the baseline hazard using a spline.","tags":["R Packages"],"title":"flexsurv","type":"post"},{"authors":null,"categories":[],"content":" In the last post on longitudinal hurdle models, I had just taken samples from the marginal mean\n\\[\\begin{align} g(\\theta; x) \u0026amp; = E(Y \\mid \\theta; x) \\\\ \u0026amp; = \\int E(Y \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}E(Y \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\\\ \u0026amp;= L^{-1}\\sum_{l = 1}^{L}\\left\\lbrace 1 - \\text{logit}^{-1} ( x^T \\gamma + u_{i\u0026#39;}^{(l)}) \\right\\rbrace \\exp(x^T\\beta + v_{i\u0026#39;}^{(l)} + \\frac{\\sigma^2}{2}).\\end{align}\\]\nOne of the issues with lognormal data is that it is highly skewed, so the mean can be very large. In a small sample, the sample mean can change a lot based on just 1 or 2 large observations. For this reason I would like to sample from other summary measures of \\(Y\\).\nSamples from \\(p(Y \\leq k \\mid \\theta; x)\\) This is quite similar to taking samples from the marginal mean.\n\\[\\begin{align} r(\\theta; x, k) \u0026amp; = p(Y \u0026lt; k \\mid \\theta; x) \\\\ \u0026amp; = \\int p(Y \u0026lt; k \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}p(Y \u0026lt; k \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\\\ \u0026amp;= L^{-1}\\sum_{l = 1}^{L}\\ \\left[ \\pi^{(l)}(x) + \\left\\lbrace 1 - \\pi^{(l)}(x)\\right\\rbrace \\Phi \\left\\lbrace \\frac{\\log(k) - x^T\\beta - v_{i\u0026#39;}^{(l)}}{\\sigma} \\right\\rbrace \\right].\\end{align}\\]\nwhere \\(\\pi^{(l)}(x):= \\text{logit}^{-1} ( x^T \\gamma + u_{i\u0026#39;}^{(l)})\\).\nAgain, I don’t know of any functions for doing this, so I built my own.\ndevtools::install_github(\u0026quot;dominicmagirr/hurlong\u0026quot;) library(brms) x \u0026lt;- data.frame(id = NA, time = \u0026quot;2\u0026quot;) hurlong::marg_pyk_q(k = 0.5, newdata = x, nsims = 1000, fit = fit_hurdle) ## id time 2.5% 50% 97.5% ## 1 NA 2 0.4239409 0.501178 0.5801595  Samples from \\(\\text{median}(Y \\mid \\theta; x)\\) Finally, I am interested in samples from the marginal median.\n\\[\\begin{align} s(\\theta; x) \u0026amp; = \\text{median}(Y \\mid \\theta; x) \\\\ \u0026amp; = \\int \\text{median}(Y \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}\\text{median}(Y\\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x) \\end{align}\\]\nTo evaluate \\(\\text{median}(Y\\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\) at each \\(l\\), I do a search for \\(m^{(l)}\\) such that\n\\[p(Y \u0026lt; m^{(l)} \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x) = 0.5.\\]\nAgain, I’ve made a function that can do this (for this specific longitudinal hurdle model).\nhurlong::marg_med(newdata = x, nsims = 1000, ks = exp(seq(log(0.01), log(100), length.out = 15)), # where to evaluate p(Y\u0026lt;k) fit = fit_hurdle) ## id time 2.5% 50% 97.5% ## 1 NA 2 0.2325832 0.4891982 0.8864017  Predictions To round off this series on longitudinal hurdle models, I want to show how to simulate draws (and find quantiles) from the posterior predictive distribution for a new observation (\\(\\tilde{Y}\\)). Firstly for a patient \\(i\\) already in the data set, where we draw from\n\\[ f(\\tilde{y} \\mid \\mathbf{y} ; x) = f(\\tilde{y} \\mid u_i, v_i, \\theta, \\mathbf{y} ; x)f(u_i, v_i, \\theta \\mid \\mathbf{y}) \\]\nlibrary(brms) predict(fit_hurdle, newdata = data.frame(id = 1, time = \u0026quot;2\u0026quot;), robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.2427486 0.3112582 0 2.078795 and, secondly, for a new patient \\(i\u0026#39;\\), where we draw from\n\\[ f(\\tilde{y} \\mid \\mathbf{y} ; x) = f(\\tilde{y} \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta, \\mathbf{y} ; x)f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) f(\\theta \\mid \\mathbf{y}) \\]\npredict(fit_hurdle, newdata = data.frame(id = NA, time = \u0026quot;2\u0026quot;), allow_new_levels = TRUE, sample_new_levels = \u0026quot;gaussian\u0026quot;, robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.4822929 0.7150474 0 57.66133  ","date":1571961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571961600,"objectID":"667624a716e015e1dfc0b086abb2e1e4","permalink":"/post/longitudinal-hurdle-models-3/","publishdate":"2019-10-25T00:00:00Z","relpermalink":"/post/longitudinal-hurdle-models-3/","section":"post","summary":"In the last post on longitudinal hurdle models, I had just taken samples from the marginal mean\n\\[\\begin{align} g(\\theta; x) \u0026amp; = E(Y \\mid \\theta; x) \\\\ \u0026amp; = \\int E(Y \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}E(Y \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\\\ \u0026amp;= L^{-1}\\sum_{l = 1}^{L}\\left\\lbrace 1 - \\text{logit}^{-1} ( x^T \\gamma + u_{i\u0026#39;}^{(l)}) \\right\\rbrace \\exp(x^T\\beta + v_{i\u0026#39;}^{(l)} + \\frac{\\sigma^2}{2}).","tags":[],"title":"Longitudinal hurdle models 3","type":"post"},{"authors":null,"categories":[],"content":" In a previous post I fit a longitudinal hurdle model using the brms package.\nlibrary(brms) summary(fit_hurdle) ## Family: hurdle_lognormal ## Links: mu = identity; sigma = identity; hu = logit ## Formula: y ~ time + (1 | q | id) ## hu ~ time + (1 | q | id) ## Data: dat (Number of observations: 800) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~id (Number of levels: 100) ## Estimate Est.Error l-95% CI u-95% CI Rhat ## sd(Intercept) 1.96 0.16 1.67 2.30 1.00 ## sd(hu_Intercept) 2.46 0.30 1.94 3.10 1.00 ## cor(Intercept,hu_Intercept) -0.90 0.04 -0.97 -0.80 1.00 ## Bulk_ESS Tail_ESS ## sd(Intercept) 1475 2467 ## sd(hu_Intercept) 3265 4837 ## cor(Intercept,hu_Intercept) 1949 3570 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.07 0.21 -0.34 0.48 1.01 770 1487 ## hu_Intercept -2.73 0.35 -3.44 -2.07 1.00 1484 3476 ## time2 -0.33 0.08 -0.48 -0.18 1.00 11586 5606 ## hu_time2 1.27 0.24 0.81 1.73 1.00 11668 5524 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.94 0.03 0.88 1.00 1.00 8823 5708 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). I’d now like to do some inference on the model, combining its zero and non-zero parts.\nThe model is: for observation \\(j\\) from patient \\(i\\),\n\\[Y_{i,j} = Z_{i,j}Y^*_{i,j},\\] \\[\\text{logit}\\left\\lbrace P(Z_{i,j} = 0) \\right\\rbrace = x_{i,j}^T\\gamma + u_i,\\]\n\\[\\log (Y^*_{i,j}) \\sim N(x_{i,j}^T\\beta + v_i, ~\\sigma^2), \\]\n\\[\\left( \\begin{array}{c} u_i \\\\ v_i \\\\ \\end{array} \\right) \\sim N\\left(\\left( \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right), \\left( \\begin{array}{c c} \\sigma_u^2 \u0026amp; \\rho \\sigma_u\\sigma_v \\\\ \\rho \\sigma_u \\sigma_v \u0026amp; \\sigma_v ^ 2\\end{array}\\right)\\right),\\] where \\(x_{i,j}^T = (1, t_{i,j})\\). Also let \\(\\theta = (\\gamma, \\beta, \\sigma, \\sigma_u, \\sigma_v, \\rho)\\).\nHow to use fitted.brmsfit Having obtained posterior samples from \\((\\theta, u_1,\\ldots,u_n,v_1,\\ldots,v_n)\\), we might want to look at samples from:\n\\[\\begin{align} h(u_i, v_i, \\theta; x) \u0026amp; = E(Y \\mid u_i, v_i, \\theta; x) \\\\ \u0026amp;= \\left\\lbrace 1 - \\text{logit}^{-1} ( x^T \\gamma + u_i) \\right\\rbrace \\exp(x^T\\beta + v_i + \\frac{\\sigma^2}{2}).\\end{align}\\]\nThis is some kind of patient-specific expectation of \\(Y\\), conditional on the random effects. If patient \\(i\\) is already in the model, then we can just take samples directly from the posterior. This can be achieved with the fitted method:\nfitted(fit_hurdle, newdata = data.frame(id = 1, time = \u0026quot;2\u0026quot;), robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.4034682 0.150417 0.1796645 0.8248473 will give the median and (2.5, 97.5) quantiles of \\(h(u_1, v_1, \\theta ; x)\\) at timepoint “2”.\nAlternatively, we might be more interested in \\(h(0, 0, \\theta ; x)\\), which in some sense is the patient-specific expectation of \\(Y\\) for an “average” patient with random effects fixed at zero. We can get this by setting re_formula = NA:\nfitted(fit_hurdle, newdata = data.frame(time = \u0026quot;2\u0026quot;), robust = TRUE, re_formula = NA) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.9615294 0.2401395 0.5689127 1.569304 Or, we might be more interested in \\(h(u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta ; x)\\) for a new patient \\(i\u0026#39;\\). Now we need to generate samples from the posterior distribution\n\\[f(u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta \\mid \\mathbf{y}) = f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) f(\\theta \\mid \\mathbf{y})\\]\nwe can do this by going through our posterior samples \\(\\theta^{(k)}\\) for \\(k = 1,\\ldots,K\\) and each time simulating\n\\[\\left( \\begin{array}{c} u_{i\u0026#39;}^{(k)} \\\\ v_{i\u0026#39;}^{(k)} \\\\ \\end{array} \\right) \\sim N\\left(\\left( \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right), \\left( \\begin{array}{c c} (\\sigma^{(k)}_u)^2 \u0026amp; \\rho \\sigma^{(k)}_u\\sigma^{(k)}_v \\\\ \\rho \\sigma^{(k)}_u \\sigma^{(k)}_v \u0026amp; (\\sigma^{(k)}_v)^2\\end{array}\\right)\\right).\\]\nThe way I expected this to be done is\nfitted(fit_hurdle, newdata = data.frame(id = NA, time = \u0026quot;2\u0026quot;), allow_new_levels = TRUE, sample_new_levels = \u0026quot;gaussian\u0026quot;, robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.8787689 1.276364 0.0009196831 58.3599 and this is indeed what’s happening, as can be seen by going through the steps manually\nps \u0026lt;- posterior_samples(fit_hurdle) sigma_error \u0026lt;- ps[,\u0026quot;sigma\u0026quot;] sigma_u \u0026lt;- ps[,\u0026quot;sd_id__hu_Intercept\u0026quot;] sigma_v \u0026lt;- ps[,\u0026quot;sd_id__Intercept\u0026quot;] rho \u0026lt;- ps[,\u0026quot;cor_id__Intercept__hu_Intercept\u0026quot;] n_mcmc \u0026lt;- length(rho) x \u0026lt;- data.frame(id = NA, time = \u0026quot;2\u0026quot;) ### simulate u_i\u0026#39; and v_i\u0026#39; u \u0026lt;- rnorm(n_mcmc, sd = sigma_u) ### include correlation v \u0026lt;- rnorm(n_mcmc, mean = u * sigma_v / sigma_u * rho, sd = sqrt((1 - rho^2) * sigma_v ^ 2)) ### extract draws from xi = x*gamma xi \u0026lt;- qlogis(fitted(fit_hurdle, newdata = x, re_formula = NA, dpar = \u0026quot;hu\u0026quot;, summary = FALSE)) ### extract draws from eta = x*beta eta \u0026lt;- fitted(fit_hurdle, newdata = x, re_formula = NA, dpar = \u0026quot;mu\u0026quot;, summary = FALSE) ey \u0026lt;- (1 - plogis(xi + u)) * exp(eta + v + sigma_error ^ 2 / 2) round(quantile(ey, probs = c(0.025, 0.5, 0.975)), 3) ## 2.5% 50% 97.5% ## 0.001 0.966 63.905  Unconditional expectation Instead of a patient-specific expectation, conditional on random effects, we might be more interested in targeting an overall expectation (for a new patient) where we integrate out the random effects. In other words, we want to take samples from\n\\[\\begin{align} g(\\theta; x) \u0026amp; = E(Y \\mid \\theta; x) \\\\ \u0026amp; = \\int E(Y \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}E(Y \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\\\ \u0026amp;= L^{-1}\\sum_{l = 1}^{L}\\left\\lbrace 1 - \\text{logit}^{-1} ( x^T \\gamma + u_{i\u0026#39;}^{(l)}) \\right\\rbrace \\exp(x^T\\beta + v_{i\u0026#39;}^{(l)} + \\frac{\\sigma^2}{2}).\\end{align}\\]\nThat is, I take \\(g(\\theta^{(k)}; x)\\) for \\(k = 1,\\ldots,K\\). At each \\(k\\), for \\(l= 1 \\ldots,L\\), I take indpendent draws\n\\[\\left( \\begin{array}{c} u_{i\u0026#39;}^{(l)} \\\\ v_{i\u0026#39;}^{(l)} \\\\ \\end{array} \\right) \\sim N\\left(\\left( \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right), \\left( \\begin{array}{c c} (\\sigma^{(k)}_u)^2 \u0026amp; \\rho \\sigma^{(k)}_u\\sigma^{(k)}_v \\\\ \\rho \\sigma^{(k)}_u \\sigma^{(k)}_v \u0026amp; (\\sigma^{(k)}_v)^2\\end{array}\\right)\\right).\\]\nto perform the inner Monte Carlo integration (probably not the best method for this 2-d example). I don’t think it’s possible to do this with brms so I’ve written my own code (which only works for this specific model).\ndevtools::install_github(\u0026quot;dominicmagirr/hurlong\u0026quot;) hurlong::marg_mean_q(newdata = x, nsims = 1000, fit = fit_hurdle) ## id time 2.5% 50% 97.5% ## 1 NA 2 3.872818 7.534902 18.91359  ","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"a90dd793d0d1ae36d7d656c84f4e2229","permalink":"/post/longitudinal-hurdle-models-2/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/post/longitudinal-hurdle-models-2/","section":"post","summary":"In a previous post I fit a longitudinal hurdle model using the brms package.\nlibrary(brms) summary(fit_hurdle) ## Family: hurdle_lognormal ## Links: mu = identity; sigma = identity; hu = logit ## Formula: y ~ time + (1 | q | id) ## hu ~ time + (1 | q | id) ## Data: dat (Number of observations: 800) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~id (Number of levels: 100) ## Estimate Est.","tags":[],"title":"Longitudinal hurdle models 2","type":"post"},{"authors":null,"categories":[],"content":" Data Recently, I have been modelling data that is longitudinal, contains excess zeros, and where the non-zero data is right-skewed and measured on a continuous scale, rather than being count data.\nI’ll simulate a semi-realistic example data set from a lognormal hurdle model. The “random effects” for the pr(zero) and non-zero parts of the model are negatively correlated.\nset.seed(180) ## 100 patients id \u0026lt;- 1:100 ## 2 timepoints time \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;) ## random effects u \u0026lt;- rnorm(100, sd = 2) v \u0026lt;- rnorm(100, mean = -0.95 * u, sd = sqrt((1 - 0.95^2) * 4)) # p(zero) is negatively correlated with Y* ## non-zero data (4 obs per id, at two timepoints) ystar1 \u0026lt;- exp(rnorm(400, mean = u, sd = 1)) ystar2 \u0026lt;- exp(rnorm(400, mean = -0.5 + u, sd = 1)) ## z = 1 if \u0026quot;cross hurdle\u0026quot;, i.e. if not zero z1 \u0026lt;- rbinom(400, size = 1, prob = 1 - plogis(-2 + v)) # p(cross hurdle) = 1 - p(zero) z2 \u0026lt;- rbinom(400, size = 1, prob = 1 - plogis(-1 + v)) dat \u0026lt;- data.frame(y = c(z1 * ystar1, z2 * ystar2), time = rep(time, each = 400), id = rep(id, 8)) In this data set there are 100 patients and two timepoints. For each patient, at each timepoint, I have simulated 4 independent observations (I’ve only done this to make model convergence a bit easier). The important point is that the data is correlated within patient, and also z (hurdle part) and ystar (non-zero part) are correlated, so that patients who start with a smaller (non-zero) y at the first timepoint are more likely to have y = 0 at the second timepoint. This can be clearly seen in the plot below.\nlibrary(tidyverse) ggplot(data = dat, mapping = aes(x = time, y = y, group = id)) + geom_point() + geom_line() + scale_y_log10()  Fit the model To fit the model I’m using the excellent brms package (https://github.com/paul-buerkner/brms)\nBürkner P. C. (2018). Advanced Bayesian Multilevel Modeling with the R Package brms. The R Journal. 10(1), 395-411. doi.org/10.32614/RJ-2018-017\nlibrary(brms) fit_hurdle \u0026lt;- brm(bf(y ~ time + (1 | q | id), hu ~ time + (1 | q | id)), data = dat, iter = 4000, family = hurdle_lognormal(), refresh = 0) summary(fit_hurdle) ## Family: hurdle_lognormal ## Links: mu = identity; sigma = identity; hu = logit ## Formula: y ~ time + (1 | q | id) ## hu ~ time + (1 | q | id) ## Data: dat (Number of observations: 800) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~id (Number of levels: 100) ## Estimate Est.Error l-95% CI u-95% CI Rhat ## sd(Intercept) 1.96 0.16 1.69 2.29 1.00 ## sd(hu_Intercept) 2.47 0.30 1.93 3.11 1.00 ## cor(Intercept,hu_Intercept) -0.90 0.04 -0.97 -0.81 1.00 ## Bulk_ESS Tail_ESS ## sd(Intercept) 1452 2688 ## sd(hu_Intercept) 2979 5484 ## cor(Intercept,hu_Intercept) 2539 4394 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.07 0.20 -0.34 0.45 1.00 885 1970 ## hu_Intercept -2.72 0.34 -3.43 -2.07 1.00 1662 3089 ## time2 -0.33 0.08 -0.49 -0.18 1.00 14798 5985 ## hu_time2 1.26 0.24 0.80 1.73 1.00 12452 5627 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.94 0.03 0.88 1.00 1.00 11293 6175 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1).  Inference From the output I can see that I have more-or-less recovered the parameters from my model. In practice, I could use this to make inference on the two parts of the model separately. In future posts I’ll discuss how to make inference/predictions when combining the two parts of the model.\n ","date":1571184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571184000,"objectID":"d054a2492f9c45c5d7b2d2c1bf228558","permalink":"/post/longitudinal-hurdle-models/","publishdate":"2019-10-16T00:00:00Z","relpermalink":"/post/longitudinal-hurdle-models/","section":"post","summary":"Data Recently, I have been modelling data that is longitudinal, contains excess zeros, and where the non-zero data is right-skewed and measured on a continuous scale, rather than being count data.\nI’ll simulate a semi-realistic example data set from a lognormal hurdle model. The “random effects” for the pr(zero) and non-zero parts of the model are negatively correlated.\nset.seed(180) ## 100 patients id \u0026lt;- 1:100 ## 2 timepoints time \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;) ## random effects u \u0026lt;- rnorm(100, sd = 2) v \u0026lt;- rnorm(100, mean = -0.","tags":[],"title":"Longitudinal hurdle models","type":"post"},{"authors":null,"categories":null,"content":"I\u0026rsquo;m a medical statistician. Quite simply, I\u0026rsquo;m interested in clinical trial design, analysis and interpretation.\nWhen I was around 16 or 17, I planned to study medicine. However, after doing some work experience and realizing how difficult that path would be, I decided to do a maths degree. Towards the end of my studies, I was fortunate to see a careers talk on Medical Statistics that led me to Lancaster University to do their MSc course \u0026ndash; a fantastic year, and I carried on to do a PhD. My research was on:\n group sequential trials multiple hypothesis testing adaptive designs  Google scholar\nI stayed in Lancaster a little longer, and also had a wonderful year postdoccing at the Medical University of Vienna. This time allowed me to publish some more papers, teach, and start my statistical consultancy career.\nI came back to the UK in 2015 to work at AstraZeneca. For two years, I worked in the early-phase oncology group, as part of clinical teams, contributing to the design of trials, and overseeing the statistical analyses conducted by CRO partners. For a further year, I worked as an internal statistical consultant as part of AstraZeneca\u0026rsquo;s Statistical Innovation team. This was an intense, but rewarding, experience that allowed me to work on trials in many different therapy areas. A highlight was working on new methods for dealing with non-proportional hazards in immuno-oncology. (Preprint here).\nSince January 2019 I have been working at a start-up company Cambridge Cancer Genomics, where I have become very interested in statistical modelling of longitudinal circulating-tumour DNA measurements.\nI built this website with the blogdown package.\n","date":1559520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559520000,"objectID":"6083a88ee3411b0d17ce02d738f69d47","permalink":"/about/","publishdate":"2019-06-03T00:00:00Z","relpermalink":"/about/","section":"","summary":"I\u0026rsquo;m a medical statistician. Quite simply, I\u0026rsquo;m interested in clinical trial design, analysis and interpretation.\nWhen I was around 16 or 17, I planned to study medicine. However, after doing some work experience and realizing how difficult that path would be, I decided to do a maths degree. Towards the end of my studies, I was fortunate to see a careers talk on Medical Statistics that led me to Lancaster University to do their MSc course \u0026ndash; a fantastic year, and I carried on to do a PhD.","tags":null,"title":"About","type":"page"},{"authors":null,"categories":[],"content":" Suppose we have the following data.\ndf \u0026lt;- dplyr::tibble(patient_id = as.character(1:12), treatment = rep(c(\u0026quot;C\u0026quot;, \u0026quot;E\u0026quot;), each = 6), survival_time = survival::Surv(time = c(2,6,8,11,17,24,7,9,13,22,23,25), event = c(1,1,1,1,1,0,1,1,1,0,0,0))) knitr::kable(df)   patient_id treatment survival_time    1 C 2  2 C 6  3 C 8  4 C 11  5 C 17  6 C 24+  7 E 7  8 E 9  9 E 13  10 E 22+  11 E 23+  12 E 25+    Let’s arrange the data in increasing order of survival time.\ndf_ordered \u0026lt;- dplyr::arrange(df, survival_time[,\u0026quot;time\u0026quot;]) knitr::kable(df_ordered)   patient_id treatment survival_time    1 C 2  2 C 6  7 E 7  3 C 8  8 E 9  4 C 11  9 E 13  5 C 17  10 E 22+  11 E 23+  6 C 24+  12 E 25+    Method 1: scores The first step is to estimate the survival probability from the pooled data using the Nelson-Aalen estimator. Then, to get the log-rank scores, we add 1 to the logarithm of the pooled survival estimate for all observed events. For censored events we do not add 1.\npooled_fit \u0026lt;- survival::survfit(survival_time ~ 1, data = df_ordered) df_logrank \u0026lt;- dplyr::mutate(df_ordered, pooled_s = exp(-cumsum(pooled_fit$n.event / pooled_fit$n.risk)), logrank_score = log(pooled_s) + survival_time[,\u0026quot;status\u0026quot;]) knitr::kable(df_logrank, digits = 3)   patient_id treatment survival_time pooled_s logrank_score    1 C 2 0.920 0.917  2 C 6 0.840 0.826  7 E 7 0.760 0.726  3 C 8 0.680 0.615  8 E 9 0.600 0.490  4 C 11 0.520 0.347  9 E 13 0.440 0.180  5 C 17 0.361 -0.020  10 E 22+ 0.361 -1.020  11 E 23+ 0.361 -1.020  6 C 24+ 0.361 -1.020  12 E 25+ 0.361 -1.020    To calculate the logrank score statistic we add up all the scores on the control arm.\nlogrank_score \u0026lt;- dplyr::summarise(dplyr::group_by(df_logrank, treatment), sum(logrank_score)) logrank_score ## # A tibble: 2 x 2 ## treatment `sum(logrank_score)` ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 C 1.66 ## 2 E -1.66  Method 2: observed - expected The score statistic is equivalent to the sum of “observed” - “expected” events at each event time. This is implemented in the survival package.\nlogrank_fit \u0026lt;- survival::survdiff(survival_time ~ treatment, data = df) rbind(logrank_fit$n, logrank_fit$obs - logrank_fit$exp)[2,] ## treatment=C treatment=E ## 1.664105 -1.664105  ","date":1559520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559520000,"objectID":"ccfb07f82da43dae6cf664dc84f3d19e","permalink":"/post/log-rank-test/","publishdate":"2019-06-03T00:00:00Z","relpermalink":"/post/log-rank-test/","section":"post","summary":"Suppose we have the following data.\ndf \u0026lt;- dplyr::tibble(patient_id = as.character(1:12), treatment = rep(c(\u0026quot;C\u0026quot;, \u0026quot;E\u0026quot;), each = 6), survival_time = survival::Surv(time = c(2,6,8,11,17,24,7,9,13,22,23,25), event = c(1,1,1,1,1,0,1,1,1,0,0,0))) knitr::kable(df)   patient_id treatment survival_time    1 C 2  2 C 6  3 C 8  4 C 11  5 C 17  6 C 24+  7 E 7  8 E 9  9 E 13  10 E 22+  11 E 23+  12 E 25+    Let’s arrange the data in increasing order of survival time.","tags":[],"title":"How to calculate the log-rank statistic","type":"post"},{"authors":["Dominic Magirr"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Dominic Magirr","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Dominic Magirr","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]