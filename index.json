[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a medical statistician interested in the design, analysis and interpretation of clinical trials.\nSpecific research interests include:\n group sequential trials multiple hypothesis testing adaptive designs survival analysis  Since February 2020 I have been working in the Statistical Methodology Group at Novartis in Basel.\n","date":1527292800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1527292800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a medical statistician interested in the design, analysis and interpretation of clinical trials.\nSpecific research interests include:\n group sequential trials multiple hypothesis testing adaptive designs survival analysis  Since February 2020 I have been working in the Statistical Methodology Group at Novartis in Basel.","tags":null,"title":"Dominic Magirr","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":["clinical trials","survival analysis"],"content":" In my opinion, many phase III trials in immuno-oncology are 10–20% larger than they need (ought) to be.\nThis is because the method we use for the primary analysis doesn’t match what we know about how these drugs work.\nFixing this doesn’t require anything fancy, just old-school stats from the 1960s.\nIn this new preprint I try to explain how I think it should be done.\n","date":1594339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594384351,"objectID":"bfe36dc818859ce1e0d0af51cc82531d","permalink":"/post/non-proportional-hazards-in-immuno-oncology-is-an-old-perspective-needed/","publishdate":"2020-07-10T00:00:00Z","relpermalink":"/post/non-proportional-hazards-in-immuno-oncology-is-an-old-perspective-needed/","section":"post","summary":"In my opinion, many phase III trials in immuno-oncology are 10–20% larger than they need (ought) to be.\nThis is because the method we use for the primary analysis doesn’t match what we know about how these drugs work.\nFixing this doesn’t require anything fancy, just old-school stats from the 1960s.\nIn this new preprint I try to explain how I think it should be done.","tags":[],"title":"Non-proportional hazards in immuno-oncology: is an old perspective needed?","type":"post"},{"authors":[],"categories":[],"content":" Building on my last post, I decided to build a shiny app to make it much easier to extract (approximate) patient-level data from a Kaplan-Meier curve. Video here and code here.\n","date":1591488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591535820,"objectID":"67a5c65111e1886c5fcf36d4da8a8879","permalink":"/post/shiny-app-for-enhancing-published-kaplan-meier-plots/","publishdate":"2020-06-07T00:00:00Z","relpermalink":"/post/shiny-app-for-enhancing-published-kaplan-meier-plots/","section":"post","summary":"Building on my last post, I decided to build a shiny app to make it much easier to extract (approximate) patient-level data from a Kaplan-Meier curve. Video here and code here.","tags":[],"title":"Shiny app for enhancing published Kaplan-Meier plots","type":"post"},{"authors":[],"categories":[],"content":" I made a video with step-by-step instructions for extracting (approximate) patient-level data from a Kaplan-Meier curve. It’s a useful thing to be able to do, and while there are plenty of references already (see below) I thought perhaps a full guide using completely free software was missing.\nReferences Guyot, P., Ades, A., Ouwens, M.J. et al. Enhanced secondary analysis of survival data: reconstructing the data from published Kaplan-Meier survival curves. BMC Med Res Methodol 12, 9 (2012). https://doi.org/10.1186/1471-2288-12-9\nWei, Y., \u0026amp; Royston, P. (2017). Reconstructing Time-to-event Data from Published Kaplan–Meier Curves. The Stata Journal, 17(4), 786–802. https://doi.org/10.1177/1536867X1801700402\nAuthor: Ankit Rohatgi Title: WebPlotDigitizer Website: https://automeris.io/WebPlotDigitizer Version: 4.2 Date: April, 2019 E-Mail: ankitrohatgi@hotmail.com Location: San Francisco, California, USA\nhttps://github.com/giabaio/survHE/blob/master/R/digitise.R\nSatagopan, Jaya M., Alexia Iasonos, and Joseph G. Kanik. “A reconstructed melanoma data set for evaluating differential treatment benefit according to biomarker subgroups.” Data in brief 12 (2017): 667-675. https://doi.org/10.1016/j.dib.2017.05.005\n ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591035901,"objectID":"6d2b143df58f019337b441a5ac384d26","permalink":"/post/extract-patient-level-data-from-a-kaplan-meier-plot/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/post/extract-patient-level-data-from-a-kaplan-meier-plot/","section":"post","summary":"I made a video with step-by-step instructions for extracting (approximate) patient-level data from a Kaplan-Meier curve. It’s a useful thing to be able to do, and while there are plenty of references already (see below) I thought perhaps a full guide using completely free software was missing.\nReferences Guyot, P., Ades, A., Ouwens, M.J. et al. Enhanced secondary analysis of survival data: reconstructing the data from published Kaplan-Meier survival curves.","tags":[],"title":"Extract patient-level data from a Kaplan-Meier plot","type":"post"},{"authors":[],"categories":["clinical trials"],"content":" I’ve spent a lot of time thinking about hypothesis tests in clinical trials recently. Periodically, it’s good to question the fundamentals of whether this is a good idea in general. I don’t think my views have changed. I was considering writing a blog post but then I came across this article by the editors of the Clinical Trials journal which pretty much sums it up. The article can also be found with DOI: 10.1177/1740774519846504.\n","date":1587859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587898833,"objectID":"ab08f612d9b6f35d97cdab92cde5204b","permalink":"/post/is-there-still-a-place-for-significance-testing-in-clinical-trials/","publishdate":"2020-04-26T00:00:00Z","relpermalink":"/post/is-there-still-a-place-for-significance-testing-in-clinical-trials/","section":"post","summary":"I’ve spent a lot of time thinking about hypothesis tests in clinical trials recently. Periodically, it’s good to question the fundamentals of whether this is a good idea in general. I don’t think my views have changed. I was considering writing a blog post but then I came across this article by the editors of the Clinical Trials journal which pretty much sums it up. The article can also be found with DOI: 10.","tags":[],"title":"Is there still a place for significance testing in clinical trials?","type":"post"},{"authors":[],"categories":[],"content":" This post records my unsuccessful attempts to use Bayesian model averaging when some models contain interaction terms.\nData Data comes from a phase 3 study of an immuno-oncology agent (MPDL3280A) versus chemotherapy (Docetaxel) in lung cancer. The endpoint I’m interested in is progression-free survival (PFS), and there are a number of other covariates (10) that could be prognostic and/or predictive.\nlibrary(dplyr) library(survival) library(BMA) dat = rio::import(\u0026quot;https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0134-3/MediaObjects/41591_2018_134_MOESM3_ESM.xlsx\u0026quot;, setclass = \u0026quot;tibble\u0026quot;, which = 3, na = c(\u0026quot;\u0026quot;, \u0026quot;.\u0026quot;)) %\u0026gt;% mutate(PFS.EVENT = -1 * (PFS.CNSR - 1), high_btmb = ifelse(btmb \u0026gt;= 16, \u0026quot;YES\u0026quot;, \u0026quot;NO\u0026quot;)) %\u0026gt;% select(TRT01P, PFS, PFS.EVENT, high_btmb, BAGE, race2, SEX, HIST, ECOGGR, PRIORTXC, TOBHX, blSLD, METSITES) %\u0026gt;% mutate_if(is.character, as.factor) head(dat) ## # A tibble: 6 x 13 ## TRT01P PFS PFS.EVENT high_btmb BAGE race2 SEX HIST ECOGGR PRIORTXC ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Docet… 1.28 1 NO 64 WHITE M NON-… 1 1 ## 2 Docet… 2.33 1 NO 65 WHITE M NON-… 0 1 ## 3 Docet… 1.94 1 YES 75 WHITE M SQUA… 1 1 ## 4 Docet… 12.3 1 NO 61 WHITE F NON-… 0 2 ## 5 MPDL3… 1.41 1 YES 53 WHITE F NON-… 1 1 ## 6 Docet… 8.54 1 \u0026lt;NA\u0026gt; 80 WHITE M NON-… 1 1 ## # … with 3 more variables: TOBHX \u0026lt;fct\u0026gt;, blSLD \u0026lt;dbl\u0026gt;, METSITES \u0026lt;dbl\u0026gt;  Goal of the analysis In an ideal world, I want to see which (if any) variables have an interaction with treatment, and, simultaneously, I want to produce reasonable estimates of the magnitudes of such effects. This is probably asking too much, but in a naive first attempt I plugged in all variables (along with their interaction with treatment) into BMA::bic.surv\nx \u0026lt;- dat[complete.cases(dat),] x$ECOGGR \u0026lt;- factor(x$ECOGGR) x$PRIORTXC \u0026lt;- factor(x$PRIORTXC) fit \u0026lt;- bic.surv(Surv(PFS, PFS.EVENT) ~ .*TRT01P, data = x) data.frame(variable = names(fit$output.names), probability_non_zero = fit$probne0) ## variable probability_non_zero ## 1 TRT01P 4.0 ## 2 high_btmb 0.0 ## 3 BAGE 0.0 ## 4 race2 0.0 ## 5 SEX 0.9 ## 6 HIST 15.6 ## 7 ECOGGR 30.9 ## 8 PRIORTXC 83.2 ## 9 TOBHX 0.0 ## 10 blSLD 38.9 ## 11 METSITES 98.9 ## 12 TRT01P.high_btmb.. 17.6 ## 13 TRT01PMPDL3280A.BAGE 4.3 ## 14 TRT01P.race2.. 0.0 ## 15 TRT01P.SEX.. 0.0 ## 16 TRT01P.HIST.. 0.0 ## 17 TRT01P.ECOGGR.. 0.8 ## 18 TRT01P.PRIORTXC.. 20.2 ## 19 TRT01P.TOBHX.. 0.0 ## 20 TRT01PMPDL3280A.blSLD 0.0 ## 21 TRT01PMPDL3280A.METSITES 1.9 This gave a posterior probability of 4% of the main effect of treatment being non-zero. I thought this was a bit low. In a univariable Cox model the effect of treatment is modest, but not tiny:\ncoxph(Surv(PFS, PFS.EVENT) ~ TRT01P, data = x) ## Call: ## coxph(formula = Surv(PFS, PFS.EVENT) ~ TRT01P, data = x) ## ## coef exp(coef) se(coef) z p ## TRT01PMPDL3280A -0.11178 0.89425 0.08404 -1.33 0.184 ## ## Likelihood ratio test=1.77 on 1 df, p=0.1835 ## n= 641, number of events= 584 Then I looked more closely at the interaction terms. Take, for example, the fourth most likely model:\nsf \u0026lt;- summary(fit) m4 \u0026lt;- cbind(names(sf[, \u0026quot;model 4\u0026quot;]), as.numeric(unname(sf[,\u0026quot;model 4\u0026quot;]))) m4 ## [,1] [,2] ## [1,] \u0026quot;TRT01P\u0026quot; NA ## [2,] \u0026quot; .MPDL3280A\u0026quot; NA ## [3,] \u0026quot;high_btmb\u0026quot; NA ## [4,] \u0026quot; .YES\u0026quot; NA ## [5,] \u0026quot;BAGE\u0026quot; NA ## [6,] \u0026quot;race2\u0026quot; NA ## [7,] \u0026quot; .OTHER\u0026quot; NA ## [8,] \u0026quot; .WHITE\u0026quot; NA ## [9,] \u0026quot;SEX\u0026quot; NA ## [10,] \u0026quot; .M\u0026quot; NA ## [11,] \u0026quot;HIST\u0026quot; NA ## [12,] \u0026quot; .SQUAMOUS\u0026quot; NA ## [13,] \u0026quot;ECOGGR\u0026quot; NA ## [14,] \u0026quot; .1\u0026quot; NA ## [15,] \u0026quot;PRIORTXC\u0026quot; NA ## [16,] \u0026quot; .2\u0026quot; \u0026quot;-0.36197\u0026quot; ## [17,] \u0026quot;TOBHX\u0026quot; NA ## [18,] \u0026quot; .NEVER\u0026quot; NA ## [19,] \u0026quot; .PREVIOUS\u0026quot; NA ## [20,] \u0026quot;blSLD\u0026quot; \u0026quot;0.00243\u0026quot; ## [21,] \u0026quot;METSITES\u0026quot; \u0026quot;0.11115\u0026quot; ## [22,] \u0026quot;TRT01P.high_btmb..\u0026quot; NA ## [23,] \u0026quot; .TRT01PMPDL3280A:high_btmbYES\u0026quot; \u0026quot;-0.268703\u0026quot; ## [24,] \u0026quot;TRT01PMPDL3280A.BAGE\u0026quot; NA ## [25,] \u0026quot;TRT01P.race2..\u0026quot; NA ## [26,] \u0026quot; .TRT01PMPDL3280A:race2OTHER\u0026quot; NA ## [27,] \u0026quot; .TRT01PMPDL3280A:race2WHITE\u0026quot; NA ## [28,] \u0026quot;TRT01P.SEX..\u0026quot; NA ## [29,] \u0026quot; .TRT01PMPDL3280A:SEXM\u0026quot; NA ## [30,] \u0026quot;TRT01P.HIST..\u0026quot; NA ## [31,] \u0026quot; .TRT01PMPDL3280A:HISTSQUAMOUS\u0026quot; NA ## [32,] \u0026quot;TRT01P.ECOGGR..\u0026quot; NA ## [33,] \u0026quot; .TRT01PMPDL3280A:ECOGGR1\u0026quot; NA ## [34,] \u0026quot;TRT01P.PRIORTXC..\u0026quot; NA ## [35,] \u0026quot; .TRT01PMPDL3280A:PRIORTXC2\u0026quot; NA ## [36,] \u0026quot;TRT01P.TOBHX..\u0026quot; NA ## [37,] \u0026quot; .TRT01PMPDL3280A:TOBHXNEVER\u0026quot; NA ## [38,] \u0026quot; .TRT01PMPDL3280A:TOBHXPREVIOUS\u0026quot; NA ## [39,] \u0026quot;TRT01PMPDL3280A.blSLD\u0026quot; NA ## [40,] \u0026quot;TRT01PMPDL3280A.METSITES\u0026quot; NA ## [41,] \u0026quot;\u0026quot; NA ## [42,] \u0026quot;nVar\u0026quot; \u0026quot;4\u0026quot; ## [43,] \u0026quot;BIC\u0026quot; \u0026quot;-17.33416\u0026quot; ## [44,] \u0026quot;post prob\u0026quot; \u0026quot;0.049\u0026quot; …this model contains 4 parameters: main effects for prior treatment, baseline sum of diameters, number of metastases, and an effect of being both TRT01P == MPDL3280A and high_btmb == YES. Note that there is no variable for the main effect of treatment (nor TMB) in this model. That is, we are fitting models with interaction terms where their main effects are not included. This is, in general, a bad thing. So bad, in fact, that R will not even allow me to fit this model using standard syntax. If I try to do it…\ncoxph(Surv(PFS, PFS.EVENT) ~ PRIORTXC + blSLD + METSITES + TRT01P:high_btmb, data = x, method = \u0026quot;breslow\u0026quot;, iter.max = 30) ## Call: ## coxph(formula = Surv(PFS, PFS.EVENT) ~ PRIORTXC + blSLD + METSITES + ## TRT01P:high_btmb, data = x, method = \u0026quot;breslow\u0026quot;, iter.max = 30) ## ## coef exp(coef) se(coef) z ## PRIORTXC2 -0.3612339 0.6968160 0.0987439 -3.658 ## blSLD 0.0022519 1.0022544 0.0009478 2.376 ## METSITES 0.1109506 1.1173397 0.0318342 3.485 ## TRT01PDocetaxel:high_btmbNO 0.2579332 1.2942524 0.1415469 1.822 ## TRT01PMPDL3280A:high_btmbNO 0.2402823 1.2716080 0.1405986 1.709 ## TRT01PDocetaxel:high_btmbYES 0.3815722 1.4645855 0.1688867 2.259 ## TRT01PMPDL3280A:high_btmbYES NA NA 0.0000000 NA ## p ## PRIORTXC2 0.000254 ## blSLD 0.017509 ## METSITES 0.000492 ## TRT01PDocetaxel:high_btmbNO 0.068418 ## TRT01PMPDL3280A:high_btmbNO 0.087452 ## TRT01PDocetaxel:high_btmbYES 0.023862 ## TRT01PMPDL3280A:high_btmbYES NA ## ## Likelihood ratio test=43.91 on 6 df, p=7.704e-08 ## n= 641, number of events= 584 …then rather than receiving a model with 4 variables, I have been given a model with 6 variables. The main effects of TRT01P and high_btmb have been included as well. (This is the behaviour for factors; you are allowed to include a continous variable in an interaction term without including it as a main effect).\nIn conclusion, the output from the model averaging is more-or-less uninterpretable when including interactions.\n Predictive distributions Although inference on individual parameters is not possible, it’s still likely that BMA will produce good predictions when I take the whole (mixed) model fit into account. At this point, I had a half-formed plan of re-fitting all of the ‘best’ models using flexsurv::flexsurvspline to give me the baseline hazards as well, and then combining the output to give me personalized predictive survival curves.\nBut then I realized how much work that would involve, especially without access to R’s standard model syntax. It’s possible with some effort to reproduce the “4th best model”:\nmm_full \u0026lt;- model.matrix(Surv(PFS, PFS.EVENT) ~ .*TRT01P, data = x) mm_4 \u0026lt;- mm_full[ ,attr(mm_full, \u0026quot;assign\u0026quot;) %in% which(fit$which[4,])] coxph(Surv(PFS, PFS.EVENT) ~ ., data = cbind(x[,c(\u0026quot;PFS\u0026quot;, \u0026quot;PFS.EVENT\u0026quot;)], as.data.frame(mm_4)), method = \u0026quot;breslow\u0026quot;, iter.max = 30) ## Call: ## coxph(formula = Surv(PFS, PFS.EVENT) ~ ., data = cbind(x[, c(\u0026quot;PFS\u0026quot;, ## \u0026quot;PFS.EVENT\u0026quot;)], as.data.frame(mm_4)), method = \u0026quot;breslow\u0026quot;, ## iter.max = 30) ## ## coef exp(coef) se(coef) z ## PRIORTXC2 -0.3619700 0.6963033 0.0987112 -3.667 ## blSLD 0.0024295 1.0024325 0.0009301 2.612 ## METSITES 0.1111500 1.1175626 0.0317456 3.501 ## `TRT01PMPDL3280A:high_btmbYES` -0.2687027 0.7643705 0.1310337 -2.051 ## p ## PRIORTXC2 0.000245 ## blSLD 0.008997 ## METSITES 0.000463 ## `TRT01PMPDL3280A:high_btmbYES` 0.040302 ## ## Likelihood ratio test=42.81 on 4 df, p=1.131e-08 ## n= 641, number of events= 584 However, functions like predict etc also wouldn’t work, and I’d have to do everything manually, and it probably wasn’t worth the effort.\n Force all main effects to be included in the model It’s possible to force all main effects to be included in the model\nfit_2 \u0026lt;- bic.surv(Surv(PFS, PFS.EVENT) ~ .*TRT01P, data = x, prior.param = c(rep(1, 11), # main effects rep(0.5, 10))) # interactions with trt Then I think I’m back to a situation where I can safely look at the effect of individual parameters…\nplot(fit_2) …but there’s not a lot of regularization going on now. Doesn’t look a lot different to a backward elimination, or something similar.\n Alternative ways forward Abandon model averaging and use something else: ridge/lasso regression or their Bayesian alternatives. Especially if I need to increase the number of dimensions as well. This is what I intend to explore next.\n ","date":1574208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574265300,"objectID":"bf81202391c9ce527a4e070621142fc0","permalink":"/post/bayesian-model-averaging-struggles-with-interaction/","publishdate":"2019-11-20T00:00:00Z","relpermalink":"/post/bayesian-model-averaging-struggles-with-interaction/","section":"post","summary":"This post records my unsuccessful attempts to use Bayesian model averaging when some models contain interaction terms.\nData Data comes from a phase 3 study of an immuno-oncology agent (MPDL3280A) versus chemotherapy (Docetaxel) in lung cancer. The endpoint I’m interested in is progression-free survival (PFS), and there are a number of other covariates (10) that could be prognostic and/or predictive.\nlibrary(dplyr) library(survival) library(BMA) dat = rio::import(\u0026quot;https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0134-3/MediaObjects/41591_2018_134_MOESM3_ESM.xlsx\u0026quot;, setclass = \u0026quot;tibble\u0026quot;, which = 3, na = c(\u0026quot;\u0026quot;, \u0026quot;.","tags":["Technical"],"title":"Bayesian model averaging: struggles with interaction","type":"post"},{"authors":[],"categories":[],"content":" In my last post I had just used the BMA package to do a quick analysis with Bayesian model averaging. The method rests on a lovely piece of theory. Posterior model probabilities are calculated via a simple application of Bayes rule:\n\\[p(M_1 | D) = \\frac{p(D | M_1)p(M_1)}{\\sum_k p(D | M_k)p(M_k)}\\]\nbut it’s a challenge calculating the terms\n\\[p(D | M_k) = \\int p(D | \\theta_k, M_k)p(\\theta_k | M_k) d\\theta_k.\\]\nRafferty (Section 4) gives a top-class explanation for why\n\\[\\log p(D | M_k) \\approx \\log p(D | \\hat{\\theta},M_k) - (d/2)\\log n\\]\nwhere \\(d\\) and \\(n\\) are the number of parameters and “observations”, respectively, in model \\(M_k\\). This is the same as saying\n\\[p(D | M_k) \\approx \\exp\\left\\lbrace -0.5 \\times \\text{BIC}(M_k)\\right\\rbrace,\\]\nwhere \\(BIC\\) is the Bayesian Information Criteria.\nReproduce BMA results manually I find it helpful sometimes to do things the long way round. We started with the survival::veterans data set, changing some covariates from numeric to categorical\nlibrary(survival) library(BMA) data(veteran) veteran$trt \u0026lt;- factor(veteran$trt) veteran$prior \u0026lt;- factor(veteran$prior) Now we can fit a Cox model to each combination of covariates\n## Extract names of covariates x_names \u0026lt;- setdiff(names(veteran), c(\u0026quot;time\u0026quot;, \u0026quot;status\u0026quot;)) ## Find all combinations of covariates xs \u0026lt;- c(\u0026quot;1\u0026quot;, unlist(purrr::map(seq_along(x_names), combn, x = x_names, FUN = function(x) paste(x, collapse = \u0026quot; + \u0026quot;)))) ## Turn into model formulas fs \u0026lt;- paste(\u0026quot;Surv(time, status)\u0026quot;, xs, sep = \u0026quot; ~ \u0026quot;) ## Fit models ms \u0026lt;- purrr::map(fs, function(f) coxph(as.formula(f), data = veteran, method = \u0026quot;breslow\u0026quot;, iter.max = 30)) From the models, we can extract the BICs, turn them into (approximate) marginal probabilites of the data, and then into (approximate) posterior model probabilities (where we assume equal prior probabilities for each model)…\nget_bic \u0026lt;- function(m){ log(m$nevent) * length(coef(m)) - 2 * rev(m$loglik)[1] } bics \u0026lt;- purrr::map_dbl(ms, get_bic) p_ds \u0026lt;- exp(-0.5 * bics) p_ms \u0026lt;- p_ds / (sum(p_ds)) ## calculate cut-off for model inclusion odds_ms \u0026lt;- p_ms / (1 - p_ms) cut_off \u0026lt;- 0.05 * max(odds_ms) / (1 + 0.05 * max(odds_ms)) plot(seq_along(p_ms), p_ms, xlab = \u0026quot;Model index\u0026quot;, ylab = \u0026quot;p(M|D)\u0026quot;) abline(h = cut_off, col = 2) Most of the models have very low posterior probability. Only 6 cross the default threshold for inclusion in the final model (odds ratio less than 20 compared to the best model). I can extract those models and re-standardise:\ninclude_indices \u0026lt;- which(p_ms \u0026gt;= cut_off) ms_include \u0026lt;- ms[include_indices] p_ms_include \u0026lt;- p_ms[include_indices] / sum(p_ms[include_indices]) data.frame(x = xs[include_indices], p = round(p_ms_include,3)) ## x p ## 1 karno 0.154 ## 2 celltype + karno 0.562 ## 3 trt + celltype + karno 0.113 ## 4 celltype + karno + diagtime 0.054 ## 5 celltype + karno + age 0.061 ## 6 celltype + karno + prior 0.056 You can check that this matches the output from the BMA::bic.surv summary.\nMarginal posterior distributions of coefficients This part is messy, to be honest, but I wanted to finish the job. To reproduce the posterior density plots, I start by putting posterior means and variances into a table, with corresponding posterior model probabilities:\npost_df \u0026lt;- purrr::map2_dfr(ms_include, p_ms_include, function(m, p) data.frame(mean = unname(coef(m)), var = unname(diag(vcov(m))), p = p, covariate = names(coef(m)), stringsAsFactors = FALSE)) head(post_df) ## mean var p covariate ## 1 -0.03324294 2.573811e-05 0.1540161 karno ## 2 0.71214811 6.387773e-02 0.5618671 celltypesmallcell ## 3 1.15080136 8.576748e-02 0.5618671 celltypeadeno ## 4 0.32514265 7.655965e-02 0.5618671 celltypelarge ## 5 -0.03090393 2.681814e-05 0.5618671 karno ## 6 0.25731308 4.025205e-02 0.1130516 trt2 I can turn this into a function to find the non-zero densities:\nnon_zero_density \u0026lt;- function(x, select_cov){ df \u0026lt;- post_df[post_df$covariate == select_cov,] ds \u0026lt;- purrr::pmap_dfc(list(p = df$p, m = df$mean, v = df$var), function(p,m,v) p * dnorm(x, m, sqrt(v))) rowSums(ds) } For example (I’ve cheated here by looking ahead for a sensible range of x for trt2)…\nx_trt2 \u0026lt;- seq(-0.5, 1, length.out = 100) nzd_trt2 \u0026lt;- non_zero_density(x = x_trt2, select_cov = \u0026quot;trt2\u0026quot;) plot(x_trt2, nzd_trt2, type = \u0026quot;l\u0026quot;, xlab = \u0026quot;coefficient for trt2\u0026quot;, ylab = \u0026quot;non-zero density\u0026quot;) This is looking good now, compared to last time, but we still need to find the posterior probability that trt2 is excluded from the model, and sort out the y-axis scale (in ?plot.bic.surv it says “The nonzero part of the distribution is scaled so that the maximum height is equal to the probability that the coefficient is nonzero.”)…\nprob_nonzero \u0026lt;- function(select_cov){ df \u0026lt;- post_df[post_df$covariate == select_cov,] sum(df$p) } rescale_nzd \u0026lt;- function(nzd, pnz){ nzd / max(nzd) * pnz } pnz_trt2 \u0026lt;- prob_nonzero(\u0026quot;trt2\u0026quot;) rnzd_trt2 \u0026lt;- rescale_nzd(nzd_trt2, pnz_trt2) plot(x_trt2, rnzd_trt2, type = \u0026quot;l\u0026quot;, xlab = \u0026quot;coefficient for trt2\u0026quot;, ylab = \u0026quot; \u0026quot;, ylim = c(0,1)) points(c(0,0), c(0, 1 - pnz_trt2), type = \u0026#39;l\u0026#39;, lwd = 3) Yep, there we go! I could now repeat this for the other covariates if I wanted.\n  ","date":1572912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572947750,"objectID":"4680c400b3b124f34f12bc917eb1c4e1","permalink":"/post/bma-2/","publishdate":"2019-11-05T00:00:00Z","relpermalink":"/post/bma-2/","section":"post","summary":"In my last post I had just used the BMA package to do a quick analysis with Bayesian model averaging. The method rests on a lovely piece of theory. Posterior model probabilities are calculated via a simple application of Bayes rule:\n\\[p(M_1 | D) = \\frac{p(D | M_1)p(M_1)}{\\sum_k p(D | M_k)p(M_k)}\\]\nbut it’s a challenge calculating the terms\n\\[p(D | M_k) = \\int p(D | \\theta_k, M_k)p(\\theta_k | M_k) d\\theta_k.","tags":["R Packages","Technical"],"title":"BMA-2","type":"post"},{"authors":[],"categories":[],"content":" I’ve used the BMA package for Bayesian model averaging a couple of times this year and think it’s great.\n(https://www.r-project.org/doc/Rnews/Rnews_2005-2.pdf)\nThe situation is this: you have a data set with a response variable and multiple explanatory variables (the goal could be prediction or inference). It’s crying out for a regression analysis. But you don’t want to use a naive variable selection method because you know that’s bad. Is there something smarter you can do, and do quickly?\nExample Let’s take the veteran data set from the survival package. This data comes from a randomized trial of two chemotherapies with a primary endpoint of overall survival. There are several covariates, which we can take a look at…\nlibrary(survival) library(BMA) data(veteran) str(veteran) ## \u0026#39;data.frame\u0026#39;: 137 obs. of 8 variables: ## $ trt : num 1 1 1 1 1 1 1 1 1 1 ... ## $ celltype: Factor w/ 4 levels \u0026quot;squamous\u0026quot;,\u0026quot;smallcell\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ time : num 72 411 228 126 118 10 82 110 314 100 ... ## $ status : num 1 1 1 1 1 1 1 1 1 0 ... ## $ karno : num 60 70 60 60 70 20 40 80 50 70 ... ## $ diagtime: num 7 5 3 9 11 5 10 29 18 6 ... ## $ age : num 69 64 38 63 65 49 69 68 43 70 ... ## $ prior : num 0 10 0 10 10 0 10 0 0 0 ... karno refers to Karnofsky performance score (from 0-100 where higher is better). Something’s gone wrong with the prior treatment variable: this should be a yes/no. Variable trt should also be categorical:\nveteran$trt \u0026lt;- factor(veteran$trt) veteran$prior \u0026lt;- factor(veteran$prior) We can use the bic.surv function to fit a Cox model for every possible combination of the explanatory variables.\ntest.bic.surv\u0026lt;- bic.surv(Surv(time,status) ~ ., data = veteran, factor.type = TRUE) The summary gives us the posterior probability for each model (by default each model is given the same prior probability)\nsummary(test.bic.surv, conditional=FALSE, digits=2) ## ## Call: ## bic.surv.formula(f = Surv(time, status) ~ ., data = veteran, factor.type = TRUE) ## ## ## 6 models were selected ## Best 5 models (cumulative posterior probability = 0.95 ): ## ## p!=0 EV SD model 1 model 2 model 3 ## trt 11.3 ## .2 0.02909 0.1058 . . 0.2573 ## celltype 84.6 ## .smallcell 0.61564 0.3538 0.7121 . 0.8196 ## .adeno 0.97619 0.4965 1.1508 . 1.1477 ## .large 0.28260 0.2830 0.3251 . 0.3930 ## karno 100.0 -0.03135 0.0052 -0.0309 -0.0332 -0.0311 ## diagtime 5.4 0.00018 0.0020 . . . ## age 6.1 -0.00036 0.0026 . . . ## prior 5.6 ## .10 0.00576 0.0542 . . . ## ## nVar 2 1 3 ## BIC -39.3626 -36.7742 -36.1558 ## post prob 0.562 0.154 0.113 ## model 4 model 5 ## trt ## .2 . . ## celltype ## .smallcell 0.7208 0.7264 ## .adeno 1.1643 1.1765 ## .large 0.3215 0.3276 ## karno -0.0318 -0.0311 ## diagtime . . ## age -0.0059 . ## prior ## .10 . 0.1026 ## ## nVar 3 3 ## BIC -34.9292 -34.7553 ## post prob 0.061 0.056 It’s possible to plot the marginal posterior distribution for each covariate (averaged according to posterior model probabilities):\nplot(test.bic.surv) Conclusions  I’ve found these graphs really useful for showing the boss. When you want to show-off potentially interesting relationships, but prevent them (to some extent) from getting carried away. This is an intuitive way to do it.\n A drawback of BMA is that it doesn’t handle missing data. Any row with NAs will be removed.\n In a future post I’ll go more into the details of what BMA is doing.\n    ","date":1572825600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572880751,"objectID":"603d3646c397f5798fde2b3e5d2d4eb5","permalink":"/post/bma/","publishdate":"2019-11-04T00:00:00Z","relpermalink":"/post/bma/","section":"post","summary":"I’ve used the BMA package for Bayesian model averaging a couple of times this year and think it’s great.\n(https://www.r-project.org/doc/Rnews/Rnews_2005-2.pdf)\nThe situation is this: you have a data set with a response variable and multiple explanatory variables (the goal could be prediction or inference). It’s crying out for a regression analysis. But you don’t want to use a naive variable selection method because you know that’s bad. Is there something smarter you can do, and do quickly?","tags":["R Packages","Technical"],"title":"BMA","type":"post"},{"authors":null,"categories":[],"content":" Previously, I started discussing the flexsurv package. I used it to fit a Weibull model. This is implemented as an accelerated failure time model. It is also a proportional hazards model (although, as I found previously, converting between the two is not so straightforward, but it can be done by SurvRegCensCov).\nNow let’s compare Weibull regression with Cox regression. Firstly, Weibull regression:\n assumes proportional hazards; the number of parameters is equal to \\(k + 2\\), where \\(k\\) is the number of covariates; we can estimate things like the median, \\(P(S\u0026gt;s^*)\\), etc. from the model… but the model might be too restrictive – we won’t estimate these things very well.  Cox regression:\n assumes proportional hazards; there are \\(k\\) parameters (one for each covariate); we don’t estimate the baseline hazard… which means we don’t get estimates for things like the median, \\(P(S\u0026gt;s^*)\\), etc.  flexsurv has a function flexsurvspline which allows one to bridge the gap between Weibull regression and Cox regresssion.\nFrom Weibull regression to Cox regression. For a Weibull distribution with shape \\(a\\), scale \\(b\\), covariates \\(x\\), and regression coefficents \\(\\gamma\\), the survival probability is\n\\[S(t; x) = \\exp\\left\\lbrace - \\left( \\frac{t}{b \\cdot \\exp(x^T\\gamma)} \\right) ^ a\\right\\rbrace\\]\nSince survival is related to the log-cumulative hazard via \\(S=\\exp(-H)\\), this means that\n\\[\\log H(t;x) = a\\log t - a\\log(b) - a x^T\\gamma\\]\nIn words, the log-cumulative hazard has a linear relationship with (log-) time, with the intercept depending on the value of \\(x\\). For a given data set, we can check if this is reasonable by looking at non-paramteric estimates, \\(\\log \\hat{H}(t; x)\\).\nlibrary(survival) library(flexsurv) library(ggplot2) ### Non-parametric analysis fit_km \u0026lt;- survfit(Surv(futime, death) ~ trt, data = myeloid) t_seq \u0026lt;- seq(1, 2500, length.out = 1000) km_sum \u0026lt;- summary(fit_km, times = t_seq, extend = TRUE) ### Weibull regression fit_weibull \u0026lt;- flexsurvreg(Surv(futime, death) ~ trt, dist = \u0026quot;weibull\u0026quot;, data = myeloid) a \u0026lt;- fit_weibull$res[\u0026quot;shape\u0026quot;, \u0026quot;est\u0026quot;] b \u0026lt;- fit_weibull$res[\u0026quot;scale\u0026quot;, \u0026quot;est\u0026quot;] trtB \u0026lt;- fit_weibull$res[\u0026quot;trtB\u0026quot;, \u0026quot;est\u0026quot;] ### plot log-cumulative hazard against log-time df \u0026lt;- data.frame(log_time = log(rep(t_seq, 2)), logcumhaz = log(-log(km_sum$surv)), trt = rep(c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;), each = 1000), logcumhaz_w = c(a * (log(t_seq) - log(b)), a * (log(t_seq) - log(b) - trtB))) ggplot(data = df, mapping = aes(x = log_time, y = logcumhaz, colour = trt)) + geom_line() + geom_line(mapping = aes(x = log_time, y = logcumhaz_w, colour = trt), linetype = 2) + theme_bw() + scale_x_continuous(limits = c(2,8)) + scale_y_continuous(limits = c(-6,0)) What’s going on here? Well, the first thing to acknowledge is that the hazards only appear to be proportional after about 150 (\\(e^5\\)) days. I’m not sure I would immediately abandon a proportional-hazards model, though, as most of the events happen when the hazards are proportional (only 10-15% of the events happen before day 150), so the right-hand-side of the plot is far more important. Looking to the right then: the relationship between the log-cumulative hazard and log-time is not really linear. The distance between the two lines is roughly the same for the two models (Weibull and non-parametric), suggesting that the Weibull model does ok at estimating the hazard ratio. However, the lack of linearity will lead to poor estimates for the medians, \\(P(S\u0026gt;s^*)\\), etc., as can be confirmed by plotting the survival curves:\nggplot(data = df, mapping = aes(x = exp(log_time), y = exp(-exp(logcumhaz)), colour = trt)) + geom_line() + geom_line(mapping = aes(x = exp(log_time), y = exp(-exp(logcumhaz_w)), colour = trt), linetype = 2) + theme_bw() To improve the model, given this lack of linearity, it seems quite natural to change from\n\\[\\log H(t;x) = a\\log t - a\\log(b) - a x^T\\gamma\\]\nto\n\\[\\log H(t;x) = s(\\log t) + x^T\\beta\\]\nwhere \\(s(\\log t)\\) is a natural cubic spline function of (log) time. One can make the model more/less flexible by choosing a large/small number of knots. By default, the knots are placed at quantiles of the uncensored event times. How many knots are required? I don’t really have a good answer for this: one or two. At most, three? In this example, I’m using two inner knots, placed at 33% and 66% of the uncensored event times (indicated by vertical lines):\nfit_spline_2 \u0026lt;- flexsurvspline(Surv(futime, death) ~ trt, data = myeloid, k = 2, scale = \u0026quot;hazard\u0026quot;) spline_2_sum \u0026lt;- summary(fit_spline_2, t = t_seq, type = \u0026quot;cumhaz\u0026quot;) df2 \u0026lt;- cbind(df, data.frame(logcumhaz_s2 = log(c(spline_2_sum$`trt=A`[\u0026quot;est\u0026quot;]$est, spline_2_sum$`trt=B`[\u0026quot;est\u0026quot;]$est)))) ggplot(data = df2, mapping = aes(x = log_time, y = logcumhaz, colour = trt)) + geom_line() + geom_line(mapping = aes(x = log_time, y = logcumhaz_s2, colour = trt), linetype = 2) + theme_bw() + scale_x_continuous(limits = c(2,8)) + scale_y_continuous(limits = c(-6,0)) + geom_vline(xintercept = fit_spline_2$knots) This looks a lot better, and we can see the improvement in the survival curves:\nggplot(data = df2, mapping = aes(x = exp(log_time), y = exp(-exp(logcumhaz)), colour = trt)) + geom_line() + geom_line(mapping = aes(x = exp(log_time), y = exp(-exp(logcumhaz_s2)), colour = trt), linetype = 2) + theme_bw()  From Cox regression to Weibull regression. If we start out from Cox regression\n\\[h(t;x)=h_0(t)\\exp(x^T\\beta)\\]\nthis means that\n\\[\\log H(t;x) = \\log H_0(t;x) + x^T\\beta\\]\nWe estimate the parameters \\(\\beta\\) from the partial likelihood, and don’t estimate \\(\\log H_0(t;x)\\). So \\(\\log H_0(t;x)\\) can be anything. However, with the flexsurvspline function, as long as we use enough knots, \\(s(\\log(t))\\) can be more-or-less anything (smooth), so the two methods will give the same information about \\(\\beta\\).\n Conclusions I can’t really see any reason not to switch from a Cox model to flexsurvspline. You don’t lose anything in terms of inference on \\(\\beta\\), only gain a nice estimate for the baseline hazard. Also, inference is all based on maximum likelihood. No special theory required.\nFrom the other side, if you start out from Weibull regression, and then realise that Weibull is the wrong model, you don’t have to think too hard about how to choose a better model, you know that flexsurvspline will be good (assuming proportional hazards is correct: for non-proportional hazards you may have to think harder).\nWhat can go wrong? In small sample sizes, I guess there could be issues with over-fitting if too many knots are chosen. But given a decent sample size, I can’t see any problems. I would be interested to see a \\(\\log H_0(t;x)\\) that is highly wiggly – doesn’t seem likely in practice.\n ","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"4e99202e9d8bdf2882886313032b1cea","permalink":"/post/flexsurv-2/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/post/flexsurv-2/","section":"post","summary":"Previously, I started discussing the flexsurv package. I used it to fit a Weibull model. This is implemented as an accelerated failure time model. It is also a proportional hazards model (although, as I found previously, converting between the two is not so straightforward, but it can be done by SurvRegCensCov).\nNow let’s compare Weibull regression with Cox regression. Firstly, Weibull regression:\n assumes proportional hazards; the number of parameters is equal to \\(k + 2\\), where \\(k\\) is the number of covariates; we can estimate things like the median, \\(P(S\u0026gt;s^*)\\), etc.","tags":["R Packages","Technical"],"title":"flexsurv 2","type":"post"},{"authors":null,"categories":[],"content":" I’m going to write about some of my favourite R packages. I’ll start with flexsurv (https://github.com/chjackson/flexsurv-dev) by Chris Jackson, which can be used to fit all kinds of parametric models to survival data. It can really do a lot, but I’ll pick out just 2 cool things I like about it:\nFit a standard survival model, but where it’s slightly easier to work out what the parameters mean. Fit a proportional hazards model, which is a lot like a Cox model, but where you also model the baseline hazard using a spline.  1. Consistent parameter values As mentioned in the tutorial (https://www.jstatsoft.org/article/view/v070i08), for simple models flexsurvreg acts as a wrapper for survival::survreg, but where the parameters in the output match those of dweibull.\nWith survival::survreg I would do, e.g.:\ndat_ovarian \u0026lt;- survival::ovarian dat_ovarian$rx \u0026lt;- factor(dat_ovarian$rx) library(survival) fit_survreg = survreg(Surv(futime, fustat) ~ rx, dist = \u0026quot;weibull\u0026quot;, data = dat_ovarian) summary(fit_survreg) ## ## Call: ## survreg(formula = Surv(futime, fustat) ~ rx, data = dat_ovarian, ## dist = \u0026quot;weibull\u0026quot;) ## Value Std. Error z p ## (Intercept) 6.825 0.344 19.84 \u0026lt;2e-16 ## rx2 0.559 0.529 1.06 0.29 ## Log(scale) -0.121 0.251 -0.48 0.63 ## ## Scale= 0.886 ## ## Weibull distribution ## Loglik(model)= -97.4 Loglik(intercept only)= -98 ## Chisq= 1.18 on 1 degrees of freedom, p= 0.28 ## Number of Newton-Raphson Iterations: 5 ## n= 26 Then I would look around online for an explanation of the output e.g. (https://stats.stackexchange.com/questions/159044/weibull-survival-model-in-r). There is also an explanation in ?survreg.\nOn the other hand, using flexsurv:\nlibrary(flexsurv) fit_flexsurvreg = flexsurvreg(Surv(futime, fustat) ~ rx, dist = \u0026quot;weibull\u0026quot;, data = dat_ovarian) fit_flexsurvreg ## Call: ## flexsurvreg(formula = Surv(futime, fustat) ~ rx, data = dat_ovarian, ## dist = \u0026quot;weibull\u0026quot;) ## ## Estimates: ## data mean est L95% U95% se exp(est) ## shape NA 1.129 0.690 1.848 0.284 NA ## scale NA 920.128 468.868 1805.704 316.508 NA ## rx2 0.500 0.559 -0.478 1.597 0.529 1.749 ## L95% U95% ## shape NA NA ## scale NA NA ## rx2 0.620 4.936 ## ## N = 26, Events: 12, Censored: 14 ## Total time at risk: 15588 ## Log-likelihood = -97.36415, df = 3 ## AIC = 200.7283 The parameters shape and scale correspond to dweibull. So I don’t have to think any further? Not quite: I still have to work out what the estimate of rx2 is doing. I might look at exp(est) = 1.749 and somehow expect this to be a hazard ratio. It’s not. It’s a multiplicative effect on the scale parameter. So when rx = 1 the scale is 920.1, and when rx = 2 the scale is 920.1 * 1.749. The hazard ratio (treatment 2 vs treatment 1) is\n\\[\\begin{align} \\frac{h_2(x)}{h_1(x)} \u0026amp; = \\left( \\frac{b_1}{b_2} \\right)^a \\\\ \u0026amp; = \\left( \\frac{920.1}{920.1 \\times 1.749} \\right)^{1.129}\\\\ \u0026amp; = 0.53 \\end{align} \\]\nwhere \\(a\\) is the common shape parameter, and \\(b_1\\) and \\(b_2\\) are the scale parameters.\nOnce I had started writing this post, I realized that it’s actually not straightforward to make inference on the hazard ratio using flexsurv. For working out variances/covariances, the survreg parameterization is indeed better. I looked around for other R packages in this space, and found SurvRegCensCov, which can do this conversion automatically for you:\nlibrary(SurvRegCensCov) ConvertWeibull(fit_survreg)$HR ## HR LB UB ## rx2 0.5318051 0.1683444 1.679989 For completeness, using flexsurv, the log-hazard ratio is\n\\[\\begin{align} \\log\\left( \\frac{h_2(x)}{h_1(x) }\\right) \u0026amp; = a\\left\\lbrace \\log(b_1) - \\log(b_2) \\right\\rbrace \\\\ \u0026amp; = -\\exp(\\log(a)) \\times \\left\\lbrace \\log(b_2) - \\log(b_1) \\right\\rbrace \\end{align}\\]\nI can extract the terms \\(\\alpha:=\\log(a)\\) and \\(\\beta:=\\log(b_2) - \\log(b_1)\\) from the fit_flexsurvreg object, as well as their (co)variance.\nalpha \u0026lt;- fit_flexsurvreg$res.t[\u0026quot;shape\u0026quot;, \u0026quot;est\u0026quot;] beta \u0026lt;- fit_flexsurvreg$res.t[\u0026quot;rx2\u0026quot;, \u0026quot;est\u0026quot;] cov_alpha_beta \u0026lt;- vcov(fit_flexsurvreg)[c(\u0026quot;shape\u0026quot;, \u0026quot;rx2\u0026quot;), c(\u0026quot;shape\u0026quot;, \u0026quot;rx2\u0026quot;)] Then work out the variance of the log-hazard ratio using the delta method.\n\\[\\text{var}\\left\\lbrace -\\beta\\exp(\\alpha) \\right\\rbrace = (-\\beta\\exp(\\alpha), -\\exp(\\alpha)) \\text{Cov}(\\alpha, \\beta) \\left( \\begin{array}{c} -\\beta\\exp(\\alpha) \\\\ -\\exp(\\alpha) \\end{array}\\right)\\]\ngrad \u0026lt;- matrix(c(-beta*exp(alpha), -exp(alpha)), ncol = 1) var_lhr = t(grad) %*% cov_alpha_beta %*% grad se_lhr = sqrt(var_lhr) se_lhr ## [,1] ## [1,] 0.5868807 And to get a 95% confidence interval for the hazard ratio…\nlog_hr = -exp(alpha) * beta log_hr_upr = log_hr + qnorm(0.975) * se_lhr log_hr_lwr = log_hr - qnorm(0.975) * se_lhr data.frame(HR = exp(log_hr), LB = exp(log_hr_lwr), UB = exp(log_hr_upr)) ## HR LB UB ## 1 0.5318051 0.1683444 1.679988  Splines The second thing I really like about flexsurv is the proportional hazards model with a spline for the baseline hazard. I’ll explore this in another post.\n ","date":1572220800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572220800,"objectID":"e943e878f08e699d8fc8f60da43195a1","permalink":"/post/flexsurv/","publishdate":"2019-10-28T00:00:00Z","relpermalink":"/post/flexsurv/","section":"post","summary":"I’m going to write about some of my favourite R packages. I’ll start with flexsurv (https://github.com/chjackson/flexsurv-dev) by Chris Jackson, which can be used to fit all kinds of parametric models to survival data. It can really do a lot, but I’ll pick out just 2 cool things I like about it:\nFit a standard survival model, but where it’s slightly easier to work out what the parameters mean. Fit a proportional hazards model, which is a lot like a Cox model, but where you also model the baseline hazard using a spline.","tags":["R Packages","Technical"],"title":"flexsurv","type":"post"},{"authors":null,"categories":[],"content":" In the last post on longitudinal hurdle models, I had just taken samples from the marginal mean\n\\[\\begin{align} g(\\theta; x) \u0026amp; = E(Y \\mid \\theta; x) \\\\ \u0026amp; = \\int E(Y \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}E(Y \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\\\ \u0026amp;= L^{-1}\\sum_{l = 1}^{L}\\left\\lbrace 1 - \\text{logit}^{-1} ( x^T \\gamma + u_{i\u0026#39;}^{(l)}) \\right\\rbrace \\exp(x^T\\beta + v_{i\u0026#39;}^{(l)} + \\frac{\\sigma^2}{2}).\\end{align}\\]\nOne of the issues with lognormal data is that it is highly skewed, so the mean can be very large. In a small sample, the sample mean can change a lot based on just 1 or 2 large observations. For this reason I would like to sample from other summary measures of \\(Y\\).\nSamples from \\(p(Y \\leq k \\mid \\theta; x)\\) This is quite similar to taking samples from the marginal mean.\n\\[\\begin{align} r(\\theta; x, k) \u0026amp; = p(Y \u0026lt; k \\mid \\theta; x) \\\\ \u0026amp; = \\int p(Y \u0026lt; k \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}p(Y \u0026lt; k \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\\\ \u0026amp;= L^{-1}\\sum_{l = 1}^{L}\\ \\left[ \\pi^{(l)}(x) + \\left\\lbrace 1 - \\pi^{(l)}(x)\\right\\rbrace \\Phi \\left\\lbrace \\frac{\\log(k) - x^T\\beta - v_{i\u0026#39;}^{(l)}}{\\sigma} \\right\\rbrace \\right].\\end{align}\\]\nwhere \\(\\pi^{(l)}(x):= \\text{logit}^{-1} ( x^T \\gamma + u_{i\u0026#39;}^{(l)})\\).\nAgain, I don’t know of any functions for doing this, so I built my own.\ndevtools::install_github(\u0026quot;dominicmagirr/hurlong\u0026quot;) library(brms) x \u0026lt;- data.frame(id = NA, time = \u0026quot;2\u0026quot;) hurlong::marg_pyk_q(k = 0.5, newdata = x, nsims = 1000, fit = fit_hurdle) ## id time 2.5% 50% 97.5% ## 1 NA 2 0.421836 0.5018633 0.5792536  Samples from \\(\\text{median}(Y \\mid \\theta; x)\\) Finally, I am interested in samples from the marginal median.\n\\[\\begin{align} s(\\theta; x) \u0026amp; = \\text{median}(Y \\mid \\theta; x) \\\\ \u0026amp; = \\int \\text{median}(Y \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}\\text{median}(Y\\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x) \\end{align}\\]\nTo evaluate \\(\\text{median}(Y\\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\) at each \\(l\\), I do a search for \\(m^{(l)}\\) such that\n\\[p(Y \u0026lt; m^{(l)} \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x) = 0.5.\\]\nAgain, I’ve made a function that can do this (for this specific longitudinal hurdle model).\nhurlong::marg_med(newdata = x, nsims = 1000, ks = exp(seq(log(0.01), log(100), length.out = 15)), # where to evaluate p(Y\u0026lt;k) fit = fit_hurdle) ## id time 2.5% 50% 97.5% ## 1 NA 2 0.2332964 0.4894882 0.8856152  Predictions To round off this series on longitudinal hurdle models, I want to show how to simulate draws (and find quantiles) from the posterior predictive distribution for a new observation (\\(\\tilde{Y}\\)). Firstly for a patient \\(i\\) already in the data set, where we draw from\n\\[ f(\\tilde{y} \\mid \\mathbf{y} ; x) = f(\\tilde{y} \\mid u_i, v_i, \\theta, \\mathbf{y} ; x)f(u_i, v_i, \\theta \\mid \\mathbf{y}) \\]\nlibrary(brms) predict(fit_hurdle, newdata = data.frame(id = 1, time = \u0026quot;2\u0026quot;), robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.2448615 0.3224153 0 2.145083 and, secondly, for a new patient \\(i\u0026#39;\\), where we draw from\n\\[ f(\\tilde{y} \\mid \\mathbf{y} ; x) = f(\\tilde{y} \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta, \\mathbf{y} ; x)f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) f(\\theta \\mid \\mathbf{y}) \\]\npredict(fit_hurdle, newdata = data.frame(id = NA, time = \u0026quot;2\u0026quot;), allow_new_levels = TRUE, sample_new_levels = \u0026quot;gaussian\u0026quot;, robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.4614108 0.6840876 0 52.57643  ","date":1571961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571961600,"objectID":"667624a716e015e1dfc0b086abb2e1e4","permalink":"/post/longitudinal-hurdle-models-3/","publishdate":"2019-10-25T00:00:00Z","relpermalink":"/post/longitudinal-hurdle-models-3/","section":"post","summary":"In the last post on longitudinal hurdle models, I had just taken samples from the marginal mean\n\\[\\begin{align} g(\\theta; x) \u0026amp; = E(Y \\mid \\theta; x) \\\\ \u0026amp; = \\int E(Y \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}E(Y \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\\\ \u0026amp;= L^{-1}\\sum_{l = 1}^{L}\\left\\lbrace 1 - \\text{logit}^{-1} ( x^T \\gamma + u_{i\u0026#39;}^{(l)}) \\right\\rbrace \\exp(x^T\\beta + v_{i\u0026#39;}^{(l)} + \\frac{\\sigma^2}{2}).","tags":["Technical"],"title":"Longitudinal hurdle models 3","type":"post"},{"authors":null,"categories":[],"content":" In a previous post I fit a longitudinal hurdle model using the brms package.\nlibrary(brms) summary(fit_hurdle) ## Family: hurdle_lognormal ## Links: mu = identity; sigma = identity; hu = logit ## Formula: y ~ time + (1 | q | id) ## hu ~ time + (1 | q | id) ## Data: dat (Number of observations: 800) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~id (Number of levels: 100) ## Estimate Est.Error l-95% CI u-95% CI Rhat ## sd(Intercept) 1.96 0.16 1.67 2.30 1.00 ## sd(hu_Intercept) 2.46 0.30 1.94 3.10 1.00 ## cor(Intercept,hu_Intercept) -0.90 0.04 -0.97 -0.80 1.00 ## Bulk_ESS Tail_ESS ## sd(Intercept) 1475 2467 ## sd(hu_Intercept) 3265 4837 ## cor(Intercept,hu_Intercept) 1949 3570 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.07 0.21 -0.34 0.48 1.01 770 1487 ## hu_Intercept -2.73 0.35 -3.44 -2.07 1.00 1484 3476 ## time2 -0.33 0.08 -0.48 -0.18 1.00 11586 5606 ## hu_time2 1.27 0.24 0.81 1.73 1.00 11668 5524 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.94 0.03 0.88 1.00 1.00 8823 5708 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). I’d now like to do some inference on the model, combining its zero and non-zero parts.\nThe model is: for observation \\(j\\) from patient \\(i\\),\n\\[Y_{i,j} = Z_{i,j}Y^*_{i,j},\\] \\[\\text{logit}\\left\\lbrace P(Z_{i,j} = 0) \\right\\rbrace = x_{i,j}^T\\gamma + u_i,\\]\n\\[\\log (Y^*_{i,j}) \\sim N(x_{i,j}^T\\beta + v_i, ~\\sigma^2), \\]\n\\[\\left( \\begin{array}{c} u_i \\\\ v_i \\\\ \\end{array} \\right) \\sim N\\left(\\left( \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right), \\left( \\begin{array}{c c} \\sigma_u^2 \u0026amp; \\rho \\sigma_u\\sigma_v \\\\ \\rho \\sigma_u \\sigma_v \u0026amp; \\sigma_v ^ 2\\end{array}\\right)\\right),\\] where \\(x_{i,j}^T = (1, t_{i,j})\\). Also let \\(\\theta = (\\gamma, \\beta, \\sigma, \\sigma_u, \\sigma_v, \\rho)\\).\nHow to use fitted.brmsfit Having obtained posterior samples from \\((\\theta, u_1,\\ldots,u_n,v_1,\\ldots,v_n)\\), we might want to look at samples from:\n\\[\\begin{align} h(u_i, v_i, \\theta; x) \u0026amp; = E(Y \\mid u_i, v_i, \\theta; x) \\\\ \u0026amp;= \\left\\lbrace 1 - \\text{logit}^{-1} ( x^T \\gamma + u_i) \\right\\rbrace \\exp(x^T\\beta + v_i + \\frac{\\sigma^2}{2}).\\end{align}\\]\nThis is some kind of patient-specific expectation of \\(Y\\), conditional on the random effects. If patient \\(i\\) is already in the model, then we can just take samples directly from the posterior. This can be achieved with the fitted method:\nfitted(fit_hurdle, newdata = data.frame(id = 1, time = \u0026quot;2\u0026quot;), robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.4034682 0.150417 0.1796645 0.8248473 will give the median and (2.5, 97.5) quantiles of \\(h(u_1, v_1, \\theta ; x)\\) at timepoint “2”.\nAlternatively, we might be more interested in \\(h(0, 0, \\theta ; x)\\), which in some sense is the patient-specific expectation of \\(Y\\) for an “average” patient with random effects fixed at zero. We can get this by setting re_formula = NA:\nfitted(fit_hurdle, newdata = data.frame(time = \u0026quot;2\u0026quot;), robust = TRUE, re_formula = NA) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.9615294 0.2401395 0.5689127 1.569304 Or, we might be more interested in \\(h(u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta ; x)\\) for a new patient \\(i\u0026#39;\\). Now we need to generate samples from the posterior distribution\n\\[f(u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta \\mid \\mathbf{y}) = f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) f(\\theta \\mid \\mathbf{y})\\]\nwe can do this by going through our posterior samples \\(\\theta^{(k)}\\) for \\(k = 1,\\ldots,K\\) and each time simulating\n\\[\\left( \\begin{array}{c} u_{i\u0026#39;}^{(k)} \\\\ v_{i\u0026#39;}^{(k)} \\\\ \\end{array} \\right) \\sim N\\left(\\left( \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right), \\left( \\begin{array}{c c} (\\sigma^{(k)}_u)^2 \u0026amp; \\rho \\sigma^{(k)}_u\\sigma^{(k)}_v \\\\ \\rho \\sigma^{(k)}_u \\sigma^{(k)}_v \u0026amp; (\\sigma^{(k)}_v)^2\\end{array}\\right)\\right).\\]\nThe way I expected this to be done is\nfitted(fit_hurdle, newdata = data.frame(id = NA, time = \u0026quot;2\u0026quot;), allow_new_levels = TRUE, sample_new_levels = \u0026quot;gaussian\u0026quot;, robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.9095811 1.315207 0.0009475099 58.14658 and this is indeed what’s happening, as can be seen by going through the steps manually\nps \u0026lt;- posterior_samples(fit_hurdle) sigma_error \u0026lt;- ps[,\u0026quot;sigma\u0026quot;] sigma_u \u0026lt;- ps[,\u0026quot;sd_id__hu_Intercept\u0026quot;] sigma_v \u0026lt;- ps[,\u0026quot;sd_id__Intercept\u0026quot;] rho \u0026lt;- ps[,\u0026quot;cor_id__Intercept__hu_Intercept\u0026quot;] n_mcmc \u0026lt;- length(rho) x \u0026lt;- data.frame(id = NA, time = \u0026quot;2\u0026quot;) ### simulate u_i\u0026#39; and v_i\u0026#39; u \u0026lt;- rnorm(n_mcmc, sd = sigma_u) ### include correlation v \u0026lt;- rnorm(n_mcmc, mean = u * sigma_v / sigma_u * rho, sd = sqrt((1 - rho^2) * sigma_v ^ 2)) ### extract draws from xi = x*gamma xi \u0026lt;- qlogis(fitted(fit_hurdle, newdata = x, re_formula = NA, dpar = \u0026quot;hu\u0026quot;, summary = FALSE)) ### extract draws from eta = x*beta eta \u0026lt;- fitted(fit_hurdle, newdata = x, re_formula = NA, dpar = \u0026quot;mu\u0026quot;, summary = FALSE) ey \u0026lt;- (1 - plogis(xi + u)) * exp(eta + v + sigma_error ^ 2 / 2) round(quantile(ey, probs = c(0.025, 0.5, 0.975)), 3) ## 2.5% 50% 97.5% ## 0.001 0.927 52.369  Unconditional expectation Instead of a patient-specific expectation, conditional on random effects, we might be more interested in targeting an overall expectation (for a new patient) where we integrate out the random effects. In other words, we want to take samples from\n\\[\\begin{align} g(\\theta; x) \u0026amp; = E(Y \\mid \\theta; x) \\\\ \u0026amp; = \\int E(Y \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}E(Y \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\\\ \u0026amp;= L^{-1}\\sum_{l = 1}^{L}\\left\\lbrace 1 - \\text{logit}^{-1} ( x^T \\gamma + u_{i\u0026#39;}^{(l)}) \\right\\rbrace \\exp(x^T\\beta + v_{i\u0026#39;}^{(l)} + \\frac{\\sigma^2}{2}).\\end{align}\\]\nThat is, I take \\(g(\\theta^{(k)}; x)\\) for \\(k = 1,\\ldots,K\\). At each \\(k\\), for \\(l= 1 \\ldots,L\\), I take indpendent draws\n\\[\\left( \\begin{array}{c} u_{i\u0026#39;}^{(l)} \\\\ v_{i\u0026#39;}^{(l)} \\\\ \\end{array} \\right) \\sim N\\left(\\left( \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right), \\left( \\begin{array}{c c} (\\sigma^{(k)}_u)^2 \u0026amp; \\rho \\sigma^{(k)}_u\\sigma^{(k)}_v \\\\ \\rho \\sigma^{(k)}_u \\sigma^{(k)}_v \u0026amp; (\\sigma^{(k)}_v)^2\\end{array}\\right)\\right).\\]\nto perform the inner Monte Carlo integration (probably not the best method for this 2-d example). I don’t think it’s possible to do this with brms so I’ve written my own code (which only works for this specific model).\ndevtools::install_github(\u0026quot;dominicmagirr/hurlong\u0026quot;) hurlong::marg_mean_q(newdata = x, nsims = 1000, fit = fit_hurdle) ## id time 2.5% 50% 97.5% ## 1 NA 2 3.845286 7.511835 19.58231  ","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"a90dd793d0d1ae36d7d656c84f4e2229","permalink":"/post/longitudinal-hurdle-models-2/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/post/longitudinal-hurdle-models-2/","section":"post","summary":"In a previous post I fit a longitudinal hurdle model using the brms package.\nlibrary(brms) summary(fit_hurdle) ## Family: hurdle_lognormal ## Links: mu = identity; sigma = identity; hu = logit ## Formula: y ~ time + (1 | q | id) ## hu ~ time + (1 | q | id) ## Data: dat (Number of observations: 800) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~id (Number of levels: 100) ## Estimate Est.","tags":["Technical"],"title":"Longitudinal hurdle models 2","type":"post"},{"authors":null,"categories":[],"content":" Data Recently, I have been modelling data that is longitudinal, contains excess zeros, and where the non-zero data is right-skewed and measured on a continuous scale, rather than being count data.\nI’ll simulate a semi-realistic example data set from a lognormal hurdle model. The “random effects” for the pr(zero) and non-zero parts of the model are negatively correlated.\nset.seed(180) ## 100 patients id \u0026lt;- 1:100 ## 2 timepoints time \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;) ## random effects u \u0026lt;- rnorm(100, sd = 2) v \u0026lt;- rnorm(100, mean = -0.95 * u, sd = sqrt((1 - 0.95^2) * 4)) # p(zero) is negatively correlated with Y* ## non-zero data (4 obs per id, at two timepoints) ystar1 \u0026lt;- exp(rnorm(400, mean = u, sd = 1)) ystar2 \u0026lt;- exp(rnorm(400, mean = -0.5 + u, sd = 1)) ## z = 1 if \u0026quot;cross hurdle\u0026quot;, i.e. if not zero z1 \u0026lt;- rbinom(400, size = 1, prob = 1 - plogis(-2 + v)) # p(cross hurdle) = 1 - p(zero) z2 \u0026lt;- rbinom(400, size = 1, prob = 1 - plogis(-1 + v)) dat \u0026lt;- data.frame(y = c(z1 * ystar1, z2 * ystar2), time = rep(time, each = 400), id = rep(id, 8)) In this data set there are 100 patients and two timepoints. For each patient, at each timepoint, I have simulated 4 independent observations (I’ve only done this to make model convergence a bit easier). The important point is that the data is correlated within patient, and also z (hurdle part) and ystar (non-zero part) are correlated, so that patients who start with a smaller (non-zero) y at the first timepoint are more likely to have y = 0 at the second timepoint. This can be clearly seen in the plot below.\nlibrary(tidyverse) ggplot(data = dat, mapping = aes(x = time, y = y, group = id)) + geom_point() + geom_line() + scale_y_log10()  Fit the model To fit the model I’m using the excellent brms package (https://github.com/paul-buerkner/brms)\nBürkner P. C. (2018). Advanced Bayesian Multilevel Modeling with the R Package brms. The R Journal. 10(1), 395-411. doi.org/10.32614/RJ-2018-017\nlibrary(brms) fit_hurdle \u0026lt;- brm(bf(y ~ time + (1 | q | id), hu ~ time + (1 | q | id)), data = dat, iter = 4000, family = hurdle_lognormal(), refresh = 0) summary(fit_hurdle) ## Family: hurdle_lognormal ## Links: mu = identity; sigma = identity; hu = logit ## Formula: y ~ time + (1 | q | id) ## hu ~ time + (1 | q | id) ## Data: dat (Number of observations: 800) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~id (Number of levels: 100) ## Estimate Est.Error l-95% CI u-95% CI Rhat ## sd(Intercept) 1.96 0.16 1.69 2.29 1.00 ## sd(hu_Intercept) 2.47 0.30 1.93 3.11 1.00 ## cor(Intercept,hu_Intercept) -0.90 0.04 -0.97 -0.81 1.00 ## Bulk_ESS Tail_ESS ## sd(Intercept) 1452 2688 ## sd(hu_Intercept) 2979 5484 ## cor(Intercept,hu_Intercept) 2539 4394 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.07 0.20 -0.34 0.45 1.00 885 1970 ## hu_Intercept -2.72 0.34 -3.43 -2.07 1.00 1662 3089 ## time2 -0.33 0.08 -0.49 -0.18 1.00 14798 5985 ## hu_time2 1.26 0.24 0.80 1.73 1.00 12452 5627 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.94 0.03 0.88 1.00 1.00 11293 6175 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1).  Inference From the output I can see that I have more-or-less recovered the parameters from my model. In practice, I could use this to make inference on the two parts of the model separately. In future posts I’ll discuss how to make inference/predictions when combining the two parts of the model.\n ","date":1571184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571184000,"objectID":"d054a2492f9c45c5d7b2d2c1bf228558","permalink":"/post/longitudinal-hurdle-models/","publishdate":"2019-10-16T00:00:00Z","relpermalink":"/post/longitudinal-hurdle-models/","section":"post","summary":"Data Recently, I have been modelling data that is longitudinal, contains excess zeros, and where the non-zero data is right-skewed and measured on a continuous scale, rather than being count data.\nI’ll simulate a semi-realistic example data set from a lognormal hurdle model. The “random effects” for the pr(zero) and non-zero parts of the model are negatively correlated.\nset.seed(180) ## 100 patients id \u0026lt;- 1:100 ## 2 timepoints time \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;) ## random effects u \u0026lt;- rnorm(100, sd = 2) v \u0026lt;- rnorm(100, mean = -0.","tags":["Technical"],"title":"Longitudinal hurdle models","type":"post"},{"authors":null,"categories":null,"content":"","date":1559559600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559559600,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2019-06-03T11:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"A review of master protocol trials in early phase oncology","tags":null,"title":"Multi-arm multi-stage designs for early phase oncology trials","type":"talk"},{"authors":null,"categories":[],"content":" Suppose we have the following data.\ndf \u0026lt;- dplyr::tibble(patient_id = as.character(1:12), treatment = rep(c(\u0026quot;C\u0026quot;, \u0026quot;E\u0026quot;), each = 6), survival_time = survival::Surv(time = c(2,6,8,11,17,24,7,9,13,22,23,25), event = c(1,1,1,1,1,0,1,1,1,0,0,0))) knitr::kable(df)   patient_id treatment survival_time    1 C 2  2 C 6  3 C 8  4 C 11  5 C 17  6 C 24+  7 E 7  8 E 9  9 E 13  10 E 22+  11 E 23+  12 E 25+    Let’s arrange the data in increasing order of survival time.\ndf_ordered \u0026lt;- dplyr::arrange(df, survival_time[,\u0026quot;time\u0026quot;]) knitr::kable(df_ordered)   patient_id treatment survival_time    1 C 2  2 C 6  7 E 7  3 C 8  8 E 9  4 C 11  9 E 13  5 C 17  10 E 22+  11 E 23+  6 C 24+  12 E 25+    Method 1: scores The first step is to estimate the survival probability from the pooled data using the Nelson-Aalen estimator. Then, to get the log-rank scores, we add 1 to the logarithm of the pooled survival estimate for all observed events. For censored events we do not add 1.\npooled_fit \u0026lt;- survival::survfit(survival_time ~ 1, data = df_ordered) df_logrank \u0026lt;- dplyr::mutate(df_ordered, pooled_s = exp(-cumsum(pooled_fit$n.event / pooled_fit$n.risk)), logrank_score = log(pooled_s) + survival_time[,\u0026quot;status\u0026quot;]) knitr::kable(df_logrank, digits = 3)   patient_id treatment survival_time pooled_s logrank_score    1 C 2 0.920 0.917  2 C 6 0.840 0.826  7 E 7 0.760 0.726  3 C 8 0.680 0.615  8 E 9 0.600 0.490  4 C 11 0.520 0.347  9 E 13 0.440 0.180  5 C 17 0.361 -0.020  10 E 22+ 0.361 -1.020  11 E 23+ 0.361 -1.020  6 C 24+ 0.361 -1.020  12 E 25+ 0.361 -1.020    To calculate the logrank score statistic we add up all the scores on the control arm.\nlogrank_score \u0026lt;- dplyr::summarise(dplyr::group_by(df_logrank, treatment), sum(logrank_score)) logrank_score ## # A tibble: 2 x 2 ## treatment `sum(logrank_score)` ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 C 1.66 ## 2 E -1.66  Method 2: observed - expected The score statistic is equivalent to the sum of “observed” - “expected” events at each event time. This is implemented in the survival package.\nlogrank_fit \u0026lt;- survival::survdiff(survival_time ~ treatment, data = df) rbind(logrank_fit$n, logrank_fit$obs - logrank_fit$exp)[2,] ## treatment=C treatment=E ## 1.664105 -1.664105  ","date":1559520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559520000,"objectID":"ccfb07f82da43dae6cf664dc84f3d19e","permalink":"/post/log-rank-test/","publishdate":"2019-06-03T00:00:00Z","relpermalink":"/post/log-rank-test/","section":"post","summary":"Suppose we have the following data.\ndf \u0026lt;- dplyr::tibble(patient_id = as.character(1:12), treatment = rep(c(\u0026quot;C\u0026quot;, \u0026quot;E\u0026quot;), each = 6), survival_time = survival::Surv(time = c(2,6,8,11,17,24,7,9,13,22,23,25), event = c(1,1,1,1,1,0,1,1,1,0,0,0))) knitr::kable(df)   patient_id treatment survival_time    1 C 2  2 C 6  3 C 8  4 C 11  5 C 17  6 C 24+  7 E 7  8 E 9  9 E 13  10 E 22+  11 E 23+  12 E 25+    Let’s arrange the data in increasing order of survival time.","tags":[],"title":"How to calculate the log-rank statistic","type":"post"},{"authors":[],"categories":[],"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three  A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1537963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537963200,"objectID":"1038d14884db9cb42dadbba4efd1b86b","permalink":"/talk/group-sequential/","publishdate":"2018-09-26T12:00:00Z","relpermalink":"/talk/group-sequential/","section":"talk","summary":"In this talk I delved into some of the history of John Whitehead's Triangular Test. The context was a mulit-arm trial of several treatments for visceral leishmaniasis, designed by Neal Alexander and colleagues from LSHTM. Together with Annabel Allison (as part of her MSc dissertation), we tried to make several improvements to the design.","tags":null,"title":"Generalizing boundaries for triangular designs, and efficacy estimation at extended follow-ups","type":"talk"},{"authors":null,"categories":null,"content":"","date":1528120800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528120800,"objectID":"596b358e2eb987ca44822814107af0e9","permalink":"/talk/non-proportional-hazards/","publishdate":"2018-06-04T14:00:00Z","relpermalink":"/talk/non-proportional-hazards/","section":"talk","summary":"Here I talk about designing a clinical trial for an immuno-oncology endpoint where we expect a delay in the seperation of the survival curves. Starting with the properties of a standard design under non-proportional hazards, I believed I had made a great improvement by adapting the sample size based on a \"ratio of hazard ratios\". However, a well-chosen group-sequential design made an equal (slightly better) improvement to the operating characteristics, and is much simpler.","tags":null,"title":"Group-sequential and adaptive designs in immuno-oncology","type":"talk"},{"authors":["Dominic Magirr","Carl-Fredrik Burman"],"categories":null,"content":"","date":1527292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527292800,"objectID":"ba911dff95322a3651a294b638ea7c1a","permalink":"/publication/wlrt/","publishdate":"2018-05-26T00:00:00Z","relpermalink":"/publication/wlrt/","section":"publication","summary":"We propose a new class of weighted logrank tests (WLRTs) that control the risk of concluding that a new drug is more efficacious than standard of care, when, in fact, it is uniformly inferior. Perhaps surprisingly, this risk is not controlled for WLRT in general. Tests from this new class can be constructed to have high power under a delayed‐onset treatment effect scenario, as well as being almost as efficient as the standard logrank test under proportional hazards.","tags":["Source Themes"],"title":"Modestly weighted logrank tests","type":"publication"},{"authors":null,"categories":null,"content":"","date":1504085400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504085400,"objectID":"d43c5d4bc26318eecdfaa14e216c503e","permalink":"/talk/vienna-cen-isbs/","publishdate":"2017-08-30T09:30:00Z","relpermalink":"/talk/vienna-cen-isbs/","section":"talk","summary":"Quite often, single-arm studies in early-phase oncology are criticized because they lead to false positives. Here, I wanted to point out that they also have a high risk of false negatives.","tags":null,"title":"Small trials, historical data. Methods and software for decision making.","type":"talk"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Dominic Magirr","Thomas Jaki","Franz Koenig","Martin Posch"],"categories":null,"content":"","date":1454284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454284800,"objectID":"bae8a4c2aa41c927886a6bb63546cc9d","permalink":"/publication/adaptive_survival_trials/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/adaptive_survival_trials/","section":"publication","summary":"Mid-study design modifications are becoming increasingly accepted in confirmatory clinical trials, so long as appropriate methods are applied such that error rates are controlled. It is therefore unfortunate that the important case of time-to-event endpoints is not easily handled by the standard theory. We analyze current methods that allow design modifications to be based on the full interim data, i.e., not only the observed event times but also secondary endpoint and safety data from patients who are yet to have an event. We show that the final test statistic may ignore a substantial subset of the observed event times. An alternative test incorporating all event times is found, where a conservative assumption must be made in order to guarantee type I error control. We examine the power of this approach using the example of a clinical trial comparing two cancer therapies.","tags":["Source Themes"],"title":"Sample Size Reassessment and Hypothesis Testing in Adaptive Survival Trials","type":"publication"},{"authors":["Dominic Magirr","Thomas Jaki","John Whitehead"],"categories":null,"content":"","date":1338508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338508800,"objectID":"1c1778141ad98763fffede1b8fe099d9","permalink":"/publication/generalized_dunnett/","publishdate":"2012-06-01T00:00:00Z","relpermalink":"/publication/generalized_dunnett/","section":"publication","summary":"We generalize the Dunnett test to derive efficacy and futility boundaries for a flexible multi-arm multi-stage clinical trial for a normally distributed endpoint with known variance. We show that the boundaries control the familywise error rate in the strong sense. The method is applicable for any number of treatment arms, number of stages and number of patients per treatment per stage. It can be used for a wide variety of boundary types or rules derived from alpha-spending functions. Additionally, we show how sample size can be computed under a least favourable configuration power requirement and derive formulae for expected sample sizes.","tags":["Source Themes"],"title":"A generalized Dunnett test for multi-arm multi-stage clinical studies with treatment selection","type":"publication"}]