[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a medical statistician interested in the design, analysis and interpretation of clinical trials.\nSpecific research interests include:\n group sequential trials multiple hypothesis testing adaptive designs survival analysis  Since February 2020 I have been working in the Statistical Methodology Group at Novartis in Basel.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a medical statistician interested in the design, analysis and interpretation of clinical trials.\nSpecific research interests include:\n group sequential trials multiple hypothesis testing adaptive designs survival analysis  Since February 2020 I have been working in the Statistical Methodology Group at Novartis in Basel.","tags":null,"title":"Dominic Magirr","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":[],"content":" I’m delighted that our new R package {nphRCT} is available on CRAN. Special thanks to Isobel Barrot. Her R skills have made this possible. Thanks also to Carl-Fredrik Burman and José Jiménez, my collaborators on this topic over the past few years.\nlibrary(dplyr) library(ggplot2) library(survival) #devtools::install_github(\u0026quot;https://github.com/cran/nphRCT.git\u0026quot;) library(nphRCT) The idea for the package was to make it easy to perform a weighted log-rank test, and in particular the modestly-weighted logrank test, in the context of an Randomized Controlled Trial (RCT) where one expects a delayed separation of survival curves.\nExample data set A dataset I often use as an example in this context is the POPLAR trial data.\nLet’s find the data from the web, and plot the Kaplan-Meier curves…\ntemplate \u0026lt;- tempfile(fileext = \u0026quot;.xlsx\u0026quot;) httr::GET(url = \u0026quot;https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0134-3/MediaObjects/41591_2018_134_MOESM3_ESM.xlsx\u0026quot;, httr::write_disk(template)) dat \u0026lt;- readxl::read_excel(template,sheet = 2) %\u0026gt;% select(PtID, ECOGGR, OS, OS.CNSR, TRT01P) %\u0026gt;% mutate(event = -1 * (OS.CNSR - 1), time = OS, arm = ifelse(TRT01P == \u0026quot;Docetaxel\u0026quot;, \u0026quot;control\u0026quot;, \u0026quot;experimental\u0026quot;)) %\u0026gt;% select(time, event, arm) %\u0026gt;% as.data.frame() km \u0026lt;- survfit(Surv(time, event) ~ arm, data = dat) survminer::ggsurvplot(km, data = dat, risk.table = TRUE, break.x.by = 6, legend.title = \u0026quot;\u0026quot;, xlab = \u0026quot;Time (months)\u0026quot;, ylab = \u0026quot;Overall survival\u0026quot;, risk.table.fontsize = 4, legend = c(0.8,0.8))  At-risk table The at-risk table (per arm) may be a good place to start. If we want, we can look at this via find_at_risk. The syntax should be familiar from the {survival} package.\nat_risk \u0026lt;- nphRCT::find_at_risk(Surv(time, event) ~ arm, data = dat, include_cens = FALSE) head(at_risk) ## t_j n_event_control n_event_experimental n_event n_risk_control ## 1 0.1971253 1 0 1 138 ## 2 0.2299795 0 1 1 137 ## 3 0.2956879 1 0 1 137 ## 4 0.3613963 1 1 2 136 ## 5 0.4599589 1 2 3 135 ## 6 0.5585216 0 1 1 134 ## n_risk_experimental n_risk ## 1 144 282 ## 2 144 281 ## 3 143 280 ## 4 143 279 ## 5 142 277 ## 6 140 274 The weighted log-rank test statistic is\n\\[ U^W = \\sum_{j=1}^k w_j \\left\\lbrace \\text{n_event_control}(t_{j}) - \\text{n_event}(t_{j}) \\times \\frac{\\text{n_risk_control}(t_{j})}{\\text{n_risk}(t_{j})}\\right\\rbrace \\]\n Weights Modestly-weighted The modestly weighted log-rank test uses weights \\(w_j = 1 / \\max\\{\\hat{S}(t_j-), s^*\\}\\), where \\(\\hat{S}(t_j-)\\) is the Kaplan-Meier estimate at \\(t_j-\\) (just prior to event time \\(t_j\\)) based on the pooled (both arms) data, and \\(s^* \\in (0,1]\\) is pre-specified. For example, taking \\(s^*=0.5\\)…\nweights_mw_s \u0026lt;- nphRCT::find_weights(Surv(time, event) ~ arm, data = dat, method = \u0026quot;mw\u0026quot;, s_star = 0.5) plot(at_risk$t_j, weights_mw_s, ylim = c(0,2), xlab = \u0026quot;Event time\u0026quot;, ylab = \u0026quot;Weight\u0026quot;, main = \u0026quot;Modestly weighted, s*=0.5\u0026quot;) In this case, the weight for the first event time is 1, and weights are increasing but capped at 2 at the time the KM curve drops to 0.5 (at around 12 months).\nAlternatively, one could choose \\(w_j = 1 / \\max\\{\\hat{S}(t_j-), \\hat{S}(t^*)\\}\\), where \\(t^*\\) is a pre-specified timepoint beyond which the weights no longer increase. The weights are then capped at \\(1 / \\hat{S}(t^*)\\). For example, taking \\(t^* = 6\\),\nweights_mw_t \u0026lt;- nphRCT::find_weights(Surv(time, event) ~ arm, data = dat, method = \u0026quot;mw\u0026quot;, t_star = 6) plot(at_risk$t_j, weights_mw_t, ylim = c(0,2), xlab = \u0026quot;Event time\u0026quot;, ylab = \u0026quot;Weight\u0026quot;, main = \u0026quot;Modestly weighted, t*=6\u0026quot;) Special case: \\(s^*=1\\) (or \\(t^*=0\\)) The special case \\(s^*=1\\) (or \\(t^*=0\\)) is the same as the standard log-rank test.\n  Fleming-Harrington \\((\\rho, \\gamma)\\) test The Fleming-Harrington \\((\\rho, \\gamma)\\) weights are \\(\\hat{S}(t_j-)^{\\rho}\\{1 - \\hat{S}(t_j-) \\}^{\\gamma}\\). Often, in delayed effect scenarios, \\(\\rho=0\\) and \\(\\gamma=1\\) is chosen.\nweights_fh_0_1 \u0026lt;- nphRCT::find_weights(Surv(time, event) ~ arm, data = dat, method = \u0026quot;fh\u0026quot;, rho = 0, gamma = 1) plot(at_risk$t_j, weights_fh_0_1, ylim = c(0,1), xlab = \u0026quot;Event time\u0026quot;, ylab = \u0026quot;Weight\u0026quot;, main = \u0026quot;Fleming-Harrington (0,1)\u0026quot;) The main difference between F-H(0,1) weights and the modest weights is that the F-H(0,1) weights start at zero. The early events are therefore much more heavily down-weighted compared to the modest test.\nSpecial case: \\(\\rho = \\gamma = 0\\) The special case \\(\\rho = \\gamma = 0\\) is the same as the standard log-rank test.\n   Performing the weighted log-rank test To perform the weighed log-rank test use the wlrt function.\n# Modestly-weighted (t*=6) nphRCT::wlrt(Surv(time, event) ~ arm, data = dat, method = \u0026quot;mw\u0026quot;, t_star = 6) ## u v_u z trt_group ## 1 -26.11708 83.62023 -2.856071 experimental # Standard log-rank test (t*=0) nphRCT::wlrt(Surv(time, event) ~ arm, data = dat, method = \u0026quot;mw\u0026quot;, t_star = 0) ## u v_u z trt_group ## 1 -19.42566 49.15217 -2.770796 experimental # Fleming-Harrington (0,1) nphRCT::wlrt(Surv(time, event) ~ arm, data = dat, method = \u0026quot;fh\u0026quot;, rho = 0, gamma = 1) ## u v_u z trt_group ## 1 -9.942946 8.522577 -3.405882 experimental  Visualization of test statistic: difference in mean score It’s possible to visualize a weighted log-rank test as a difference in mean “score” on the two treatment arms, where the scores are a transformation of the 2-dimensional outcomes (time, event status) to the real line. This is explained in more detail here.\nWe’ll try it now for our examples. In these plots, each dot corresponds to a patient in the trial. On the x-axis is their time-to-event/censoring, and on the y-axis is their score. For the log-rank test, we see two almost parallel lines, with the top line corresponding to the observed events, and the bottom line corresponding to censored observations. So someone who dies at month 1 gets a score close to 1 (the worst score), and someone who is censored at month 24 gets a score close to -1 (the best score). Note that the scaling is arbitrary. The dashed horizontal lines represent the mean score on the two treatment arms. The difference between these to lines is the log-rank test statistic (a re-scaled version of it).\n# Standard log-rank test (t*=0) nphRCT::find_scores(Surv(time, event) ~ arm, data = dat, method = \u0026quot;mw\u0026quot;, t_star = 0) %\u0026gt;% plot() For the modestly-weighted test, the scores (for observed events) are essentially fixed at 1 until time \\(t^*\\). This is by design.\n# Modestly-weighted (t*=6) nphRCT::find_scores(Surv(time, event) ~ arm, data = dat, method = \u0026quot;mw\u0026quot;, t_star = 6) %\u0026gt;% plot() In contrast, for the Fleming-Harrington-(0,1) test, the scores (for observed events) are increasing with time for early follow-up times. So someone who dies at month 1 gets a lower (better) score than someone who dies at month 12. Basically, one has downweighted the early events to such an extent that the test actually “rewards” the experimental treatment for causing early harm.\n# Fleming-Harrington (0,1) nphRCT::find_scores(Surv(time, event) ~ arm, data = dat, method = \u0026quot;fh\u0026quot;, rho = 0, gamma = 1) %\u0026gt;% plot() Other versions of Fleming-Harrington test e.g., (1,1), (0,0.5), (0.5,0.5), etc. suffer from the same problem.\n# Fleming-Harrington (1,1) nphRCT::find_scores(Surv(time, event) ~ arm, data = dat, method = \u0026quot;fh\u0026quot;, rho = 1, gamma = 1) %\u0026gt;% plot() # Fleming-Harrington (0,0.5) nphRCT::find_scores(Surv(time, event) ~ arm, data = dat, method = \u0026quot;fh\u0026quot;, rho = 0, gamma = 0.5) %\u0026gt;% plot() # Fleming-Harrington (0.5,0.5) nphRCT::find_scores(Surv(time, event) ~ arm, data = dat, method = \u0026quot;fh\u0026quot;, rho = 0.5, gamma = 0.5) %\u0026gt;% plot()  ","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662040801,"objectID":"9e877cfb250d57f40086d8588e472e3a","permalink":"/post/2022-09-01-nphrct/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/post/2022-09-01-nphrct/","section":"post","summary":"I’m delighted that our new R package {nphRCT} is available on CRAN. Special thanks to Isobel Barrot. Her R skills have made this possible. Thanks also to Carl-Fredrik Burman and José Jiménez, my collaborators on this topic over the past few years.\nlibrary(dplyr) library(ggplot2) library(survival) #devtools::install_github(\u0026quot;https://github.com/cran/nphRCT.git\u0026quot;) library(nphRCT) The idea for the package was to make it easy to perform a weighted log-rank test, and in particular the modestly-weighted logrank test, in the context of an Randomized Controlled Trial (RCT) where one expects a delayed separation of survival curves.","tags":[],"title":"{nphRCT} on CRAN","type":"post"},{"authors":[],"categories":[],"content":"  This is a quick follow up to my previous post on pseudo-observations where I reproduced Table 1 from “Causal inference in survival analysis using pseudo-observations” by Per K. Andersen, Elisavet Syriopouloua, and Erik T. Parner. The simulation scenario for Table 1 was a correctly specified Cox model that didn’t show off the robustness property of the pseudo-observations method. In this post I’ll aim to reproduce Table 2, which does demonstrate this point.\n Table 1: Table 2 (last row) from Andersen et al. (2017)  HR_1 ACE Cens_Prob Cox_Bias Psuedo_Bias    5 0.384 0.684 -0.107 0.001    The simulation scenario in Table 2 is described in the paper as follows\nThe baseline hazard was constant =0.511 for the unexposed (A=0) and the hazard ratio for the exposed (A=1) increased linearly over time from HR(0)=1 at time 0 to a hazard ratio of HR(1)=1,1.5,2,5 at time 1. The exposure distribution was binomial with probability 0.50.\nOk, I’m already struggling a bit. How do I simulate from a model with a linearly increasing hazard ratio? That’s not something I do every day.\nI’ll focus on the situation when the hazard ratio is 5 at time 1. I can write the survival distribution as \\[S(t) = \\exp(-\\int_{0}^t h(s)ds)\\] where in this case \\[h(s)=0.511\\times(1 + 4s),~s \\in (0,1)\\] so that \\[S(t) = \\exp(-0.511(t + 2t^2))\\] This means that I can simulate the time-to-event \\(T_i^*\\) by first simulating \\(U_i \\sim U(0,1)\\) and then solving \\[U_i = \\exp(-0.511(T_i^* + 2(T_i^*)^2))\\] for \\(T_i^*\\). I can just about remember that \\[T_i^* = \\frac{-0.5 \\pm \\sqrt{0.25-2\\log(U_i)/0.511}}{2}\\] (and I know that \\(T_i^*\\) is positive). Let’s check I have the correct scenario according to Table 2, remembering that we’re interested in the difference in survival probabilities at \\(t=1\\).\nn \u0026lt;- 5000000 u \u0026lt;- runif(n) t_star_1 \u0026lt;- (-0.5 + sqrt(0.25 - 2 * log(u) / 0.511)) / 2 t_star_0 \u0026lt;- rexp(n, 0.511) a \u0026lt;- rbinom(n, 1, 0.5) t_star \u0026lt;- a * t_star_1 + (1 - a) * t_star_0 true_ace \u0026lt;- mean(t_star[a == 1] \u0026lt; 1) - mean(t_star[a == 0] \u0026lt; 1) true_ace ## [1] 0.3840153 Looks good so far!\nThe censoring distribution was described as follows\nThe censoring had a constant hazard of 0, 0.1, 0.2, 0.5, 1.0, and 2.0.\nAnd I’m focusing on the last row of Table 2 where the censoring hazard was equal to 2.0\ncens \u0026lt;- rexp(n, 2) mean(cens \u0026lt; pmin(1, t_star)) ## [1] 0.6841254 This also matches Table 2.\nNext I’ll look at the bias. I’ll first need a function that can simulate one data set under this scenario\n####################################### ## function to simulate one data set sim_dat \u0026lt;- function(n){ u \u0026lt;- runif(n) t_star_1 \u0026lt;- (-0.5 + sqrt(0.25 - 2 * log(u) / 0.511)) / 2 t_star_0 \u0026lt;- rexp(n, 0.511) a \u0026lt;- rbinom(n, 1, 0.5) t_star \u0026lt;- a * t_star_1 + (1 - a) * t_star_0 cens \u0026lt;- rexp(n, 2) times \u0026lt;- pmin(1, pmin(t_star, cens)) event \u0026lt;- as.numeric(t_star \u0026lt; pmin(cens, 1)) df \u0026lt;- data.frame(times = times, event = event, a = a) df } Bias (pseudo-observations) To find the bias of the pseudo-observation approach I can copy and paste my code from my previous post (more or less – I remove baseline covariates \\(L\\) since they are not included in this scenario)…\nlibrary(prodlim) ######################### ## function to find pseudo values. ## df must have following columns ## \u0026quot;times\u0026quot; (numeric) ## \u0026quot;event\u0026quot; (numeric 0/1) ## \u0026quot;a\u0026quot; (numeric 0/1) pseudo_surv \u0026lt;- function(df, tau = 1){ f = prodlim(Hist(times, event) ~ 1, data = df) df$pseudo_obs \u0026lt;- unname(jackknife(f, times = tau)) df } ############################### ## what do we do with the pseudovalues? ## fit a linear model and use g-formula to find ## average causal effect (ACE) ## df must have following columns ## \u0026quot;times\u0026quot; (numeric) ## \u0026quot;event\u0026quot; (numeric 0/1) ## \u0026quot;a\u0026quot; (numeric 0/1) ## \u0026quot;pseudo_obs\u0026quot; (numeric) find_ace \u0026lt;- function(df){ ## how many patients in data set n \u0026lt;- dim(df)[1] ## fit linear model to pseudovalues fit \u0026lt;- lm(pseudo_obs ~ a, data = df) ## predict pseudovalue for each patient under A = 0 predict_0 = predict(fit, newdata = data.frame(a = rep(0, n))) ## predict pseudovalue for each patient under A = 1 predict_1 = predict(fit, newdata = data.frame(a = rep(1, n))) ## average causal effect is p(T\u0026gt;1 | A=0) - p(T \u0026gt; 1 | A=1) ace \u0026lt;- mean(predict_0) - mean(predict_1) ace } ################################### ## bring 3 functions together sim_ace \u0026lt;- function(n){find_ace(pseudo_surv(sim_dat(n)))} In the paper, a sample size of 1,000,000 is used with 10,000 replications. As I don’t want to wait forever, I’ll reduce this to n=200 and 1,000 replications.\n## n = 200 res_pseudo \u0026lt;- purrr::map_dbl(rep(200, 1000), sim_ace) mean(res_pseudo) - true_ace #bias ## [1] 0.001565089 This looks to be essentially unbiased, in agreement with Table 2.\n Bias (Cox model) I didn’t do this last time, so need to write a couple more functions…\nlibrary(survival) find_ace_cox \u0026lt;- function(df){ ## how many patients in data set n \u0026lt;- dim(df)[1] ## fit cox model fit_cox \u0026lt;- coxph(Surv(times, event) ~ a, data = df) ## predict S(1) for each patient under A = 0 predict_0 = predict(fit_cox, newdata = data.frame(times = rep(1, n), event = rep(1, n), a = rep(0, n)), type = \u0026quot;survival\u0026quot;) ## predict S(1) for each patient under A = 1 predict_1 = predict(fit_cox, newdata = data.frame(times = rep(1, n), event = rep(1, n), a = rep(1, n)), type = \u0026quot;survival\u0026quot;) ## average causal effect is p(T\u0026gt;1 | A=0) - p(T \u0026gt; 1 | A=1) ace \u0026lt;- mean(predict_0) - mean(predict_1) ace } ################################### ## bring2 functions together sim_ace_cox \u0026lt;- function(n){find_ace_cox(sim_dat(n))} and check the bias\n## n = 200 res_cox \u0026lt;- purrr::map_dbl(rep(200, 1000), sim_ace_cox) mean(res_cox) - true_ace #bias ## [1] -0.1173701 which looks close enough to Table 2 to me, considering I’ve reduced the sample size / number of replications.\n Comments This example nicely demonstrates the bias in estimating the ACE if using a Cox model when the true data generating model has non-proportional hazards. Using a pseudo-observations approach in this case avoided this bias.\n ","date":1656892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656924823,"objectID":"06a3d4486de88c89861661964080e939","permalink":"/post/2022-07-04-psuedo-observations-2/","publishdate":"2022-07-04T00:00:00Z","relpermalink":"/post/2022-07-04-psuedo-observations-2/","section":"post","summary":"This is a quick follow up to my previous post on pseudo-observations where I reproduced Table 1 from “Causal inference in survival analysis using pseudo-observations” by Per K. Andersen, Elisavet Syriopouloua, and Erik T. Parner. The simulation scenario for Table 1 was a correctly specified Cox model that didn’t show off the robustness property of the pseudo-observations method. In this post I’ll aim to reproduce Table 2, which does demonstrate this point.","tags":[],"title":"Pseudo-observations 2","type":"post"},{"authors":[],"categories":[],"content":"  Idea / motivation I’ve set myself the challenge of reproducing the results in Table I of “Causal inference in survival analysis using pseudo-observations” by Per K. Andersen, Elisavet Syriopouloua, and Erik T. Parner.\nI’m doing this because I’ve seen a lot about pseudo-observations recently, and while I kind of get the gist of it, I would like to deepen my understanding by getting hands on with some examples.\nFor more general motivation, i.e., what’s the big deal about pseudo-observations?, I think this sentence from the introduction sums it up nicely\nThis approach has the advantage that when focus is on a single time point t0, an assumption like the proportional hazards assumption for all values t of time that is imposed when using a Cox model can be avoided.\nThe idea is that we would like to estimate the average causal effect (ACE) defined as\n\\[E(\\mathbb{1}\\{ Y^1 \u0026lt; t_0\\}) - E(\\mathbb{1}\\{ Y^0 \u0026lt; t_0\\})\\] where \\(Y^1\\) and \\(Y^0\\) denote the potential outcomes when exposed to two possible treatments \\(A=0\\) or \\(A=1\\). We also have available some prognostic information in the form of baseline covariates \\(L\\) that we would like to make use of to improve the efficiency of our inference.\n G-formula applied to the Cox model One approach to estimating the ACE is to first fit a Cox model,\n\\[h(t)=h_0(t)\\exp(\\beta^T L + \\gamma A),\\] then, for each subject \\(i=1,\\ldots,n\\), use the model to predict survival to \\(t_0\\) if exposed to \\(A = 1\\),\n\\[\\hat{S}(t_0 \\mid L_i, A_i = 1) = \\exp(-\\hat{\\Lambda}_0\\exp(\\hat{\\beta}^T L_i + \\hat{\\gamma})),\\] and if exposed to \\(A = 0\\),\n\\[\\hat{S}(t_0 \\mid L_i, A_i = 0) = \\exp(-\\hat{\\Lambda}_0\\exp(\\hat{\\beta}^T L_i)),\\] and, finally, (point) estimate the ACE as\n\\[\\frac{1}{n}\\sum_{i=1}^n \\hat{S}(t_0 \\mid L_i, A_i = 0) - \\frac{1}{n}\\sum_{i=1}^n \\hat{S}(t_0 \\mid L_i, A_i = 1).\\]\nThere is probably some nice theory for constructing a confidence interval for the ACE, but the easiest thing to say is that we could use non-parametric bootstrap.\nOverall, this is a nice procedure if the Cox model is correctly specified. If the model is misspecified then we may have a biased point estimate and confidence intervals with incorrect coverage. Depending on one’s perspective, this might be a really bad thing.\n G-formula applied to pseudo-observations A potentially more robust (less bias under model misspecification) way to estimate ACE is to use pseudo-observations.\nWhat’s a pseudo-observation? Suppose we are interested in the parameter \\(\\theta = E(\\mathbb{1}\\{ Y \u0026gt; t_0\\})\\) and a consistent estimator, \\(\\hat{\\theta}\\), exists, based on the observations from the full sample \\(i=1,\\ldots,n\\), e.g., based on the Kaplan-Meier estimator (with suitable assumptions about censoring). A pseudo-observation (or pseudo-value) for subject \\(i\\) is defined as\n\\[\\theta_i = n \\hat{\\theta} - (n-1)\\hat{\\theta}^{-i},\\] where \\(\\hat{\\theta}^{-i}\\) is the estimator applied to the sample of size n−1 obtained by eliminating subject i from the whole sample.\nSomewhat miraculously (to me at least…the maths is way above my level – see, e.g., this paper), these pseudo-observations have (approximately) the same conditional expectation as the thing we were originally interested in\n\\[E(\\theta_i \\mid L_i, A_i) \\approx E(\\mathbb{1}\\{ Y_i \u0026gt; t_0\\} \\mid L_i, A_i).\\]\nThis means that we can express the average causal effect as\n\\[ \\begin{align} \\text{ACE} \u0026amp; = E(\\mathbb{1}\\{ Y^0 \u0026gt; t_0\\}) - E(\\mathbb{1}\\{ Y^1 \u0026gt; t_0\\})\\\\ \u0026amp; = E_L(E(\\mathbb{1}\\{ Y_i \u0026gt; t_0\\} \\mid L_i, A_i = 0)) - E_L(E(\\mathbb{1}\\{ Y_i \u0026gt; t_0\\} \\mid L_i, A_i = 1)) \\\\ \u0026amp; \\approx E_L(E(\\theta_i \\mid L_i, A_i = 0)) - E_L(E(\\theta_i \\mid L_i, A_i = 1)) \\end{align} \\] and (point) estimate this by, e.g., fitting a linear model to the pseudo-observations and then applying the G-formula. Again, a confidence interval could be based on non-parametric bootstrap. This doesn’t rely on e.g. the proportional hazards assumption to be valid. (Do we still have to correctly model the conditional expectation of the pseudo-observations in the linear model? There’s a comment in the discussion section that we do, but given that ANCOVA is robust to this kind of miss-specification, can we hope for the same here?…I don’t know.)\n  Reproducing Table I Here are the first couple of rows of Table I from “Causal inference in survival analysis using pseudo-observations” by Per K. Andersen, Elisavet Syriopouloua, and Erik T. Parner.\n Table 1: Table I from Andersen et al. (2017)    n Cox_Coverage Cox_Bias Cox_SD Pseudo_Coverage Pseudo_Bias Pseudo_SD    200 0.946 0.000 0.074 0.945 0.000 0.078  400 0.940 -0.001 0.053 0.945 0.000 0.054    The simulation scenario was described as follows.\nThe baseline hazard was constant =0.511 for the unexposed (A=0), and the hazard ratio for exposure was HR=1.37. The exposure distribution was binomial with probability 0.50. Two further Gaussian variables L1 and L2 with a standard deviation of 0.2 were included with a hazard ratio of 1.1 per unit. This corresponds to a true ACE of 0.104=0.504−0.400 at time t0=1 (computed using 5,000,000 replications).\nLet’s start by checking whether I can reproduce that…\n######################################## ## check I have the correct scenario n \u0026lt;- 5000000 a \u0026lt;- rbinom(n, 1, 0.5) l_1 \u0026lt;- rnorm(n, sd = 0.2) l_2 \u0026lt;- rnorm(n, sd = 0.2) rate \u0026lt;- 0.511 * (1.37 * a + 1 * (1 -a)) * exp(log(1.1) * l_1 + log(1.1) * l_2) t_star \u0026lt;- rexp(n, rate = rate) true_ace \u0026lt;- mean(t_star[a == 1] \u0026lt; 1) - mean(t_star[a == 0] \u0026lt; 1) true_ace ## [1] 0.1038744 Looks good! Next, the censoring distribution was described.\nThe censoring time has a constant rate of 0.31, corresponding to an observed censoring frequency of 20.3% before time t0=1(computed using 5,000,000 replications). Subjects still at risk at t0=1 were (administratively) censored at that time.\nLet’s check that as well…\ncens \u0026lt;- rexp(n, 0.31) mean(cens \u0026lt; pmin(1, t_star)) ## [1] 0.2031564 Good stuff!\nNext, I’ll check whether I can reproduce the bias and SD results in Table I. I’ll focus on the pseudo-observations approach. This will require several steps. Firstly, I write a function to simulate one dataset according to the scenario described.\n####################################### ## function to simulate one data set sim_dat \u0026lt;- function(n){ a \u0026lt;- rbinom(n, 1, 0.5) l_1 \u0026lt;- rnorm(n, sd = 0.2) l_2 \u0026lt;- rnorm(n, sd = 0.2) rate \u0026lt;- 0.511 * (1.37 * a + 1 * (1 -a)) * exp(log(1.1) * l_1 + log(1.1) * l_2) t_star \u0026lt;- rexp(n, rate = rate) cens \u0026lt;- rexp(n, 0.31) times \u0026lt;- pmin(1, pmin(t_star, cens)) event \u0026lt;- as.numeric(t_star \u0026lt; pmin(cens, 1)) df \u0026lt;- data.frame(times = times, event = event, a = a, l_1 = l_1, l_2 = l_2) df } Then I write a function to calculate the pseudo-observations (via the R package {prodlim}):\nlibrary(prodlim) ######################### ## function to find pseudo values. ## df must have following columns ## \u0026quot;times\u0026quot; (numeric) ## \u0026quot;event\u0026quot; (numeric 0/1) ## \u0026quot;a\u0026quot; (numeric 0/1) ## \u0026quot;l_1\u0026quot; (numeric) ## \u0026quot;l_2\u0026quot; (numeric) pseudo_surv \u0026lt;- function(df, tau = 1){ f = prodlim(Hist(times, event) ~ 1, data = df) df$pseudo_obs \u0026lt;- unname(jackknife(f, times = tau)) df } And finally a function to apply the G-formula to the psuedo-values:\n############################### ## what do we do with the pseudovalues? ## fit a linear model and use g-formula to find ## average causal effect (ACE) ## df must have following columns ## \u0026quot;times\u0026quot; (numeric) ## \u0026quot;event\u0026quot; (numeric 0/1) ## \u0026quot;a\u0026quot; (numeric 0/1) ## \u0026quot;l_1\u0026quot; (numeric) ## \u0026quot;l_2\u0026quot; (numeric) ## \u0026quot;pseudo_obs\u0026quot; (numeric) find_ace \u0026lt;- function(df){ ## how many patients in data set n \u0026lt;- dim(df)[1] ## fit linear model to pseudovalues fit \u0026lt;- lm(pseudo_obs ~ a + l_1 + l_2, data = df) ## predict pseudovalue for each patient under A = 0 predict_0 = predict(fit, newdata = data.frame(a = rep(0, n), l_1 = df$l_1, l_2 = df$l_2)) ## predict pseudovalue for each patient under A = 1 predict_1 = predict(fit, newdata = data.frame(a = rep(1, n), l_1 = df$l_1, l_2 = df$l_2)) ## average causal effect is p(T\u0026gt;1 | A=0) - p(T \u0026gt; 1 | A=1) ace \u0026lt;- mean(predict_0) - mean(predict_1) ace } I could then link these three functions together to estimate the average causal effect for a single simulated data set\nsim_ace \u0026lt;- function(n){find_ace(pseudo_surv(sim_dat(n)))} sim_ace(n = 200) ## [1] 0.1387292 and I could do this 1,000 times (the paper used 10,000 but I’ll take my chances) and compare with Table I\n## n = 200 res_200 \u0026lt;- purrr::map_dbl(rep(200, 1000), sim_ace) mean(res_200) - true_ace #bias ## [1] -0.002274479 sd(res_200) ## [1] 0.07649481 ## n = 400 res_400 \u0026lt;- purrr::map_dbl(rep(400, 1000), sim_ace) mean(res_400) - true_ace #bias ## [1] -0.0001083186 sd(res_400) ## [1] 0.05380005 That’s good enough for me!\nReproducing the coverage Reproducing the coverage results is considerably more difficult (or, at least, more computationally intensive) than the bias / SD, because the confidence intervals are based on non-parametric bootstrap.\nThe following code would do it (via the R package {boot}) but it takes over an hour on my machine so I won’t run it now…\n################################### ## function allowing random sampling of rows of dataset boot_ace \u0026lt;- function(data, indices) { d \u0026lt;- data[indices,] # allows boot to select sample find_ace(pseudo_surv(d)) } ##################################### ## simulate a dataset of size n ## find bootstrap CI for ACE ## and check whether this covers ## the true ACE is_covered \u0026lt;- function(n, true_ace){ dat_orig \u0026lt;- sim_dat(n) ### based on 1000 bootstrap samples results \u0026lt;- boot::boot(data = dat_orig, statistic = boot_ace, R = 1000) boot_ci \u0026lt;- boot::boot.ci(results, type = \u0026quot;perc\u0026quot;) true_ace \u0026gt; boot_ci$percent[1,4] \u0026amp;\u0026amp; true_ace \u0026lt; boot_ci$percent[1,5] } ## find coverage for n = 200 based on 1000 simulated trials cov_200 \u0026lt;- purrr::map_dbl(rep(200, 1000), is_covered, true_ace = true_ace) mean(cov_200) …but you can trust me that it gave a result of 0.945 which matches close enough to Table I.\n  Comments The scenario considered in Table I was a correctly specified Cox model, so this example doesn’t show off the robustness properties of the pseudo-observation approach. I might revisit this in a future post if I get time.\nI feel a bit more confident using pseudo-observations now. But I still have some questions:\n just how robust is the method? It looks good in this example and the other examples from the paper, but does it breakdown? what about efficiency? It’s based on the Kaplan-Meier estimate, which involves essentially a dichotomization of the survival time at \\(t_0\\). This must lead to some inefficiency compared to a model. Not really apparent in these examples looking at the SDs, but here all observations were censored at \\(t_0\\) which is not reflective of projects I work on. how much is gained compared to simple Kaplan-Meier estimation? This will depend on the strength of the prognostic covariates. In this simulation example the prognostic covariates were pretty pathetic, so doubt much was gained.  Overall, I’m impressed with pseudo-observations. They are a quite a difficult concept, but don’t seem so far out of reach compared to other methods I’ve seen that provide unbiased estimation under model-misspecification. I’ll keep trying to learn more about this.\n References Andersen, Per K., Elisavet Syriopoulou, and Erik T. Parner. “Causal inference in survival analysis using pseudo‐observations.” Statistics in medicine 36.17 (2017): 2669-2681\nGraw, Frederik, Thomas A. Gerds, and Martin Schumacher. “On pseudo-values for regression analysis in competing risks models.” Lifetime Data Analysis 15.2 (2009): 241-255.\n ","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656663555,"objectID":"0269bd318834836ad7d7d8f870b0a9e7","permalink":"/post/2022-07-01-pseudo-observations-on-pseudo-observations/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/post/2022-07-01-pseudo-observations-on-pseudo-observations/","section":"post","summary":"Idea / motivation I’ve set myself the challenge of reproducing the results in Table I of “Causal inference in survival analysis using pseudo-observations” by Per K. Andersen, Elisavet Syriopouloua, and Erik T. Parner.\nI’m doing this because I’ve seen a lot about pseudo-observations recently, and while I kind of get the gist of it, I would like to deepen my understanding by getting hands on with some examples.","tags":[],"title":"Pseudo observations on pseudo-observations","type":"post"},{"authors":[],"categories":[],"content":"  Small simulation study Quick function to simulate data from an RCT with equal allocation to treatments 0 and 1, a single covariate, and a binary endpoint:\nsim_data \u0026lt;- function(n){ z \u0026lt;- rep(0:1, each = n / 2) ## trt assignment x \u0026lt;- rnorm(n) ## covariate eta \u0026lt;- -1 + 2 * x - log(0.55) * z ## linear predictor pi \u0026lt;- exp(eta) / (1 + exp(eta)) ## prob(outcome = 1) y \u0026lt;- rbinom(n, 1, pi) ## outcome data.frame(z, x, y) } The data generating mechansim is a logistic model (but also see later). I’ll draw it\nx \u0026lt;- seq(-2, 2, length.out = 100) plot(x, plogis(-1 + 2 * x), ylim = c(0,1), type = \u0026quot;l\u0026quot;, ylab = \u0026quot;P(Y=1)\u0026quot;) points(x, plogis(-1 + 2 * x - log(0.55)), col = 2, type = \u0026quot;l\u0026quot;) legend(\u0026quot;topleft\u0026quot;, c(\u0026quot;Z=0\u0026quot;, \u0026quot;Z=1\u0026quot;), lty = c(1,1), col = c(1,2)) Now I’ll write a function to give me:\nDirect adjustment via logistic regression including terms for treatment and the covariate. Giving conditional log-odds ratio. Standardization of (1) to give the unconditional log-odds ratio (using the stdReg package). Unadjusted analysis: logistic regression with just a term for treatment. Giving unconditional log-odds ratio.  library(\u0026quot;stdReg\u0026quot;) ###################################### sim_one_trial_with_analysis \u0026lt;- function(n){ ### simulate data dat \u0026lt;- sim_data(n) ### fit a direct adjustment model... fit_dir_adj \u0026lt;- glm(y ~ x + z, family = \u0026quot;binomial\u0026quot;, data = dat) ### ...extract point estimate and z statistic dir_adj_est \u0026lt;- summary(fit_dir_adj)$coef[\u0026quot;z\u0026quot;, \u0026quot;Estimate\u0026quot;] dir_adj_z \u0026lt;- summary(fit_dir_adj)$coef[\u0026quot;z\u0026quot;, \u0026quot;z value\u0026quot;] ### apply standardization / g-formula... std_fit \u0026lt;- stdReg::stdGlm(fit_dir_adj, data = dat, X = \u0026quot;z\u0026quot;) sum_std_fit \u0026lt;- summary(std_fit, transform = \u0026quot;logit\u0026quot;, contrast = \u0026quot;difference\u0026quot;, reference = 0) ### ...extract point estimate and z statistic g_est \u0026lt;- sum_std_fit$est.table[\u0026quot;1\u0026quot;, \u0026quot;Estimate\u0026quot;] g_z \u0026lt;- g_est / sum_std_fit$est.table[\u0026quot;1\u0026quot;, \u0026quot;Std. Error\u0026quot;] ### fit unadjusted model... fit_unadj \u0026lt;- glm(y ~ z, family = \u0026quot;binomial\u0026quot;, data = dat) ### ...extract point estimate and z statistic unadj_est \u0026lt;- summary(fit_unadj)$coef[\u0026quot;z\u0026quot;, \u0026quot;Estimate\u0026quot;] unadj_z \u0026lt;- summary(fit_unadj)$coef[\u0026quot;z\u0026quot;, \u0026quot;z value\u0026quot;] data.frame(dir_adj_est = dir_adj_est, dir_adj_z = dir_adj_z, g_est = g_est, g_z = g_z, unadj_est = unadj_est, unadj_z = unadj_z) } I’ll use this to simulate 1000 trials each with sample size 300…\nres \u0026lt;- purrr::map_df(rep(300, 1000), sim_one_trial_with_analysis) and plot the results, firstly in terms of the point estimates…\npar(mfrow = c(1,2)) plot(res$g_est, res$unadj_est, xlim = c(-0.5, 2), ylim = c(-0.5, 2), xlab = \u0026quot;log(OR) \u0026#39;standardization\u0026#39;\u0026quot;, ylab = \u0026quot;log(OR) \u0026#39;unadjusted\u0026#39;\u0026quot;) abline(a=0,b = 1,col = 2,lwd = 3) plot(res$g_est, res$dir_adj_est, xlim = c(-0.5, 2), ylim = c(-0.5, 2), xlab = \u0026quot;log(OR) \u0026#39;standardization\u0026#39;\u0026quot;, ylab = \u0026quot;log(OR) \u0026#39;direct adjusted\u0026#39;\u0026quot;) abline(a=0,b = 1,col = 2,lwd = 3) The standardized and unadjusted estimates do not appear systematically different. The directly adjusted point estimates are systematically larger (because they are estimates of a different parameter - the condidional log-odds ratio).\nNow the standardized Z-statistics…\npar(mfrow = c(1,2)) plot(res$g_z, res$unadj_z, xlab = \u0026quot;Z \u0026#39;standardization\u0026#39;\u0026quot;, ylab = \u0026quot;Z \u0026#39;unadjusted\u0026#39;\u0026quot;) abline(a=0,b = 1,col = 2,lwd = 3) abline(h = 2.33, col = 2, lty = 2) abline(v = 2.33, col = 2, lty = 2) plot(res$g_z, res$dir_adj_z, xlab = \u0026quot;Z \u0026#39;standardization\u0026#39;\u0026quot;, ylab = \u0026quot;Z \u0026#39;direct adjusted\u0026#39;\u0026quot;) abline(a=0,b = 1,col = 2,lwd = 3) abline(h = 2.33, col = 2, lty = 2) abline(v = 2.33, col = 2, lty = 2)  Appendix: misspecified model I’ll repeat the exercise for a misspecified model example. Here I’ll draw it the data generating mechanism…\nx \u0026lt;- seq(-2, 2, length.out = 100) plot(x, exp(-exp(2 - 2 * x)), ylim = c(0,1), type = \u0026quot;l\u0026quot;, ylab = \u0026quot;P(Y=1)\u0026quot;) points(x, exp(-exp(2 - 2 * x + log(0.4) * x)), col = 2, type = \u0026quot;l\u0026quot;) legend(\u0026quot;topleft\u0026quot;, c(\u0026quot;Z=0\u0026quot;, \u0026quot;Z=1\u0026quot;), lty = c(1,1), col = c(1,2)) …and here are the results showing similar things…\nsim_data \u0026lt;- function(n){ z \u0026lt;- rep(0:1, each = n / 2) x \u0026lt;- rnorm(n) eta \u0026lt;- 2 - 2 * x + log(0.4) * z * x pi \u0026lt;- exp(-exp(eta)) y \u0026lt;- rbinom(n, 1, pi) data.frame(z, x, y) } res \u0026lt;- purrr::map_df(rep(300, 1000), sim_one_trial_with_analysis) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred par(mfrow = c(1,2)) plot(res$g_est, res$unadj_est, xlim = c(-0.5, 2), ylim = c(-0.5, 2), xlab = \u0026quot;log(OR) \u0026#39;standardization\u0026#39;\u0026quot;, ylab = \u0026quot;log(OR) \u0026#39;unadjusted\u0026#39;\u0026quot;) abline(a=0,b = 1,col = 2,lwd = 3) plot(res$g_est, res$dir_adj_est, xlim = c(-0.5, 2), ylim = c(-0.5, 2), xlab = \u0026quot;log(OR) \u0026#39;standardization\u0026#39;\u0026quot;, ylab = \u0026quot;log(OR) \u0026#39;direct adjusted\u0026#39;\u0026quot;) abline(a=0,b = 1,col = 2,lwd = 3) par(mfrow = c(1,2)) plot(res$g_z, res$unadj_z, xlab = \u0026quot;Z \u0026#39;standardization\u0026#39;\u0026quot;, ylab = \u0026quot;Z \u0026#39;unadjusted\u0026#39;\u0026quot;) abline(a=0,b = 1,col = 2,lwd = 3) abline(h = 2.33, col = 2, lty = 2) abline(v = 2.33, col = 2, lty = 2) plot(res$g_z, res$dir_adj_z, xlab = \u0026quot;Z \u0026#39;standardization\u0026#39;\u0026quot;, ylab = \u0026quot;Z \u0026#39;direct adjusted\u0026#39;\u0026quot;) abline(a=0,b = 1,col = 2,lwd = 3) abline(h = 2.33, col = 2, lty = 2) abline(v = 2.33, col = 2, lty = 2)  ","date":1653436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653462605,"objectID":"242c0e3a02686a9a469db1f675b04cc4","permalink":"/post/2022-05-25-conditional-and-unconditional-treatment-effects-in-rcts/","publishdate":"2022-05-25T00:00:00Z","relpermalink":"/post/2022-05-25-conditional-and-unconditional-treatment-effects-in-rcts/","section":"post","summary":"Small simulation study Quick function to simulate data from an RCT with equal allocation to treatments 0 and 1, a single covariate, and a binary endpoint:\nsim_data \u0026lt;- function(n){ z \u0026lt;- rep(0:1, each = n / 2) ## trt assignment x \u0026lt;- rnorm(n) ## covariate eta \u0026lt;- -1 + 2 * x - log(0.55) * z ## linear predictor pi \u0026lt;- exp(eta) / (1 + exp(eta)) ## prob(outcome = 1) y \u0026lt;- rbinom(n, 1, pi) ## outcome data.","tags":[],"title":"Conditional and unconditional treatment effects in RCTs","type":"post"},{"authors":[],"categories":[],"content":"  I already feel exhausted after writing the title. I’m not really going to explain all these differences, sorry. I’m not capable. And the terms are not used in a consistent way anyway. Two references that I have found helpful are\n Lecture https://myweb.uiowa.edu/pbreheny/7210/f15/notes/9-3.pdf by Patrick Breheny Paper https://doi.org/10.2307/2529941 by Stephen Lagakos  The crucial question when thinking about censoring mechanisms is whether or not it is valid to use the same likelihood function as you would have done had the censoring times been fixed in advance. If not the case, this would invalidate the use of standard survival analysis tecniques (KM, Cox, standard parametric models,…).\nThinking directly about likelihood functions may be a bit too abstract. For me, in the context of clinical trials, the criterion I find most helpful is:\n“A censored observation at time \\(t\\) provides only the information that the true survival time exceeds \\(t\\)…the event of censoring provides no prognostic information about subsequent survival time.”\nThat was a quote from the Lagakos paper. He refers to this condition as “non-prognostic censoring.”\nIf this condition holds then it is valid to use the same likelihood as if the censoring times had been fixed. (One could easily think of situations where it might not hold e.g. dropout due to an adverse event, etc.)\nThis condition is also a bit more general than what he defines as “independent censoring”.\n“Independent” (or possibly “random” censoring) Consider a trial where everyone has a “true” survival time \\(\\tilde{T}_i\\) (which is a random variable), everyone has a censoring time \\(C_i\\) (which is a random variable), and all of these random variables are all independent of each other. Furthermore, the distribution of \\(\\tilde{T}_i\\) is assumed to be “functionally independent” to the distribution of \\(C\\) (I think this basically means that the distributions do not share any parameters). What we observe is \\(T_i = \\min(\\tilde{T}_i, C_i)\\) and \\(\\delta_i = I\\{\\tilde{T}_i \u0026lt; C_i\\}\\).\nThis is how Lagakos defines “independent censoring”, although he does say “also referred to as random censorship”. In his lecture notes, Breheny calls this a “random censorship model”.\nAn example would be a clinical trial where patients enter the study at a time point that is random, and the study ends at a pre-fixed calendar time. This particular case is sometimes referred to as type I censoring.\nThis situation of random/independent censoring can also be extended such that the random variables are only independent of each other conditional on the value of some covariate \\(x\\). An example would be if patients recruited earlier in the trial tended to have a better/worse prognosis than patients recruited later in the trial. In this case, there is a covariate (time of recruitment) that could be used to account for this.\n “Non-prognostic” (or possibly “independent” censoring) Consider an event-driven clinical trial that stops after a pre-specified number of events has been observed. This particular case is sometimes called type II censoring. Strictly speaking, \\(\\tilde{T}_i\\) and \\(C_i\\) are not independent random variables, so this doesn’t fall into the category above. However, it may still be reasonable to assume that the censoring provides no prognostic information about subsequent survival time. Hence Lagakos’ use of “non-prognostic” to capture this important set of situations.\nSometimes, however, the term “independent censoring mechanism” is used in a more general sense to cover any situation where it is valid to use the same likelihood as if the censoring times had been fixed. This is how Breheny uses the term in his lecture notes. “Ignorable” is another word that might be used for this.\nApparently, the set of situations where it is valid to use the same likelihood function as if the censoring times had been fixed is broader than just the situations satisfying the “non-prognostic” condition (which is already broader than “independent/random” censoring in the sense of the previous section). Lagakos provides a technical condition that captures this broader set of situations, and he calls censoring models that satisfy the condition “non-informative censoring models”. Unfortunately, I don’t understand this condition. But he himself says it “is the least easily interpreted condition […] in terms of the physical process, yet it is the most general one mathematically.” I take it from this that I don’t need to worry about it.\n Another perspective on “(non)informative” censoring Interestingly, in his lecture notes, Breheny uses informative/non-informative in a slightly different (I think) sense, by considering time-to-event and time-to-censoring distributions that have a shared parameter. Then, censoring would be “informative” about the parameter of interest even for the “non-prognostic” censoring situation above. In this case, it would still be “valid” (though inefficient) to use the same likelihood as if the censoring times had been fixed (see also this stack exchange post). However, I’ve gone past the limits of my knowledge here. As I can’t really imagine something like this in the context I work in, I’m not going to worry about it either.\n References  Lecture https://myweb.uiowa.edu/pbreheny/7210/f15/notes/9-3.pdf by Patrick Breheny Paper https://doi.org/10.2307/2529941 by Lagakos   ","date":1642550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642603565,"objectID":"ba336f7766deb2103e68abe1187e1adc","permalink":"/post/2022-01-19-difference-between-fixed-random-independent-type-i-type-ii-non-prognostic-non-ignorable-and-non-informative-censoring/","publishdate":"2022-01-19T00:00:00Z","relpermalink":"/post/2022-01-19-difference-between-fixed-random-independent-type-i-type-ii-non-prognostic-non-ignorable-and-non-informative-censoring/","section":"post","summary":"I already feel exhausted after writing the title. I’m not really going to explain all these differences, sorry. I’m not capable. And the terms are not used in a consistent way anyway. Two references that I have found helpful are\n Lecture https://myweb.uiowa.edu/pbreheny/7210/f15/notes/9-3.pdf by Patrick Breheny Paper https://doi.org/10.2307/2529941 by Stephen Lagakos  The crucial question when thinking about censoring mechanisms is whether or not it is valid to use the same likelihood function as you would have done had the censoring times been fixed in advance.","tags":[],"title":"What's the difference between fixed, random, independent, type I, type II, (non)prognostic, (non)ignorable, and (non)informative censoring?","type":"post"},{"authors":[],"categories":["clinical trials","survival analysis"],"content":"  (Mild) panic In my previous post I looked into how survival::survfit produces standard errors and confidence intervals for a survival curve based on a Cox proportional hazards model. I discovered (I could also have just read it from the documentation) that when you ask for the standard error fit_1$std.err after fit_1 \u0026lt;- survfit(...), it provides you not with the standard error of the estimator of the survival probability, but instead with the standard error of the estimator of the cumulative hazard.\nIt then occurred to me that I had been using survival::survfit in two of my recent papers. And I had been extracting the standard errors in order to calculate a test statistic for the difference between milestone survival probabilities on the two treatment arms of a clinical trial.\n“Oh s***…have I included standard errors for the wrong thing?…are my results wrong?…am I going to have to submit corrections to the articles?”\nThis motivated me to look more closely again at what survival::survfit is doing. Something I obviously should have done as I was writing the papers!\n Kaplan-Meier estimation My examples just involved Kaplan-Meier estimation and not a Cox model, so I’ll quickly fit a KM for a toy example.\n### create a toy data set df \u0026lt;- data.frame(time = c(3,5,7,12,17,19,25,26,30), event = rep(1,9)) ### Kaplan-Meier estimate library(survival) fit \u0026lt;- survfit(Surv(time, event) ~ 1, data = df) fit$std.err ## [1] 0.1178511 0.1781742 0.2357023 0.2981424 0.3726780 0.4714045 0.6236096 ## [8] 0.9428090 Inf As mentioned, this is the standard error for the cumulative hazard. I could have derived it “manually” based on the formulas in slide 22 of this presentation https://mathweb.ucsd.edu/~rxu/math284/slect2.pdf, noting that \\(\\log(S)=-H\\),\nn_at_risk \u0026lt;- 9:1 event \u0026lt;- rep(1, 9) var_H \u0026lt;- cumsum(event / (n_at_risk - event) / n_at_risk) sqrt(var_H)  ## [1] 0.1178511 0.1781742 0.2357023 0.2981424 0.3726780 0.4714045 0.6236096 ## [8] 0.9428090 Inf ### this matches fit$std.err ## [1] 0.1178511 0.1781742 0.2357023 0.2981424 0.3726780 0.4714045 0.6236096 ## [8] 0.9428090 Inf What happens when you apply summary.survfit? The summary.survfit method is useful for getting some more detailed information about your model.\nsummary(fit) ## Call: survfit(formula = Surv(time, event) ~ 1, data = df) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 3 9 1 0.889 0.105 0.7056 1.000 ## 5 8 1 0.778 0.139 0.5485 1.000 ## 7 7 1 0.667 0.157 0.4200 1.000 ## 12 6 1 0.556 0.166 0.3097 0.997 ## 17 5 1 0.444 0.166 0.2141 0.923 ## 19 4 1 0.333 0.157 0.1323 0.840 ## 25 3 1 0.222 0.139 0.0655 0.754 ## 26 2 1 0.111 0.105 0.0175 0.705 ## 30 1 1 0.000 NaN NA NA Notice here that the std.err looks different to before, and indeed…\nsummary(fit)$std.err ## [1] 0.1047566 0.1385799 0.1571348 0.1656347 0.1656347 0.1571348 0.1385799 ## [8] 0.1047566 NaN ### is not the same as fit$std.err ## [1] 0.1178511 0.1781742 0.2357023 0.2981424 0.3726780 0.4714045 0.6236096 ## [8] 0.9428090 Inf It turns out summary(fit)$std.err is giving the standard error of the estimator of the survival probability. From the formula at the bottom of the slide (printed above), we see that this is just equal to the estimate of the survival probability multiplied by the standard error of the cumulative hazard:\nsqrt(var_H) * fit$surv ## [1] 0.1047566 0.1385799 0.1571348 0.1656347 0.1656347 0.1571348 0.1385799 ## [8] 0.1047566 NaN ### this matches summary(fit)$std.err ## [1] 0.1047566 0.1385799 0.1571348 0.1656347 0.1656347 0.1571348 0.1385799 ## [8] 0.1047566 NaN Fortunately, I’d been using summary(fit)$std.err and not fit$std.err in my code, so I could breathe a sigh of relief.\n  Confidence intervals Although it turned out I hadn’t made an error in my papers, I’m still a bit disappointed I didn’t think more carefully about this issue. A standard error for \\(S\\) suffers from the obvious limitation that \\(S\\) is restricted to \\((0,1)\\). A standard error for \\(\\log(S)\\) is probably better, but \\(\\log(S)\\) is still restricted to \\((-\\infty, 0)\\). A standard error for \\(\\log(-\\log(S))\\) might be the best option. survfit allows you to use these transformations when constructing confidence intervals.\nConfidence intervals for S based on s.e.(S) ### task: can I reproduce confidence intervals? fit_plain \u0026lt;- survfit(Surv(time, event) ~ 1, data = df, conf.type = \u0026quot;plain\u0026quot;) ## reported lower CI for S fit_plain$lower ## [1] 0.68356980 0.50616616 0.35868804 0.23091758 0.11980647 0.02535471 0.00000000 ## [8] 0.00000000 NaN ## standard error of cumulative hazard se_H \u0026lt;- fit_plain$std.err ## standard error of S se_S \u0026lt;- se_H * fit_plain$surv ## lower CI (forced to be between 0 and 1) pmax(0, pmin(1, fit_plain$surv - se_S * qnorm(.975))) ## [1] 0.68356980 0.50616616 0.35868804 0.23091758 0.11980647 0.02535471 0.00000000 ## [8] 0.00000000 NaN ## this matches fit_plain$lower ## [1] 0.68356980 0.50616616 0.35868804 0.23091758 0.11980647 0.02535471 0.00000000 ## [8] 0.00000000 NaN  Confidence intervals for S based on s.e.(log(S)) ### task: can I reproduce confidence intervals? fit_log \u0026lt;- survfit(Surv(time, event) ~ 1, data = df, conf.type = \u0026quot;log\u0026quot;) ## reported lower CI for S fit_log$lower ## [1] 0.70555750 0.54852119 0.42002836 0.30970501 0.21408853 0.13231787 0.06545910 ## [8] 0.01750802 NA ## standard error of cumulative hazard se_H \u0026lt;- fit_log$std.err ## standard error of log(S) se_log_S \u0026lt;- se_H ## lower CI (forced to be less than 1) pmin(1, exp(log(fit_log$surv) - se_log_S * qnorm(.975))) ## [1] 0.70555750 0.54852119 0.42002836 0.30970501 0.21408853 0.13231787 0.06545910 ## [8] 0.01750802 0.00000000 ## this matches fit_log$lower ## [1] 0.70555750 0.54852119 0.42002836 0.30970501 0.21408853 0.13231787 0.06545910 ## [8] 0.01750802 NA  Confidence intervals for S based on s.e.(log(-log(S))) ### task: can I reproduce confidence intervals? fit_log_log \u0026lt;- survfit(Surv(time, event) ~ 1, data = df, conf.type = \u0026quot;log-log\u0026quot;) ## reported lower CI for S fit_log_log$lower ## [1] 0.432965090 0.364751231 0.281682242 0.204241776 0.135872478 0.078289473 ## [7] 0.033711455 0.006129242 NA ## standard error of cumulative hazard se_H \u0026lt;- fit_log_log$std.err ## standard error of log(-log((S)) [by delta method -- see slide deck] se_log_log_S \u0026lt;- se_H / log(fit_log_log$surv) ## lower CI exp(-exp(log(-log(fit_log_log$surv)) - se_log_log_S * qnorm(.975))) ## [1] 0.432965090 0.364751231 0.281682242 0.204241776 0.135872478 0.078289473 ## [7] 0.033711455 0.006129242 NaN ## this matches fit_log_log$lower ## [1] 0.432965090 0.364751231 0.281682242 0.204241776 0.135872478 0.078289473 ## [7] 0.033711455 0.006129242 NA   References https://mathweb.ucsd.edu/~rxu/math284/slect2.pdf\n ","date":1642464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642500853,"objectID":"c326f3f5101aa6a268e8704478f35a52","permalink":"/post/2022-01-18-be-careful-with-standard-errors-in-survival-survfit/","publishdate":"2022-01-18T00:00:00Z","relpermalink":"/post/2022-01-18-be-careful-with-standard-errors-in-survival-survfit/","section":"post","summary":"(Mild) panic In my previous post I looked into how survival::survfit produces standard errors and confidence intervals for a survival curve based on a Cox proportional hazards model. I discovered (I could also have just read it from the documentation) that when you ask for the standard error fit_1$std.err after fit_1 \u0026lt;- survfit(...), it provides you not with the standard error of the estimator of the survival probability, but instead with the standard error of the estimator of the cumulative hazard.","tags":[],"title":"Be careful with standard errors in `survival::survfit`","type":"post"},{"authors":[],"categories":["survival analysis"],"content":"  A colleague caught me out recently when they asked about a confidence interval for a survival curve based on a Cox model. This can be done in R using survival::survfit after survival::coxph. But the question was: does this take into account the uncertainty in the baseline hazard. I had to admit that I wasn’t 100% sure. So here is an example to clear it up…\n1. Understanding survfit.coxph standard errors Create a toy data set and apply survfit.coxph library(survival) ### create a toy data set df \u0026lt;- data.frame(time = c(3,5,7,12,17,19,25,26,30), event = rep(1,9), trt = factor(c(\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;,\u0026quot;A\u0026quot;))) fit_cox \u0026lt;- coxph(Surv(time, event) ~ trt, data = df) surv_cox \u0026lt;- survfit(fit_cox, newdata = data.frame(trt = c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;)))  Can I reproduce the estimates and standard errors? Here are the estimates of survival and std.err (of something) I would like to reproduce in order to understand how they were calculated. The first column corresponds to trt==\"A\" and the second column to trt==\"B\"…\nsurv_cox$surv ## 1 2 ## [1,] 0.90379995 0.88376336 ## [2,] 0.80760960 0.77025266 ## [3,] 0.70882912 0.65677247 ## [4,] 0.61007108 0.54677825 ## [5,] 0.50768592 0.43685968 ## [6,] 0.40537172 0.33184585 ## [7,] 0.29719960 0.22711842 ## [8,] 0.18948065 0.13105127 ## [9,] 0.06970604 0.03862672 surv_cox$std.err ## [,1] [,2] ## [1,] 0.1072286 0.1313337 ## [2,] 0.1709941 0.2049614 ## [3,] 0.2351795 0.2853216 ## [4,] 0.3095973 0.3685374 ## [5,] 0.3930877 0.4799235 ## [6,] 0.5054737 0.6046534 ## [7,] 0.6435328 0.8159658 ## [8,] 0.8908094 1.0923455 ## [9,] 1.3392316 2.1694201 Step 1: Estimate the baseline cumulative hazard This uses Breslow’s estimator (see slide 39 of https://www.uio.no/studier/emner/matnat/math/STK4080/h16/undervisningsmateriell/lecture8_16.pdf)\nThe Cox model \\(h_0(t)\\exp(x\\beta)\\) has been parameterized in such a way that \\(x=0\\) corresponds to trt==\"A\" and \\(x=1\\) corresponds to trt==\"B\".\nbeta_hat \u0026lt;- fit_cox$coef df$x \u0026lt;- ifelse(df$trt == \u0026quot;B\u0026quot;, 1, 0) df$exp_b \u0026lt;- exp(df$x * beta_hat) ## See formula for Breslow estimator: H_0 \u0026lt;- cumsum(1 / rev(cumsum(rev(df$exp_b)))) H_0 ## [1] 0.1011472 0.2136765 0.3441408 0.4941798 0.6778923 0.9029508 1.2133513 ## [8] 1.6634684 2.6634684 Note I could instead have used the basehaz function:\nbasehaz(fit_cox) ## hazard time ## 1 0.1011472 3 ## 2 0.2136765 5 ## 3 0.3441408 7 ## 4 0.4941798 12 ## 5 0.6778923 17 ## 6 0.9029508 19 ## 7 1.2133513 25 ## 8 1.6634684 26 ## 9 2.6634684 30  Step 2: Transform to estimated survival curve ### For trt == \u0026quot;A\u0026quot; exp(-H_0) ## [1] 0.90379995 0.80760960 0.70882912 0.61007108 0.50768592 0.40537172 0.29719960 ## [8] 0.18948065 0.06970604 ## this matches surv_cox$surv[,1] ## [1] 0.90379995 0.80760960 0.70882912 0.61007108 0.50768592 0.40537172 0.29719960 ## [8] 0.18948065 0.06970604 ### For trt == \u0026quot;B\u0026quot; ### the cumulative hazard is equal to the baseline ### cumulative hazard muliplied by exp(beta) exp(-H_0 * exp(beta_hat)) ## [1] 0.88376336 0.77025266 0.65677247 0.54677825 0.43685968 0.33184585 0.22711842 ## [8] 0.13105127 0.03862672 ## this matches: surv_cox$surv[,2] ## [1] 0.88376336 0.77025266 0.65677247 0.54677825 0.43685968 0.33184585 0.22711842 ## [8] 0.13105127 0.03862672  Step 3: Variance of Breslow’s estimator The variance of Breslow’s estimator is somewhat complicated. It is given on slide 47 of https://www.uio.no/studier/emner/matnat/math/STK4080/h16/undervisningsmateriell/lecture8_16.pdf\nAnd here I just implement their result:\n## see definition of s_0 and s_1 in slides: s_0 = rev(cumsum(rev(df$exp_b))) s_1 = rev(cumsum(rev(df$exp_b) * rev(df$x))) var_beta \u0026lt;- c(fit_cox$var) var_breslow \u0026lt;- var_beta * cumsum(s_1 / s_0 ^ 2) ^ 2 + cumsum(1 / s_0 ^ 2) ## Note that... sqrt(var_breslow) ## [1] 0.1072286 0.1709941 0.2351795 0.3095973 0.3930877 0.5054737 0.6435328 ## [8] 0.8908094 1.3392316 ## ... already matches surv_cox$std.err[,1] ## [1] 0.1072286 0.1709941 0.2351795 0.3095973 0.3930877 0.5054737 0.6435328 ## [8] 0.8908094 1.3392316 So the $.std.err given in survfit.coxph is reporting the standard error of the cumulative hazard. When trt==\"A\" this is the same as the standard error of the baseline cumulative hazard.\nTo get the standard error when trt==\"B\", note that the cumulative hazard on treatment B is equal to the baseline cumulative hazard multiplied by \\(\\exp(\\beta)\\). So we have to use the delta method, taking into account the variance of the breslow estimator, the variance of beta_hat, and their covariance.\nThis is messy.\nTo find their covariance, we use the formulas from https://www.uio.no/studier/emner/matnat/math/STK4080/h16/undervisningsmateriell/lecture8_16.pdf again, where they use the notation \\(A\\) for the cumulative hazard:\nOne can see from this that the covariance between \\(\\hat{\\beta}\\) and the Breslow estimator is going to be…\ncov_breslow_beta \u0026lt;- -var_beta * cumsum(s_1 / s_0 ^ 2)  Now we are ready to use the delta method (where \\(\\Sigma\\) is the covariance matrix between the Breslow estimator and \\(\\hat{\\beta}\\)):\n\\[\\text{var} \\left\\lbrace \\hat{A}_0 \\exp(x\\hat{\\beta}) \\right\\rbrace = (\\exp(x\\hat{\\beta}), x\\hat{A}_0 \\exp(x\\hat{\\beta}))\\hat{\\Sigma} (\\exp(x\\hat{\\beta}), x\\hat{A}_0 \\exp(x\\hat{\\beta}))^T\\]\nApplying this to our example:\nvar_chaz_B \u0026lt;- numeric(length(H_0)) for (i in seq_along(H_0)){ Sigma_hat_i \u0026lt;- matrix(c(var_breslow[i], cov_breslow_beta[i], cov_breslow_beta[i], var_beta), 2, 2) var_chaz_B[i] \u0026lt;- c(exp(beta_hat), H_0[i] * exp(beta_hat)) %*% Sigma_hat_i %*% c(exp(beta_hat), H_0[i] * exp(beta_hat)) } sqrt(var_chaz_B) ## [1] 0.1313337 0.2049614 0.2853216 0.3685374 0.4799235 0.6046534 0.8159658 ## [8] 1.0923455 2.1694201 surv_cox$std.err[,2] ## [1] 0.1313337 0.2049614 0.2853216 0.3685374 0.4799235 0.6046534 0.8159658 ## [8] 1.0923455 2.1694201  Step 4 (bonus). Confidence interval for survival curve I’ll just focus on the lower confidence bounds:\nsurv_cox$lower ## 1 2 ## [1,] 0.732485749 0.6831947370 ## [2,] 0.577631682 0.5154300933 ## [3,] 0.447050313 0.3754471028 ## [4,] 0.332545266 0.2655287902 ## [5,] 0.234962333 0.1705416091 ## [6,] 0.150519749 0.1014505772 ## [7,] 0.084192245 0.0458881994 ## [8,] 0.033060280 0.0154040543 ## [9,] 0.005050267 0.0005498878 Having found the standard error for the cumulative hazard, it’s easy to convert this into a confidence interval for the survival curve:\ncbind(exp(-(H_0 + sqrt(var_breslow) * qnorm(.975))), exp(-(H_0 * exp(beta_hat) + sqrt(var_chaz_B) * qnorm(.975)))) ## [,1] [,2] ## [1,] 0.732485749 0.6831947370 ## [2,] 0.577631682 0.5154300933 ## [3,] 0.447050313 0.3754471028 ## [4,] 0.332545266 0.2655287902 ## [5,] 0.234962333 0.1705416091 ## [6,] 0.150519749 0.1014505772 ## [7,] 0.084192245 0.0458881994 ## [8,] 0.033060280 0.0154040543 ## [9,] 0.005050267 0.0005498878    Confidence interval for marginal survival curve Having established how surfit.coxph calculates standard errors and confidence intervals for a particular fixed covariate \\(x\\), the next question is: can one produce a confidence interval for the marginal survival curve, standardized over a distribution on \\(x\\), for example the distribution from the original data set?\nI don’t know an “official” answer for this, but based on the above, one option that occurs to me is to simulate from the normal approximation to the joint distribution of \\((\\hat{A}_0, \\hat{\\beta})\\), then, for each realization, calculate the marginal survival probability, then take quantiles.\nWe can try it, anyway.\nlibrary(mvtnorm) ## set up dummy vector to store results marginal_mean \u0026lt;- numeric(length(H_0)) marginal_lower \u0026lt;- numeric(length(H_0)) marginal_upper \u0026lt;- numeric(length(H_0)) ## function to calculate the mean survival curve ## averaged across the covariates in the original ## data frame. ## Note that the normal approximation to the cumulative ## hazard may stray below 0 (meaning survival above 1), ## so I have to use pmin here: get_mean_surv \u0026lt;- function(sims){ mean(pmin(1, exp(-sims[1] * exp(sims[2] * df$x)))) } ## For each time point t_i... for (i in seq_along(H_0)){ ## ...extract mean... Mu_hat_i \u0026lt;- c(H_0[i], beta_hat) ## ...and covariance of (Breslow, beta_hat) Sigma_hat_i \u0026lt;- matrix(c(var_breslow[i], cov_breslow_beta[i], cov_breslow_beta[i], var_beta), 2, 2) ## simulate from this distribution sims_i \u0026lt;- rmvnorm(10000, mean = Mu_hat_i, sigma = Sigma_hat_i) ## find the marginal survival curve for each realization mean_surv_i \u0026lt;- apply(sims_i, 1, get_mean_surv) ## store mean and quantiles to use as estimate and CI marginal_mean[i] \u0026lt;- mean(mean_surv_i) marginal_lower[i] \u0026lt;- quantile(mean_surv_i, c(0.025)) marginal_upper[i] \u0026lt;- quantile(mean_surv_i, c(0.975)) } ## Print result for our example cbind(mean = marginal_mean, lower = marginal_lower, upper = marginal_upper) ## mean lower upper ## [1,] 0.8880155 0.696913234 1.0000000 ## [2,] 0.7972256 0.554475707 1.0000000 ## [3,] 0.7049566 0.436028562 1.0000000 ## [4,] 0.6157913 0.341393835 1.0000000 ## [5,] 0.5244201 0.253445004 1.0000000 ## [6,] 0.4343175 0.175397268 1.0000000 ## [7,] 0.3425416 0.107482202 1.0000000 ## [8,] 0.2483972 0.049395838 1.0000000 ## [9,] 0.1469971 0.006160858 0.9667786 Obviously, this toy data set is too small to rely on the normal approximations here. But in principle I think this method is reasonable. Of course, it assumes the Cox model is correct.\n ","date":1642377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642417787,"objectID":"c1e24661a8d49446efdc4654d9e54055","permalink":"/post/2022-01-17-confidence-interval-for-a-survival-curve-based-on-a-cox-model/","publishdate":"2022-01-17T00:00:00Z","relpermalink":"/post/2022-01-17-confidence-interval-for-a-survival-curve-based-on-a-cox-model/","section":"post","summary":"A colleague caught me out recently when they asked about a confidence interval for a survival curve based on a Cox model. This can be done in R using survival::survfit after survival::coxph. But the question was: does this take into account the uncertainty in the baseline hazard. I had to admit that I wasn’t 100% sure. So here is an example to clear it up…\n1. Understanding survfit.coxph standard errors Create a toy data set and apply survfit.","tags":[],"title":"Confidence interval for a survival curve based on a Cox model","type":"post"},{"authors":[],"categories":[],"content":"  Motivation I don’t understand the details of concordance in survival analysis, so I’m trying to explain it to myself.\n Probabilistic index I recently read the very nice paper On the interpretation of the hazard ratio in Cox regression by Jan De Neve and Thomas Gerds.\nSuppose in the context of a clinical trial we use a Cox model to compare an experimental treatment (A=1) with a control treatment (A=0), where (for simplicity here) treatment is the only term in the model (assume the model is correct, by the way). Let \\(\\theta\\) denote the hazard ratio.\nDe Neve and Gerds recommend interpreting the treatment effect as a probabilistic index, that is, the probability that a randomly chosen patient on the experimental arm has a survival time (\\(T_{1,j}\\)) that is longer than the survival time (\\(T_{0,i}\\)) for a randomly chosen patient on the control arm . They show that:\n\\[P(T_{0,i} \u0026lt; T_{1,j}) = \\frac{1}{1 + \\theta}.\\]\nThat’s a super interesting relationship. However, they only show a proof for uncensored data. How should we interpret the probabilistic index in a trial with a maximum follow-up time and administrative right censoring (i.e. almost every trial)?\nIn their discussion, De Neve and Gerds say that we must assume proportional hazards also holds after the maximum follow up time. This suggests that when interpreting \\(P(T_{0,i} \u0026lt; T_{1,j})\\), we are indeed thinking about an infinite follow-up.\nThis makes me slightly uncomfortable. We can check (to some extent) whether proportional hazards looks like a reasonable assumption for the duration of the study. Assuming it continues to hold until all patients have had their event is a bit more of a stretch. It got me wondering: does the following relationship still hold:\n\\[P(T_{0,i} \u0026lt; T_{1,j} \\mid \\text{It\u0026#39;s possible to know given censoring}) = \\frac{1}{1 + \\theta}?\\]\nHow can I formulate that a bit more precisely? Denote the censoring times for patients \\(i\\) and \\(j\\) as \\(C_{0,i}\\) and \\(C_{1,j}\\) (and assume, by the way, that censoring and survival distributions are independent). In order to know for sure whether or not \\(T_{0,i} \u0026lt; T_{1,j}\\), I need the minimum of \\(T_{1,j}\\) and \\(T_{0,i}\\) to be uncensored. Also, if the maximum of \\(T_{1,j}\\) and \\(T_{0,i}\\) is censored, I at least need the censoring time to come after the minimum of \\(T_{1,j}\\) and \\(T_{0,i}\\). So, having picked two patients at random, in order to establish who has the better survival time I need \\(\\min\\{T_{0,i},T_{1,j}\\} \u0026lt; \\min\\{C_{0,i}, C_{1,j} \\}\\). My question becomes: does\n\\[P(T_{0,i} \u0026lt; T_{1,j} \\mid \\min\\{T_{0,i},T_{1,j}\\} \u0026lt; \\min\\{C_{0,i}, C_{1,j} \\} ) = \\frac{1}{1 + \\theta}?\\]\nIs the answer obvious? I have a bad feeling it might be and I’m about to do some unnecessary, tedious maths. I’ll condition on the censoring times. Let \\(\\tau:= \\min\\{C_{0,i}, C_{1,j}\\}\\). I’m interested in\n\\[ \\begin{aligned} P(T_{0,i} \u0026lt; T_{1,j} \\mid \\min\\{T_{0,i},T_{1,j}\\} \u0026lt; \\tau ) \u0026amp; = P(T_{0,i} \u0026lt; T_{1,j} \\mid T_{0,i} \u0026lt; \\tau \\cup T_{1,j} \u0026lt; \\tau ) \\\\ \u0026amp; = \\frac{P(T_{0,i} \u0026lt; T_{1,j} \\cap T_{0,i} \u0026lt; \\tau )}{ P( T_{0,i} \u0026lt; \\tau \\cup T_{1,j} \u0026lt; \\tau )} \\\\ \\end{aligned} \\]\nwhere I have used the fact that if \\(T_{1,j} \u0026lt; \\tau\\) but \\(T_{0,j} \u0026gt; \\tau\\), then it is impossible for \\(T_{0,i} \u0026lt; T_{1,j}\\). Starting with the numerator, I apply the survival distribution \\(S_1\\) to everything:\n\\[ \\begin{aligned} P(T_{0,i} \u0026lt; T_{1,j} \\cap T_{0,i} \u0026lt; \\tau )\u0026amp; = P(S_1(T_{0,i}) \u0026gt; S_1(T_{1,j}) \\cap S_1(T_{0,i}) \u0026gt; S_1(\\tau) )\\\\ \u0026amp;= P(S_0(T_{0,i})^\\theta \u0026gt; S_1(T_{1,j}) \\cap S_0(T_{0,i})^\\theta \u0026gt; S_1(\\tau) )\\\\ \u0026amp;= P(U_0^\\theta \u0026gt; U_1 \\cap U_0 \u0026gt; u_*^{1/\\theta} ) \\end{aligned} \\] where I have used proportional hazards, \\(S_1(t)=S_0(t)^\\theta\\) for \\(t \\in (0,\\tau)\\), and defined \\(U_0 = S_0(T_{0,i})\\), \\(U_1 = S_1(T_{1,i})\\), and \\(u_*=S_1(\\tau)\\). Since \\(U_0\\) and \\(U_1\\) are independent uniform, we have\n\\[ \\begin{aligned} P(U_0^\\theta \u0026gt; U_1 \\cap U_0 \u0026gt; u_*^{1/\\theta} ) \u0026amp;= \\int_{u_*^{1/\\theta}}^1 u_0^\\theta du_0 \\\\ \u0026amp;= \\left[ \\frac{u_0^{\\theta+1}}{\\theta+1} \\right]_{u_*^{1/\\theta}}^1 \\\\ \u0026amp;= \\frac{1 - u_*^{1+\\frac{1}{\\theta}}}{\\theta + 1} \\end{aligned} \\]\nSimilarly for the denominator, \\[ \\begin{aligned} P( T_{0,i} \u0026lt; \\tau \\cup T_{1,j} \u0026lt; \\tau )\u0026amp; = P( S_1(T_{0,i}) \u0026gt; S_1(\\tau) \\cup S_1(T_{1,j}) \u0026gt; S_1(\\tau) )\\\\ \u0026amp;= P( S_0(T_{0,i})^{\\theta} \u0026gt; S_1(\\tau) \\cup S_1(T_{1,j}) \u0026gt; S_1(\\tau) ) \\\\ \u0026amp;= P( U_0 \u0026gt; u_*^{1/\\theta} \\cup U_1 \u0026gt; u_* )\\\\ \u0026amp;= 1 - P( U_0 \u0026lt; u_*^{1/\\theta} \\cap U_1 \u0026lt; u_* ) \\\\ \u0026amp;= 1 - u_*^{1+\\frac{1}{\\theta}}, \\end{aligned} \\] and dividing the numerator by the denominator finally gives the result that \\[ P(T_{0,i} \u0026lt; T_{1,j} \\mid \\min\\{T_{0,i},T_{1,j}\\} \u0026lt; \\min\\{C_{0,i}, C_{1,j} \\} ) = \\frac{1}{1 + \\theta}. \\] Maybe this was overkill. But it’s settled my mind a little. If someone tells me that the probabilistic index is \\(\\phi=1/(1+\\theta)\\) (in the context of a proportional hazards model applied to a particular trial with administrative censoring), then I can imagine repeatedly selecting two patients at random, one from each arm. Out of the pairs where it is possible to establish who has better survival, the fraction of times that it is the patient from the experimental arm is \\(\\phi\\).\n[As an aside, I much prefer the interpretation \\(S_1(t)=S_0(t)^\\theta\\) as the interpretation of the hazard ratio. Looking at the above derivation, this is the more fundamental relationship. But, then again, there are several interpretations of the hazard ratio, each with pros and cons…would need a separate post.]\n Link with concordance I can try to show an example using the concordance() function from R’s survival package. Firstly, I set up a data frame with some exponentially distributed survival data from a clinical trial. The control arm median is 12 months. The experimental arm median is 18 months. Recruitment is uniform over 12 months. The sample size of 5000 per arm is deliberately oversized for illustration purposes (so the point estimate of the hazard ratio is close to 0.66).\nlibrary(dplyr) library(survival) set.seed(325) df_ex \u0026lt;- data.frame(rec = runif(10000, 0, 12), y_star = c(rexp(5000, rate = log(2) / 12), rexp(5000, rate = log(2) / 18)), delta_star = 1, trt = rep(c(\u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;), each = 5000)) %\u0026gt;% mutate(delta = as.numeric(y_star + rec \u0026lt; 18), y = pmin(y_star, 18 - rec)) head(df_ex) ## rec y_star delta_star trt delta y ## 1 3.4275172 15.893085 1 0 0 14.572483 ## 2 9.3391460 8.595456 1 0 1 8.595456 ## 3 2.1307085 9.754036 1 0 1 9.754036 ## 4 2.1295279 8.575436 1 0 1 8.575436 ## 5 0.6061914 4.822211 1 0 1 4.822211 ## 6 6.3240452 4.126706 1 0 1 4.126706 The data frame contains two versions of the data set. (y_star, delta_star) is the survival time and event status (1=event; 0=censored) for the uncensored data set, assuming an infinite follow-up time. (y, delta) is the survival time and event status, assuming administrative censoring 18 months after the start of the study. I can quickly plot (without tidying up axis labels etc) the Kaplan Meier estimates for the two data sets:\nfit_inf \u0026lt;- survfit(Surv(y_star, delta_star) ~ trt, data = df_ex) fit_18 \u0026lt;- survfit(Surv(y, delta) ~ trt, data = df_ex) plot(fit_inf, col = 1:2) plot(fit_18, col = 1:2) I’ll start with the infinite follow-up data set, and fit a Cox model to confirm that the hazard ratio is essentially 0.66…\ncph_inf \u0026lt;- coxph(Surv(y_star, delta_star) ~ trt, data = df_ex) summary(cph_inf)$coef ## coef exp(coef) se(coef) z Pr(\u0026gt;|z|) ## trt1 -0.4197819 0.6571902 0.02043946 -20.53781 9.891931e-94 I could then estimate the probabilistic index as \\(1/(1+0.66)= 0.6\\).\nNext I calculate the concordance…\ncon_inf \u0026lt;- concordance(cph_inf) con_inf ## Call: ## concordance.coxph(object = cph_inf) ## ## n= 10000 ## Concordance= 0.5505 se= 0.002813 ## concordant discordant tied.x tied.y tied.xy ## 15025449 9974551 24995000 0 0 We see that the concordance is 0.55. So concordance is not the same as the probabilistic index.\nWhat is concordance? See this nice vignette by Thernau \u0026amp; Atkinson. The motivation for concordance usually comes from evaluating the performance of a risk prediction model, rather than quantifying treatment effect. It was proposed in this context by Frank Harrell in this 1982 paper. The idea is that if I randomly select two patients from the entire population, what is the probability that the model prediction agrees with reality in terms of who has the longer survival time? Loosely speaking, concordance is defined as \\[ P(\\hat{T}_i \u0026lt; \\hat{T}_j \\mid T_{i} \u0026lt; T_{j}), \\] and if there were no ties, this would be the same as \\[ P(T_i \u0026lt; T_j \\mid \\hat{T}_i \u0026lt; \\hat{T}_j). \\] It’s just saying that, out of all possible samples of size 2 from the whole population, how often does the actual ordering of events match the predicted ordering. However, to deal with ties, the definition of concordance is typically extended to \\[ \\begin{align} \\text{Concordance}:\u0026amp;=P( \\hat{T}_i \u0026lt; \\hat{T}_j \\mid T_i \u0026lt; T_j, \\hat{T}_i\\neq \\hat{T}_j)P(\\hat{T}_i \\neq \\hat{T}_j\\mid T_i \\neq T_j) + \\frac{1}{2}P(\\hat{T}_i = \\hat{T}_j \\mid T_i \\neq T_j),\\\\ \\end{align} \\] or, equivalently, \\[ P(T_i \u0026lt; T_j \\mid \\hat{T}_i \u0026lt; \\hat{T}_j, T_i \\neq T_j)P(\\hat{T}_i \\neq \\hat{T}_j \\mid T_i \\neq T_j) + \\frac{1}{2}P(\\hat{T}_i = \\hat{T}_j \\mid T_i \\neq T_j). \\]\nThis is just saying that, out of all samples of size 2 from the whole population, if we discard samples with tied events, how often can we correctly predict from the model who will have the longer survival times (where if our model gives us tied predictions then we choose randomly / are correct with probability 0.5)?\nIn our clinical trial context, and for our first simulated data set where there are no ties in \\(T\\) and no censoring, we have \\[ \\begin{align} \\text{Concordance}\u0026amp;=P(T_i \u0026lt; T_j \\mid A_i = 0, A_j = 1)P(A_i \\neq A_j) + \\frac{1}{2}P(A_i = A_j)\\\\ \u0026amp;= \\text{Probabilistic Index } \\times P(A_i \\neq A_j) + \\frac{1}{2}P(A_i = A_j). \\end{align} \\]\nSince \\(P(A_i = A_j)\\) is just telling us the allocation ratio, the Concordance and Probabilistic Index are essentially carrying the same information about the treatment effect. The Probabilistic Index is more natural for describing a treatment effect. Concordance is more natural for describing performance of a risk prediction model.\nGoing back to our example\ncon_inf ## Call: ## concordance.coxph(object = cph_inf) ## ## n= 10000 ## Concordance= 0.5505 se= 0.002813 ## concordant discordant tied.x tied.y tied.xy ## 15025449 9974551 24995000 0 0 the concordance() function has helpfully provided us with the number of concordant, discordant and tied.x pairs. From these, we can get empirical estimates for both the probabilistic index and the concordance. The total number of pairs is choose(10000,2) = 49,995,000. The number of pairs where both patients are on the same treatment arm is 2*choose(5000,2) = 24,995,000. That’s what we see in tied.x. We have 5000 * 5000 = 25,000,000 pairs where one patient comes from each arm. Of these, we see that 15,025,449 are concordant, meaning that the survival of the patient on the experimental arm was longer than the survival of the patient on the control arm, and the rest were discordant. So an empirical estimate for the probabilistic index is 15,025,449 / 25,000,000 = 0.60, which agrees with our theoretical result above. And the empirical estimate for the concordance is\n\\[ \\frac{15,025,449}{25,000,000}\\times\\frac{25,000,000}{49,995,000} + 0.5 \\times \\frac{24,995,000}{49,995,000} = 0.550514, \\] which is exactly what we see.\nNow let’s move on to the censored data set…\ncph_18 \u0026lt;- coxph(Surv(y, delta) ~ trt, data = df_ex) summary(cph_18)$coef ## coef exp(coef) se(coef) z Pr(\u0026gt;|z|) ## trt1 -0.3942244 0.6742028 0.03099125 -12.7205 4.549172e-37 In this case everything looks the same…\ncon_18 \u0026lt;- concordance(cph_18) con_18 ## Call: ## concordance.coxph(object = cph_18) ## ## n= 10000 ## Concordance= 0.5495 se= 0.004008 ## concordant discordant tied.x tied.y tied.xy ## 9023362 6051734 14914532 0 0 except that all pairs where it is impossible to establish for certain which patient had the longer survival time have been removed from consideration, and hence we see lower numbers of concordant, discordant and tied.x pairs. An empirical estimate of the probabilistic index, which remember I defined as \\[P(T_{0,i} \u0026lt; T_{1,j} \\mid \\text{It\u0026#39;s possible to know given censoring})\\] is concordant / (concordant + discordant) = 9023362 / (9023362 + 6051734) = 0.60, mathching the theoretical result.\nFor the concordance, I also need to modify the definition. Let \\(E_{i,j}\\) denote the event “It’s possible to know which of \\(T_i\\) and \\(T_j\\) happens first”. Then\n\\[ \\text{Concordance}:=P(T_i \u0026lt; T_j \\mid \\hat{T}_i \u0026lt; \\hat{T}_j, T_i \\neq T_j,E_{i,j})P(\\hat{T}_i \\neq \\hat{T}_j \\mid T_i \\neq T_j,E_{i,j}) + \\frac{1}{2}P(\\hat{T}_i = \\hat{T}_j \\mid T_i \\neq T_j,E_{i,j}). \\] I can estimate this empirically as\n\\[ \\frac{9023362}{9023362 + 6051734}\\times\\frac{9023362 + 6051734}{9023362 + 6051734 + 14914532} + 0.5 \\times \\frac{14914532}{9023362 + 6051734 + 14914532} = 0.5495443, \\]\nwhich is exactly what we see.\n  Further Topics I wanted to explain a bit more about\n standard errors for concordance. Both for empirical estimates. And using the reationship with the hazard ratio. alternative weighted versions of concordance (see the vignette) a non-proportional hazards example.  But I’ve run out of steam. Maybe I’ll come back to it another time.\n P.S. standard errors Follows (https://cran.r-project.org/web/packages/survival/vignettes/concordance.pdf) closely.\nLet’s take the same example as before, except we reduce the sample size from 10,000 to 400 so that it makes sense to worry about standard errors.\nlibrary(dplyr) library(survival) set.seed(325) df_ex \u0026lt;- data.frame(rec = runif(400, 0, 12), y_star = c(rexp(200, rate = log(2) / 12), rexp(200, rate = log(2) / 18)), delta_star = 1, trt = rep(c(\u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;), each = 200)) %\u0026gt;% mutate(delta = as.numeric(y_star + rec \u0026lt; 18), y = pmin(y_star, 18 - rec)) head(df_ex) ## rec y_star delta_star trt delta y ## 1 3.4275172 39.246498 1 0 0 14.572483 ## 2 9.3391460 25.012807 1 0 0 8.660854 ## 3 2.1307085 1.414864 1 0 1 1.414864 ## 4 2.1295279 12.747489 1 0 1 12.747489 ## 5 0.6061914 17.157924 1 0 1 17.157924 ## 6 6.3240452 10.810010 1 0 1 10.810010 fit_inf \u0026lt;- survfit(Surv(y_star, delta_star) ~ trt, data = df_ex) fit_18 \u0026lt;- survfit(Surv(y, delta) ~ trt, data = df_ex) plot(fit_inf, col = 1:2) plot(fit_18, col = 1:2) Confidence interval for Probabilistic Index using the relationship \\(\\phi = 1 / (1 + \\theta)\\) ## Infinite follow-up cph_inf \u0026lt;- coxph(Surv(y_star, delta_star) ~ trt, data = df_ex) ## Point estimate 1 / (1 + exp(cph_inf$coefficients)) ## trt1 ## 0.5793188 ## CI rev(1 / (1 + exp(confint(cph_inf)))) ## [1] 0.5299843 0.6271182 ## 18 month max follow-up cph_18 \u0026lt;- coxph(Surv(y, delta) ~ trt, data = df_ex) ## Point estimate 1 / (1 + exp(cph_18$coefficients)) ## trt1 ## 0.5730088 ## CI rev(1 / (1 + exp(confint(cph_18)))) ## [1] 0.4958763 0.6467470  Standard error of concordance (empirical) The standard error for the empirical estimator of the concordance can be calculated based on the nice observation by Therneau and Watson that a standardized version of this estimator is equal to the score statistic from a Cox model with a time-dependent covariate \\(n(t_i)r_i(t_i)\\), where \\(n(t_i)\\) and \\(r_i(t_i)\\) are defined as ” the number of subjects still at risk at time t” and “the rank of \\(x_i\\) among all those still at risk at time \\(t\\)”, respectively. I found it quite difficult to get my head around the exact definition of \\(n(t_i)\\) and \\(r_i(t_i)\\), and also to understand why there is this equivalence. To avoid that pain again, I’ll now try to explain in excessive detail…\nStarting with the empirical estimate for concordance:\n\\[ \\tilde{C} = \\frac{\\sum_{\\text{pairs}: E_{i,j}\\cap (T_i \u0026lt; T_j)} I(\\hat{T}_i\u0026lt;\\hat{T}_j)+ 0.5I(\\hat{T}_i\u0026lt;\\hat{T}_j)}{\\sum_{\\text{pairs}: E_{i,j}\\cap (T_i \u0026lt; T_j) } I(\\hat{T}_i\u0026lt;\\hat{T}_j) + I(\\hat{T}_i\u0026gt;\\hat{T}_j) + I(\\hat{T}_i=\\hat{T}_j)}\\]\nwhere \\(E_{i,j}\\) is the event that it’s possible to establish which of \\(T_i\\) and \\(T_j\\) is larger given censoring, we can then rearrange:\n\\[ \\left\\lbrace \\sum_{\\text{pairs}: E_{i,j}\\cap (T_i \u0026lt; T_j)} 1 \\right\\rbrace \\left( 2\\tilde{C}-1 \\right) = \\sum_{\\text{pairs}: E_{i,j}\\cap (T_i \u0026lt; T_j)} I(\\hat{T}_i\u0026lt;\\hat{T}_j) - I(\\hat{T}_i\u0026gt;\\hat{T}_j) =: U.\\]\nThe next step is to recognize that we can sum up the terms in \\(U\\) in a particular order:\n\\[ U = \\sum_i \\sum_{j: E_{i,j} \\cap (T_j \u0026gt; T_i)} \\text{sign}(\\hat{T}_j - \\hat{T}_i)\\] where \\(\\text{sign}(z)\\)=-1,0 or 1, depending if \\(z\\) is \\(\u0026lt;0\\), \\(=0\\) or \\(\u0026gt;0\\).\nFor a pair \\((i,j)\\) to satisfy \\(E_{i,j} \\cap (T_j \u0026gt; T_i)\\), we firstly need the survival time \\(T_i\\) to be uncensored. Also, we need patient \\(j\\) to be in the at-risk set at time \\(T_i\\). This is sufficient to satisfy the condition. So we have\n\\[U = \\sum_i \\delta_i \\sum_{j: j\\neq i,j\\in R(T_i) } \\text{sign}(\\hat{T}_j - \\hat{T}_i),\\]\nwhere \\(\\delta_i\\) is an indicator that patient \\(i\\) has an event, and \\(R\\) denotes the risk set. The same expression is found in Appendix A of Therneau and Watson. At this point, we can introduce the more precise definitions for \\(r_i(t_i)\\) and \\(n(t_i)\\):\n\\[r_i(t_i):= \\frac{1}{n(t_i)}\\sum_{j: j\\neq i,j\\in R(T_i)} \\left[ I(\\hat{T}_i \u0026lt; \\hat{T}_j) + \\frac{1}{2}I(\\hat{T}_i = \\hat{T}_j)\\right], \\] where \\(n(t_i):= |j: j\\neq i,j\\in R(T_i)|\\), i.e., the size of the risk set (excluding \\(i\\) itself). Next (following T\u0026amp;W),\n\\[ \\begin{align} r_i(t_i) \u0026amp;= \\frac{1}{n(t_i)}\\sum_{j: j\\neq i,j\\in R(T_i)} \\left[ I(\\hat{T}_i \u0026lt; \\hat{T}_j) + \\frac{1}{2}I(\\hat{T}_i = \\hat{T}_j)\\right]\\\\ \u0026amp;= \\frac{1}{n(t_i)}\\sum_{j: j\\neq i,j\\in R(T_i)} \\left[ \\text{sign}(\\hat{T}_j - \\hat{T}_i) + 1 \\right] /2\\\\ \u0026amp;= \\frac{1}{2} + \\frac{1}{n(t_i)}\\sum_{j: j\\neq i,j\\in R(T_i)} \\frac{\\text{sign}(\\hat{T}_j - \\hat{T}_i)}{2} \\end{align} \\]\nand therefore\n\\[U = \\sum_i 2\\delta_i n(t_i)\\{r_i(t_i) - 1 / 2\\}\\]\nTo see that this is equivalent to a Cox model, we must further recognize that \\(\\bar{r} = 1/2\\), where \\(\\bar{r}\\) is the average of the \\(r_i\\) across all \\(i\\) at risk at time \\(t_i\\) (this is because the \\(j\\neq i\\) terms cancel each other out), and finally write down the partial likelihood from the Cox model with a time-dependent covariate \\(z(t)\\):\n\\[\\sum_i \\delta_i \\left[ z_i(t_i) - \\frac{\\sum_{j \\in R(t_i)} z_j(t_i) \\exp\\left[ z_j(t_i)\\beta \\right]}{\\exp\\left[ z_j(t_i)\\beta \\right]} \\right] \\mid_{\\beta = 0} = \\sum_i \\delta_i \\left[ z_i(t_i) - \\bar{z}(t_i)\\right],\\]\nobserving the equivalence between this expression and the expression for \\(U\\) when \\(z_i(t_i) =2 n(t_i)r_i(t_i)\\).\nA nice consequence of using the Cox model is that we can write down the variance of this statistic under the null hypothesis (see, e.g., page 128 of https://www4.stat.ncsu.edu/~dzhang2/st745/chap6.pdf):\n\\[ \\sum_i \\delta_i \\left[ \\sum_{j \\in R(t_i)} \\frac{z_j(t_i)^2}{n(t_i)} - \\bar{z}(t_i)^2 \\right].\\] In principle, I think this is enough to be able to calculate the variance for the empirical estimate of concordance for a general Cox model, but I can’t be bothered any more to actually go through a more complicated example (i.e. a Cox model with more than one variable).\nIn the context of just two predictions (e.g. in a simple RCT with \\(A=0\\) and \\(A=1\\)), the score statistic reduces to the Gehan-Wilcoxon statistic (this is also shown in the appendix of https://www.mayo.edu/research/documents/bsi-techreport-85/doc-20433003)\n\\[ U= \\sum_k n_k (d_{1,k} - \\frac{n_{1,k}}{n_1}) \\]\nwith variance\n\\[\\text{var}({U}) = \\sum_kn_{k}^2 \\times\\text{var}\\left(d_{1,k} - \\frac{n_{1,k}}{n_1}\\right)\\].\nAnd from these formulas I can reconstruct the estimate of the variance for our example.\nThe terms \\(\\{2r_i(t_i) - 1\\}\\) are guaranteed to be between \\(-1\\) and \\(1\\), and they are provided in the rank column by survival::concordance, if we explicitly ask for it:\ncon_inf \u0026lt;- concordance(cph_inf, ranks = TRUE) head(con_inf$ranks) ## time rank timewt casewt variance ## 98 0.01382162 0.5012531 399 1 0.2500000 ## 313 0.06709051 -0.5000000 398 1 0.2499984 ## 257 0.09001529 -0.5012594 397 1 0.2500000 ## 40 0.12598446 0.5000000 396 1 0.2499984 ## 395 0.14877580 -0.5012658 395 1 0.2500000 ## 262 0.18041986 -0.5025381 394 1 0.2499984 tail(con_inf$ranks) ## time rank timewt casewt variance ## 332 98.80991 -0.1666667 6 1 0.1224490 ## 348 99.66819 -0.2000000 5 1 0.1388889 ## 164 108.75980 1.0000000 4 1 0.1600000 ## 368 109.71749 0.0000000 3 1 0.0000000 ## 205 111.79013 0.0000000 2 1 0.0000000 ## 396 113.71013 0.0000000 1 1 0.0000000 Next I print out the concordance again…\ncon_inf ## Call: ## concordance.coxph(object = cph_inf, ranks = TRUE) ## ## n= 400 ## Concordance= 0.5331 se= 0.01436 ## concordant discordant tied.x tied.y tied.xy ## 22640 17360 39800 0 0 …and show I can reproduce it from the formula above for \\(U\\) based on \\(r\\) and \\(n\\) (79800 is the total number of pairs)…\n(sum(con_inf$ranks[,\u0026quot;rank\u0026quot;] * con_inf$ranks[,\u0026quot;timewt\u0026quot;]) / 79800 + 1) / 2 ## [1] 0.5330827 …and I can reproduce the variance for the specific case of just two groups\ncon_inf$cvar ## [1] 0.0002092043 (sum((con_inf$ranks[\u0026quot;timewt\u0026quot;] + 1) ^2 * con_inf$ranks[\u0026quot;variance\u0026quot;])) / (79800^2) / 4 ## [1] 0.0002092043  Still to do  jacknife estimate of variance weighted version of concordance non-proportional hazards  I don’t think I’ll get round to this now. Refer to https://www.mayo.edu/research/documents/bsi-techreport-85/doc-20433003\n  References De Neve, Jan, and Thomas A. Gerds. “On the interpretation of the hazard ratio in Cox regression.” Biometrical Journal 62.3 (2020): 742-750.\nHarrell, Frank E., et al. “Evaluating the yield of medical tests.” Jama 247.18 (1982): 2543-2546.\nTherneau, Terry, and Elizabeth Atkinson. “1 The concordance statistic.” (2020).\n ","date":1639440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639494200,"objectID":"f74677dd677401ac1ef06e44081b968c","permalink":"/post/2021-12-14-notes-on-concordance/","publishdate":"2021-12-14T00:00:00Z","relpermalink":"/post/2021-12-14-notes-on-concordance/","section":"post","summary":"Motivation I don’t understand the details of concordance in survival analysis, so I’m trying to explain it to myself.\n Probabilistic index I recently read the very nice paper On the interpretation of the hazard ratio in Cox regression by Jan De Neve and Thomas Gerds.\nSuppose in the context of a clinical trial we use a Cox model to compare an experimental treatment (A=1) with a control treatment (A=0), where (for simplicity here) treatment is the only term in the model (assume the model is correct, by the way).","tags":[],"title":"Notes on concordance","type":"post"},{"authors":[],"categories":["survival analysis","clinical trials"],"content":" This post is to express some minor frustration with some papers I’ve read recently evaluating the performance of restricted mean survival time as a summary measure in oncology studies.\nI should say that I’m not a saint when it comes to designing simulation studies. Consciously and/or unconsciously, it’s tempting to give our favourite methods an easier ride.\nNevertheless, a couple of things bother me, and they’re related to each other. One is the choice of censoring distribution, and the other is the choice of \\(\\tau\\) in the definition of restricted mean survival time (I’ll explain).\nRestricted mean survival time The restricted mean survival time up until month \\(\\tau\\) is the average amount of time patients are alive for out of the first \\(\\tau\\) months:\n\\[\\text{RMST}(\\tau):=E\\left\\lbrace \\text{min}(\\tau, T)\\right\\rbrace\\]\nIt’s sometimes defined as the area under the survival curve \\(\\text{RMST}(\\tau):=\\int_{0}^{\\tau}S(t)dt\\). Is it obvious that that’s the same thing? Maybe it should be, but not to me right now…\n\\[ \\begin{aligned} E\\left\\lbrace \\text{min}(\\tau, T)\\right\\rbrace \u0026amp; = \\int_{0}^{\\tau}tf(t)dt + \\tau\\int_{\\tau}^{\\infty}f(t)dt\\\\ \u0026amp; = \\left[ tF(t) \\right]_0^{\\tau} - \\int_{0}^{\\tau}F(t)dt +\\tau S(\\tau)\\\\ \u0026amp;= \\tau\\left\\lbrace 1 - S(\\tau) \\right\\rbrace - \\int_{0}^{\\tau}\\left\\lbrace 1 - S(t)\\right\\rbrace dt +\\tau S(\\tau) \\\\ \u0026amp;= \\int_{0}^{\\tau}S(t)dt \\end{aligned} \\] As a summary measure for treatment effect, one could use a difference \\(\\text{RMST}_E(\\tau)-\\text{RMST}_C(\\tau)\\), or a ratio \\(\\text{RMST}_E(\\tau)/\\text{RMST}_C(\\tau)\\) contrasting the experimental and control arms. One could perform estimation by plugging in the Kaplan Meier estimates, \\(\\hat{S}_E(t)\\) and \\(\\hat{S}_C(t)\\), for example.\n How to choose \\(\\tau\\)? A very basic consideration is that everything later than \\(\\tau\\) will be ignored. This suggests that taking \\(\\tau\\) as large as possible might generally (not always) be the most powerful strategy.\nCensoring distributions This brings me on to censoring distributions, and comparisons with the log-rank test / Cox model. With the log-rank test, every event contributes to the test statistic. So whatever the choice of \\(\\tau\\), if there are many events after \\(\\tau\\), then this often (not always!) means that RMST will tend to lose power compared to log-rank / Cox. So if you’re conducting a simulation study and you would like to show the benefits of RMST, what’s a good censoring distribution?\nWell, something that looks like this…\nI see this again and again. What this graph is showing is that there is some reasonably high (e.g. 30%) number of patients who’s censoring time is equal to the length of the study. In other words these 30% of patients are recruited immediately upon the study starting. In this situation there’s no problem with pushing \\(\\tau\\) right up to the length of the study, and RMST would use all of the data in the same way as log-rank / Cox.\nThe problem of course is that recruitment never looks like this! If a more realistic censoring distribution were used where recruitment starts slowly and increases gradually, then this would create a problem for pushing \\(\\tau\\) right up to the length of the study, because things would become unstable when only a few patients are at risk. One would be forced to bring \\(\\tau\\) earlier, and one would see power loss (under certain situations) compared to the log-rank test.\n Comparisons based on real trial data Simulation studies can be manipulated to favour one method or another. Why not take a large selection of real studies and see which method performed best? Yes, an excellent idea from Horiguchi and colleagues. And to be fair, not only an idea, they’ve actually gone to the trouble of doing it. I still don’t think the comparison is totally fair though. They define \\(\\tau\\) as “the minimum of the maximum observed times from two groups”. To see what this really means, I took a look at Kaplan-Meier curves from their first study (alphabetically), and drew (very badly) a line where \\(\\tau\\) is. I don’t think anyone could report a difference in RMST at this \\(\\tau\\) and keep a straight face. It’s clearly ridiculous. Regardless of whether or not the asymptotics hold well enough to give a valid test (they probably don’t), you’d simply have to bring \\(\\tau\\) forward.\nIt won’t look this silly for every study of course, but that’s difficult to predict. As a general strategy, I think one would need to be more conservative.\nFrom Agarwala, Sanjiv S., et al. Journal of Clinical Oncology 35.8 (2017): 885. (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5455684/)\n  Conclusions I don’t want to dismiss restricted mean survival time as a useful summary measure. In some situations I’m sure it is a good idea. In some situations a log-rank test / Cox model is a bad idea. All I’m saying is that power comparisons should be a bit fairer, or at least, we should look at the details more closely. Like I said, I’m not immune to this kind of bias.\n ","date":1634256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634301915,"objectID":"c27439e9e20403c837132229e9cca05b","permalink":"/post/trouble-with-tau/","publishdate":"2021-10-15T00:00:00Z","relpermalink":"/post/trouble-with-tau/","section":"post","summary":"This post is to express some minor frustration with some papers I’ve read recently evaluating the performance of restricted mean survival time as a summary measure in oncology studies.\nI should say that I’m not a saint when it comes to designing simulation studies. Consciously and/or unconsciously, it’s tempting to give our favourite methods an easier ride.\nNevertheless, a couple of things bother me, and they’re related to each other.","tags":[],"title":"Trouble with tau","type":"post"},{"authors":[],"categories":[],"content":"  Recently, I’ve become interested in the robustness of standard statistical methods for RCTs under model misspecification. It seems to be a hot topic at the moment. I’d like to believe this is not such a big problem. I don’t think it is a big problem. But I’m trying to challenge my beliefs nevertheless.\nStratified log-rank test The null hypothesis that is generally considered when performing an unstratified log-rank test is:\n\\[H_{0}: S_{E}(t) = S_{C}(t)\\text{ for all }t\u0026gt;0\\] where \\(S_{E}\\) and \\(S_{C}\\) denote the survival distributions on the experimental and control arms, respectively, in the full trial population. Personally, I prefer one-sided testing and a null hypothesis\n\\[\\tilde{H}_{0}: S_{E}(t) \\leq S_{C}(t)\\text{ for all }t\u0026gt;0\\]\nbut this is unconventional (strictly speaking, it’s harder to reject \\(\\tilde{H}_{0}\\) than \\(H_{0}\\), but some might argue that \\(\\tilde{H}_{0}\\) is still too easy to reject and we need a bigger null hypothesis space – a subject for another post). In any case, under some standard assumptions, an unstratified log-rank test controls \\(\\alpha\\) when testing \\(H_{0}\\) or \\(\\tilde{H}_{0}\\).\nThe null hypothesis that is generally considered when performing a stratified log-rank test is:\n\\[H_{0,S}: S_{E,i}(t) = S_{C,i}(t)\\text{ for all }t\u0026gt;0\\text{ and for all strata }i\\] where \\(S_{E,i}\\) and \\(S_{C,i}\\) denote the survival distributions on the experimental and control arms, respectively, in the \\(i\\)th stratum. Again, I would prefer a one-sided version\n\\[\\tilde{H}_{0,S}: S_{E,i}(t) \\leq S_{C,i}(t)\\text{ for all }t\u0026gt;0\\text{ and for all strata }i.\\]\nA stratified log-rank test controls \\(\\alpha\\) when testing \\(H_{0,S}\\) or \\(\\tilde{H}_{0,S}\\), but a question some people might ask is: does a stratified log-rank test control \\(\\alpha\\) when thought of as a test of \\(H_{0}\\) or \\(\\tilde{H}_{0}\\)? (I’m not entirely convinced this is a sensible question, but let’s leave that aside).\nThe answer to the question is no. Below is a (rather extreme) example where the population consists of two equally sized strata. Under the control treatment, the first stratum has much poorer survival than the second stratum. The experimental treatment is beneficial in the first stratum and harmful in the second stratum. However, overall, marginally, the survival distributions on the two arms are practically the same (experimental very fractionally worse).\nNow consider a trial in this population, where 1000 patients are recruited at a uniform rate for 12 months, and patients are subsequently followed-up for a further 12 months, making the total trial length 24 months, at which point all the survival times for all surviving patients are censored.\nI simulated this trial 10,000 times, and the percentage of times that the stratified log-rank test (actually I used a stratified Cox model with treatment as the only covariate – all code below) produced a Z-value for the treatment term that was below the 2.5% quantile of the normal distribution (low values of Z favouring the experimental arm) was 38%. The percentage of times for the unstratified Cox model was 1%.\n Is this a problem? No I don’t think so. Firstly, there is no problem for the null hypotheses \\(H_{0,S}\\) or \\(\\tilde{H}_{0,S}\\). Secondly, this is a very extreme violation of the assumptions of the stratified Cox model. Thirdly, consider the point estimates for the hazard ratios in the two strata in the subset of simulated data sets where the stratified log-rank test rejects and the unstratified log-rank test does not. These are highlighted in blue in the figure below. In these cases we would observe a point estimate between 0.5 and 0.7 in the first stratum, and a point estimate between 1.1 and 1.6 in the second stratum.\nSo I see this as more of a curiosity than a practical problem. It’s interesting to compare with logistic regression, where it’s been shown (here) that a stratified test controls \\(\\alpha\\) not only for the stratified null but also for the overall trial population null (see also here). The same goes for continuous data analyzed by ANCOVA which I think is quite well known.\nAs a final comment, it’s important to keep in mind the efficiency gains from using a stratified test in more typical situations where its assumptions are reasonable.\n Code library(survival) library(dplyr) library(purrr) library(ggplot2) ## survival probability for a 2-piece exponential surv_2 \u0026lt;- function(t, lambda_1, lambda_2, t_star){ p_1 \u0026lt;- exp(-lambda_1 * t) p_2 \u0026lt;- exp(-lambda_1 * t_star) * exp(-lambda_2 * pmax(0, (t - t_star))) (t \u0026lt; t_star) * p_1 + (t \u0026gt;= t_star) * p_2 } ## function to plot the three survival distributions plot_strat \u0026lt;- function(t_end = 24, t_star_1 = 9, t_star_0 = 9, strat_1_rate_c_1 = log(2) / 4, strat_1_rate_c_2 = log(2) / 4, strat_1_rate_e_1 = log(2) / 4 * 0.6, strat_1_rate_e_2 = log(2) / 4 * 0.6, strat_0_rate_c_1 = log(2) / 15, strat_0_rate_c_2 = log(2) / 15, strat_0_rate_e_1 = log(2) / 15 * 1.1, strat_0_rate_e_2 = log(2) / 15 * 1, prop_strat_1 = 0.1){ t_seq \u0026lt;- seq(0, t_end, length.out = 100) s_c_1 \u0026lt;- surv_2(t_seq, lambda_1 = strat_1_rate_c_1, lambda_2 = strat_1_rate_c_2, t_star = t_star_1) s_e_1 \u0026lt;- surv_2(t_seq, lambda_1 = strat_1_rate_e_1, lambda_2 = strat_1_rate_e_2, t_star = t_star_1) s_c_0 \u0026lt;- surv_2(t_seq, lambda_1 = strat_0_rate_c_1, lambda_2 = strat_0_rate_c_2, t_star = t_star_0) s_e_0 \u0026lt;- surv_2(t_seq, lambda_1 = strat_0_rate_e_1, lambda_2 = strat_0_rate_e_2, t_star = t_star_0) par(mfrow = c(1,3)) plot(t_seq, s_c_1, type = \u0026quot;l\u0026quot;, ylim = c(0,1), xlab = \u0026quot;time\u0026quot;, ylab = \u0026quot;Surv\u0026quot;, main = paste(\u0026quot;First stratum (\u0026quot;, prop_strat_1,\u0026quot;)\u0026quot;)) points(t_seq, s_e_1, type = \u0026quot;l\u0026quot;, col = 2) legend(\u0026quot;topright\u0026quot;, c(\u0026quot;Control\u0026quot;, \u0026quot;Experimental\u0026quot;), lty = c(1,1), col = c(1,2)) plot(t_seq, s_c_0, type = \u0026quot;l\u0026quot;, ylim = c(0,1), col = 1, lty = 1, xlab = \u0026quot;time\u0026quot;, ylab = \u0026quot;Surv\u0026quot;, main = paste(\u0026quot;Second stratum (\u0026quot;, 1 - prop_strat_1,\u0026quot;)\u0026quot;)) points(t_seq, s_e_0, type = \u0026quot;l\u0026quot;, col = 2, lty = 1) legend(\u0026quot;topright\u0026quot;, c(\u0026quot;Control\u0026quot;, \u0026quot;Experimental\u0026quot;), lty = c(1,1), col = c(1,2)) plot(t_seq, prop_strat_1 * s_c_1 + (1 - prop_strat_1) * s_c_0, type = \u0026quot;l\u0026quot;, ylim = c(0,1), xlab = \u0026quot;time\u0026quot;, ylab = \u0026quot;Surv\u0026quot;, main = \u0026quot;Overall\u0026quot;) points(t_seq, prop_strat_1 * s_e_1 + (1 - prop_strat_1) * s_e_0, type = \u0026quot;l\u0026quot;, col = 2) legend(\u0026quot;topright\u0026quot;, c(\u0026quot;Control\u0026quot;, \u0026quot;Experimental\u0026quot;), lty = c(1,1), col = c(1,2)) } ## plot the three survival distributions plot_strat(t_end = 24, t_star_1 = 9, t_star_0 = 6, strat_1_rate_c_1 = log(2) / 4, strat_1_rate_c_2 = log(2) / 4, strat_1_rate_e_1 = log(2) / 4 * 0.6, strat_1_rate_e_2 = log(2) / 4 * 0.6, strat_0_rate_c_1 = log(2) / 15, strat_0_rate_c_2 = log(2) / 15, strat_0_rate_e_1 = log(2) / 15 * 2.3, strat_0_rate_e_2 = log(2) / 15 * 0.9, prop_strat_1 = 0.5) ## function for simulating from a piece-wise exponential distribution t_piecewise_exp \u0026lt;- function(n = 10, change_points = c(6, 12), lambdas = c(log(2) / 9, log(2) / 9, log(2) / 9)){ t_lim \u0026lt;- matrix(rep(c(diff(c(0, change_points)), Inf), each = n), nrow = n) t_sep \u0026lt;- do.call(cbind, purrr::map(lambdas, rexp, n = n)) which_cells \u0026lt;- t(apply(t_sep \u0026lt; t_lim, 1, function(x){ rep(c(T,F), c(min(which(x)), length(x) - min(which(x)))) } )) rowSums(pmin(t_sep, t_lim) * which_cells) } ## function to simulate trial data (uncensored) sim_t_uncensored \u0026lt;- function(model, recruitment){ rec_0 \u0026lt;- recruitment$r_period * runif(recruitment$n_0) ^ (1 / recruitment$k) rec_1 \u0026lt;- recruitment$r_period * runif(recruitment$n_1) ^ (1 / recruitment$k) time_0 \u0026lt;- t_piecewise_exp(recruitment$n_0, model$change_points, model$lambdas_0) time_1 \u0026lt;- t_piecewise_exp(recruitment$n_1, model$change_points, model$lambdas_1) data.frame(time = c(time_0, time_1), rec = c(rec_0, rec_1), group = rep(c(\u0026quot;control\u0026quot;, \u0026quot;experimental\u0026quot;), c(recruitment$n_0, recruitment$n_1))) } ## function to apply a data cut off to uncensored data set apply_dco \u0026lt;- function(df, dco = NULL, events = NULL){ if (is.null(dco) \u0026amp;\u0026amp; is.null(events)) stop(\u0026quot;Must specify either dco or events\u0026quot;) df$cal_time \u0026lt;- df$time + df$rec if (is.null(dco)){ dco \u0026lt;- sort(df$cal_time)[events] } df_dco \u0026lt;- df[df$rec \u0026lt; dco, ] df_dco$event \u0026lt;- df_dco$cal_time \u0026lt;= dco df_dco$time \u0026lt;- pmin(df_dco$time, dco - df_dco$rec) df_dco$dco \u0026lt;- dco df_dco } ## function to simulate and analyze a single trial sim_1_trial \u0026lt;- function(dummy, model_1, ## strata 1 model_0, ## strata 0 recruitment_1, recruitment_0){ df_uncensored_1 \u0026lt;- sim_t_uncensored(model_1, recruitment_1) df_uncensored_0 \u0026lt;- sim_t_uncensored(model_0, recruitment_0) df_final_1 \u0026lt;- apply_dco(df_uncensored_1, dco = 24) df_final_0 \u0026lt;- apply_dco(df_uncensored_0, dco = 24) df_final_1$cov \u0026lt;- \u0026quot;1\u0026quot; df_final_0$cov \u0026lt;- \u0026quot;0\u0026quot; df_final \u0026lt;- rbind(df_final_1, df_final_0) cox_str \u0026lt;- summary(coxph(Surv(time, event) ~ group + strata(cov), data = df_final)) cox_unstr \u0026lt;- summary(coxph(Surv(time, event) ~ group, data = df_final)) cox_0 \u0026lt;- summary(coxph(Surv(time, event) ~ group, data = df_final %\u0026gt;% filter(cov == \u0026quot;0\u0026quot;))) cox_1 \u0026lt;- summary(coxph(Surv(time, event) ~ group, data = df_final %\u0026gt;% filter(cov == \u0026quot;1\u0026quot;))) data.frame(z_str = cox_str$coefficients[1,\u0026quot;z\u0026quot;], z_unstr = cox_unstr$coefficients[1,\u0026quot;z\u0026quot;], hr_1 = cox_1$coefficients[1,\u0026quot;exp(coef)\u0026quot;], hr_0 = cox_0$coefficients[1,\u0026quot;exp(coef)\u0026quot;]) } ## specify recruitment recruitment_1 = list(n_0 = 250, n_1 = 250, r_period = 12, k = 1) recruitment_0 = list(n_0 = 250, n_1 = 250, r_period = 12, k = 1) ## specify models model_1 = list(change_points = c(9), lambdas_0 = c(log(2) / 4, log(2) / 4), lambdas_1 = c(log(2) / 4 * 0.6, log(2) / 4 * 0.6)) model_0 = list(change_points = c(6), lambdas_0 = c(log(2) / 15, log(2) / 15), lambdas_1 = c(log(2) / 15 * 2.3, log(2) / 15 * 0.9)) set.seed(352) res \u0026lt;- purrr::map_df(1:10000, sim_1_trial, model_1 = model_1, model_0 = model_0, recruitment_1 = recruitment_1, recruitment_0 = recruitment_0) ## rejections from stratified test mean(res$z_str \u0026lt; qnorm(0.025)) ## [1] 0.3774 ## rejections from unstratified test mean(res$z_unstr \u0026lt; qnorm(0.025)) ## [1] 0.0111 ## plot hazard ratios by strata res \u0026lt;- res %\u0026gt;% mutate(only_stratified_rejects = z_str \u0026lt; qnorm(0.025) \u0026amp; z_unstr \u0026gt; qnorm(0.025)) ggplot(res, aes(x = hr_0, y = hr_1, colour = only_stratified_rejects)) + geom_point() + xlab(\u0026quot;estimated HR second strata\u0026quot;) + ylab(\u0026quot;estimated HR first strata\u0026quot;) + scale_x_continuous(limits = c(0.9, 2.2)) + scale_y_continuous(limits = c(0.4, 0.8))   ","date":1633564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639494394,"objectID":"675f58130dedb0584d81f46c129bce1b","permalink":"/post/2021-10-07-a-stratified-log-rank-test-does-not-control-alpha-under-a-marginal-null-hypothesis-and-that-s-ok/","publishdate":"2021-10-07T00:00:00Z","relpermalink":"/post/2021-10-07-a-stratified-log-rank-test-does-not-control-alpha-under-a-marginal-null-hypothesis-and-that-s-ok/","section":"post","summary":"Recently, I’ve become interested in the robustness of standard statistical methods for RCTs under model misspecification. It seems to be a hot topic at the moment. I’d like to believe this is not such a big problem. I don’t think it is a big problem. But I’m trying to challenge my beliefs nevertheless.\nStratified log-rank test The null hypothesis that is generally considered when performing an unstratified log-rank test is:","tags":[],"title":"A stratified log-rank test does not control alpha under a marginal null hypothesis, and that's ok","type":"post"},{"authors":[],"categories":[],"content":" If you’re interested in doing multiple imputation in R, it’s best to use a specialist package. There are many good options out there, including mice (https://www.jstatsoft.org/article/view/v045i03), smcfcs (https://cran.r-project.org/web/packages/smcfcs/vignettes/smcfcs-vignette.html), norm2 (https://usermanual.wiki/Document/norm2UserGuide.911613350/view), and many others (https://cran.r-project.org/web/views/MissingData.html).\nThe aim of this post is for me to deepen my understanding of multiple imputation and not forget it. More specifically, I want a middle way which is somewhere between a black box command (leaves me unsatisfied) and full gory details (ahhhhh!).\nIn terms of the methodological approach of using multiple imputation for sensitivity analysis, I’ll follow the excellent paper by Cro et al. (https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8569). The paper comes with a Stata package (https://journals.sagepub.com/doi/pdf/10.1177/1536867X1601600211) which was based on original SAS Macros by James Roger (https://www.lshtm.ac.uk/research/centres-projects-groups/missing-data#dia-missing-data).\nThe key tool I’ll be using in R is the brms package (https://cran.r-project.org/web/packages/brms/index.html). The brilliant thing about this is that I can understand each step conceptually, without knowing too much about the details.\nThis is probably an inefficient way to do this though. There could well be errors too.\nlibrary(tidyverse) library(brms) Data set I’ll use the “low dropout” data set from Mallinckrodt et al. (https://journals.sagepub.com/doi/pdf/10.1177/2168479013501310) which is available via https://www.lshtm.ac.uk/research/centres-projects-groups/missing-data#dia-missing-data.\nlow_data \u0026lt;- haven::read_sas(\u0026quot;low1.sas7bdat\u0026quot;) head(low_data) ## # A tibble: 6 x 6 ## PATIENT POOLINV trt basval week change ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1005 101 2 16 1 -3 ## 2 1005 101 2 16 2 -5 ## 3 1005 101 2 16 4 -10 ## 4 1005 101 2 16 6 -11 ## 5 1005 101 2 16 8 -13 ## 6 1006 101 2 17 1 -1 This data set is in long format. It consists of patient ID, treatment (1 or 2), treatment centre (POOLINV), baseline value, timepoint (week), and change from baseline (the outcome variable).\nI won’t describe the context here, or do any exploratory analysis – see the Mallinckrodt et al paper for this (https://journals.sagepub.com/doi/pdf/10.1177/2168479013501310).\nFor my analysis, I’ll first convert it to a wide format, with a separate variable for the outcome at each timepoint (week 1, 2, 4, 6 and 8).\nlow_data_wide \u0026lt;- low_data %\u0026gt;% pivot_wider(names_from = week, names_prefix = \u0026quot;week\u0026quot;, values_from = change) head(low_data_wide) ## # A tibble: 6 x 9 ## PATIENT POOLINV trt basval week1 week2 week4 week6 week8 ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1005 101 2 16 -3 -5 -10 -11 -13 ## 2 1006 101 2 17 -1 -2 -6 -10 -12 ## 3 1008 101 1 32 -6 -12 -17 -20 -22 ## 4 1011 101 1 18 -1 -5 -8 NA NA ## 5 1012 101 1 22 -6 -9 -13 -16 -17 ## 6 1015 101 2 29 -6 -14 -14 -20 -25 We can see now that there are some missing values.\nAt this point I’ll also split the data set by treatment group. This is because I’ll use a separate imputation model for each treatment.\nlow_data_1 \u0026lt;- low_data_wide %\u0026gt;% dplyr::filter(trt == \u0026quot;1\u0026quot;) low_data_2 \u0026lt;- low_data_wide %\u0026gt;% dplyr::filter(trt == \u0026quot;2\u0026quot;) The next step is to fit the imputation model, which is basically a Bayesian MMRM, using brms. The covariarates in each model are baseline (basval) and centre (POOLINV), which both get crossed with timepoint. And the covariance matrices are unstructured.\nfit_1 \u0026lt;- brm(data = low_data_1, family = gaussian, bf(mvbind(week1, week2, week4, week6, week8) | mi() ~ basval + POOLINV) + set_rescor(TRUE), refresh = 0) fit_2 \u0026lt;- brm(data = low_data_2, family = gaussian, bf(mvbind(week1, week2, week4, week6, week8) | mi() ~ basval + POOLINV) + set_rescor(TRUE), refresh = 0) We can take a quick look at the summary of the first model fit to reassure ourselves it is giving us the right kind of thing.\nsummary(fit_1) ## Family: MV(gaussian, gaussian, gaussian, gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## mu = identity; sigma = identity ## mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: week1 | mi() ~ basval + POOLINV ## week2 | mi() ~ basval + POOLINV ## week4 | mi() ~ basval + POOLINV ## week6 | mi() ~ basval + POOLINV ## week8 | mi() ~ basval + POOLINV ## Data: low_data_1 (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## week1_Intercept 1.60 1.57 -1.51 4.71 1.00 4000 ## week2_Intercept -2.20 2.35 -6.71 2.43 1.00 3363 ## week4_Intercept -0.25 2.56 -5.21 4.67 1.00 2839 ## week6_Intercept 0.25 2.94 -5.53 5.99 1.00 2658 ## week8_Intercept 5.11 3.18 -1.24 11.45 1.00 3234 ## week1_basval -0.19 0.07 -0.33 -0.05 1.00 4603 ## week1_POOLINV121 -0.08 0.76 -1.60 1.41 1.00 3794 ## week1_POOLINV131 0.09 0.65 -1.19 1.42 1.00 3684 ## week1_POOLINV141 0.78 0.69 -0.60 2.10 1.00 3695 ## week2_basval -0.13 0.10 -0.33 0.07 1.00 4158 ## week2_POOLINV121 1.12 1.14 -1.17 3.29 1.00 2923 ## week2_POOLINV131 -0.89 0.99 -2.79 1.07 1.00 2865 ## week2_POOLINV141 0.59 1.03 -1.41 2.61 1.00 3027 ## week4_basval -0.34 0.11 -0.56 -0.12 1.00 3469 ## week4_POOLINV121 0.82 1.22 -1.56 3.15 1.00 2511 ## week4_POOLINV131 -1.74 1.08 -3.84 0.40 1.00 2620 ## week4_POOLINV141 0.24 1.13 -1.97 2.42 1.00 2610 ## week6_basval -0.45 0.13 -0.70 -0.19 1.00 3247 ## week6_POOLINV121 0.29 1.42 -2.48 3.11 1.00 2632 ## week6_POOLINV131 -0.75 1.24 -3.17 1.66 1.00 2343 ## week6_POOLINV141 -0.25 1.27 -2.64 2.26 1.00 2458 ## week8_basval -0.70 0.14 -0.97 -0.42 1.00 3638 ## week8_POOLINV121 -0.50 1.56 -3.53 2.56 1.00 3161 ## week8_POOLINV131 -2.67 1.36 -5.25 -0.00 1.00 2597 ## week8_POOLINV141 -1.22 1.39 -3.84 1.49 1.00 2932 ## Tail_ESS ## week1_Intercept 3397 ## week2_Intercept 2979 ## week4_Intercept 2889 ## week6_Intercept 2744 ## week8_Intercept 3078 ## week1_basval 3552 ## week1_POOLINV121 3029 ## week1_POOLINV131 3199 ## week1_POOLINV141 3162 ## week2_basval 3157 ## week2_POOLINV121 3272 ## week2_POOLINV131 3013 ## week2_POOLINV141 3075 ## week4_basval 3213 ## week4_POOLINV121 2567 ## week4_POOLINV131 3273 ## week4_POOLINV141 2821 ## week6_basval 2879 ## week6_POOLINV121 2651 ## week6_POOLINV131 2558 ## week6_POOLINV141 2869 ## week8_basval 3229 ## week8_POOLINV121 2997 ## week8_POOLINV131 2840 ## week8_POOLINV141 3062 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_week1 2.50 0.18 2.18 2.89 1.00 5461 3139 ## sigma_week2 3.73 0.27 3.25 4.30 1.00 3838 3248 ## sigma_week4 4.05 0.29 3.54 4.66 1.00 3645 3259 ## sigma_week6 4.53 0.34 3.93 5.26 1.00 3573 2732 ## sigma_week8 4.88 0.38 4.19 5.71 1.00 4169 3132 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## rescor(week1,week2) 0.53 0.07 0.38 0.67 1.00 3978 ## rescor(week1,week4) 0.42 0.08 0.24 0.56 1.00 3800 ## rescor(week2,week4) 0.71 0.05 0.60 0.80 1.00 3129 ## rescor(week1,week6) 0.32 0.09 0.14 0.49 1.00 3557 ## rescor(week2,week6) 0.53 0.07 0.37 0.66 1.00 3500 ## rescor(week4,week6) 0.76 0.04 0.67 0.84 1.00 3506 ## rescor(week1,week8) 0.17 0.10 -0.03 0.36 1.00 3875 ## rescor(week2,week8) 0.33 0.09 0.14 0.50 1.00 3791 ## rescor(week4,week8) 0.55 0.07 0.39 0.68 1.00 3528 ## rescor(week6,week8) 0.85 0.03 0.78 0.90 1.00 3724 ## Tail_ESS ## rescor(week1,week2) 3313 ## rescor(week1,week4) 3299 ## rescor(week2,week4) 3356 ## rescor(week1,week6) 3151 ## rescor(week2,week6) 2833 ## rescor(week4,week6) 2767 ## rescor(week1,week8) 3453 ## rescor(week2,week8) 3145 ## rescor(week4,week8) 3300 ## rescor(week6,week8) 3391 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Next, I’ll extract the posterior samples from the MCMC.\np_1 \u0026lt;- posterior_samples(fit_1) dim(p_1) ## [1] 4000 58 We can see that we have 4000 samples from 58 “parameters”. Actually, only the first 40 are what I would think of as model parameters, e.g., (printing the head of columns 1,2,3)\nhead(p_1[,c(1,2,3)]) ## b_week1_Intercept b_week2_Intercept b_week4_Intercept ## 1 0.5154750 -5.1858678 -3.870133 ## 2 1.6749945 -3.3279521 -2.459063 ## 3 3.1583047 -0.7224206 4.742484 ## 4 3.4688916 1.9246520 2.719808 ## 5 4.2199799 -0.7346923 5.604683 ## 6 0.4128793 -4.0833588 -2.098488 the rest are samples from the predictive distribution of the missing data, e.g., (printing the head of columns 51,52,53)\nhead(p_1[,c(51,52,53)]) ## Ymi_week8[9] Ymi_week8[18] Ymi_week8[20] ## 1 -4.059760 -13.807886 -5.103871 ## 2 -3.287829 -6.579197 -10.596638 ## 3 -2.680057 1.211975 -10.349368 ## 4 -1.487953 -8.258051 -5.253324 ## 5 -4.687085 -11.235663 -10.320470 ## 6 -1.184016 -5.524158 -5.053344 We see this corresponds to missing data on the week 8 variable in rows 9,18, and 20, respectively, of the original data set (low_data_1).\nThat’s the beautiful thing about MCMC. I can sample from the predictive distribution of the missing data “for free”. To create mulitple imputed data sets, all I need to do is extract samples from the MCMC chain and use them to fill in the missing data in the original data set.\nTo actually do this, I need to introduce a small piece of ugly code (sorry!). Essentially, I’m extracting the location of the missing data in the original data set via the column names of the MCMC object.\nI’m creating 5 imputed data sets because that’s all I’m prepared to type (one could easily take more and create a function to avoid typing). As I have 4000 (correlated) samples from the posterior, and need to create 5 imputed data sets, I’m picking out rows 800, 1600, 2400, 3200 and 4000.\n### create 5 copies of the data set (TRT 1), plus original (labelled 0). low_data_1_0 \u0026lt;- low_data_1 %\u0026gt;% mutate(.id = PATIENT, .imp = 0) low_data_1_1 \u0026lt;- low_data_1 %\u0026gt;% mutate(.id = PATIENT, .imp = 1) low_data_1_2 \u0026lt;- low_data_1 %\u0026gt;% mutate(.id = PATIENT, .imp = 2) low_data_1_3 \u0026lt;- low_data_1 %\u0026gt;% mutate(.id = PATIENT, .imp = 3) low_data_1_4 \u0026lt;- low_data_1 %\u0026gt;% mutate(.id = PATIENT, .imp = 4) low_data_1_5 \u0026lt;- low_data_1 %\u0026gt;% mutate(.id = PATIENT, .imp = 5) ### extract the positions of NAs in data set that need to be replaced. s_1_1 \u0026lt;- str_split(names(p_1), \u0026quot;Ymi_\u0026quot;, simplify = TRUE) s_2_1 \u0026lt;- str_split(s_1_1[,2], \u0026quot;\\\\[\u0026quot;, simplify = TRUE) col_i_1 \u0026lt;- s_2_1[,1] row_i_1 \u0026lt;- as.numeric(str_split(s_2_1[,2], \u0026quot;\\\\]\u0026quot;, simplify = TRUE)[,1]) ### replace NAs with values from MCMC chain for (i in seq_along(col_i_1)){ if(col_i_1[i] != \u0026quot;\u0026quot;){ low_data_1_1[row_i_1[i], col_i_1[i]] \u0026lt;- as.numeric(p_1[800, i]) low_data_1_2[row_i_1[i], col_i_1[i]] \u0026lt;- as.numeric(p_1[1600, i]) low_data_1_3[row_i_1[i], col_i_1[i]] \u0026lt;- as.numeric(p_1[2400, i]) low_data_1_4[row_i_1[i], col_i_1[i]] \u0026lt;- as.numeric(p_1[3200, i]) low_data_1_5[row_i_1[i], col_i_1[i]] \u0026lt;- as.numeric(p_1[4000, i]) } } If we look at the first two rows of the original data set, followed by the first two rows of the first imputed data set, we can see that the missing values have been filled in.\nhead(low_data_1_0, 2) ## # A tibble: 2 x 11 ## PATIENT POOLINV trt basval week1 week2 week4 week6 week8 .id .imp ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1008 101 1 32 -6 -12 -17 -20 -22 1008 0 ## 2 1011 101 1 18 -1 -5 -8 NA NA 1011 0 head(low_data_1_1, 2) ## # A tibble: 2 x 11 ## PATIENT POOLINV trt basval week1 week2 week4 week6 week8 .id .imp ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1008 101 1 32 -6 -12 -17 -20 -22 1008 1 ## 2 1011 101 1 18 -1 -5 -8 -15.4 -17.4 1011 1 I now need to do exactly the same thing for the treatment 2 data set:\np_2 \u0026lt;- posterior_samples(fit_2) ### create 5 copies of the data set (TRT 1), plus original (labelled 0). low_data_2_0 \u0026lt;- low_data_2 %\u0026gt;% mutate(.id = PATIENT, .imp = 0) low_data_2_1 \u0026lt;- low_data_2 %\u0026gt;% mutate(.id = PATIENT, .imp = 1) low_data_2_2 \u0026lt;- low_data_2 %\u0026gt;% mutate(.id = PATIENT, .imp = 2) low_data_2_3 \u0026lt;- low_data_2 %\u0026gt;% mutate(.id = PATIENT, .imp = 3) low_data_2_4 \u0026lt;- low_data_2 %\u0026gt;% mutate(.id = PATIENT, .imp = 4) low_data_2_5 \u0026lt;- low_data_2 %\u0026gt;% mutate(.id = PATIENT, .imp = 5) ### extract the positions of NAs in data set that need to be replaced. s_1_2 \u0026lt;- str_split(names(p_2), \u0026quot;Ymi_\u0026quot;, simplify = TRUE) s_2_2 \u0026lt;- str_split(s_1_2[,2], \u0026quot;\\\\[\u0026quot;, simplify = TRUE) col_i_2 \u0026lt;- s_2_2[,1] row_i_2 \u0026lt;- as.numeric(str_split(s_2_2[,2], \u0026quot;\\\\]\u0026quot;, simplify = TRUE)[,1]) ### replace NAs with values from MCMC chain for (i in seq_along(col_i_2)){ if(col_i_2[i] != \u0026quot;\u0026quot;){ low_data_2_1[row_i_2[i], col_i_2[i]] \u0026lt;- as.numeric(p_2[800, i]) low_data_2_2[row_i_2[i], col_i_2[i]] \u0026lt;- as.numeric(p_2[1600, i]) low_data_2_3[row_i_2[i], col_i_2[i]] \u0026lt;- as.numeric(p_2[2400, i]) low_data_2_4[row_i_2[i], col_i_2[i]] \u0026lt;- as.numeric(p_2[3200, i]) low_data_2_5[row_i_2[i], col_i_2[i]] \u0026lt;- as.numeric(p_2[4000, i]) } } and then I can stack all of the imputed data sets together. Note that I created a label .imp for each imputed data set.\nlow_data_mi \u0026lt;- rbind(low_data_1_0, low_data_2_0, low_data_1_1, low_data_2_1, low_data_1_2, low_data_2_2, low_data_1_3, low_data_2_3, low_data_1_4, low_data_2_4, low_data_1_5, low_data_2_5) The final step is to fit an analysis model (ANCOVA: week 8 vs treatment, baseline and centre) to each imputed data set, and then combine the results using Rubin’s Rules (https://bookdown.org/mwheymans/bookmi/rubins-rules.html). I claimed in the title that I wouldn’t use a specialist multiple imputation package, but I’ll borrow some code from mice for this step (that’s why I created the .id and .imp columns above – mice recognizes these):\nlibrary(mice) fit_mi \u0026lt;- with(as.mids(low_data_mi), lm(week8 ~ trt + POOLINV + basval)) summary(pool(fit_mi)) ## estimate std.error statistic df p.value ## (Intercept) 1.7477586 1.89808520 0.9208009 178.3657 3.583981e-01 ## trt2 -1.8635641 0.72553680 -2.5685315 103.8203 1.163510e-02 ## POOLINV121 -0.6095234 1.07376910 -0.5676484 153.3716 5.711037e-01 ## POOLINV131 -2.1373912 0.94122937 -2.2708505 126.8167 2.484221e-02 ## POOLINV141 -0.1464228 0.96397585 -0.1518946 114.2162 8.795380e-01 ## basval -0.5532151 0.08369395 -6.6099764 186.9722 3.901515e-10 We can see that the estimated treatment effect is around -1.8 with standard error around 0.7. Comparing this with Table 4 in the Mallinckrodt et al paper, we can see that it’s in the right ballpark, bearing in mind slight differences in the model and the low number of imputations.\nUp until now I’ve only performed imputations under a missing-at-random assumption. Given that we could just fit an MMRM model directly, there wasn’t much point in going to the trouble of multiple imputation. However, the advantage of multiple imputation is that it’s easy to perform certain types of sensitivity analysis.\nFor example, to perform \\(\\delta\\)-based sensitivity analysis (see Cro et al.), I may, as an example, add a fixed amount \\(\\delta = 2\\) to the imputations on the experimental treatment arm. To do this, I’ll literally copy-paste by code above, but adding delta:\ndelta \u0026lt;- 2 ### replace NAs with values from MCMC chain for (i in seq_along(col_i_2)){ if(col_i_2[i] != \u0026quot;\u0026quot;){ low_data_2_1[row_i_2[i], col_i_2[i]] \u0026lt;- as.numeric(p_2[800, i]) + delta low_data_2_2[row_i_2[i], col_i_2[i]] \u0026lt;- as.numeric(p_2[1600, i]) + delta low_data_2_3[row_i_2[i], col_i_2[i]] \u0026lt;- as.numeric(p_2[2400, i]) + delta low_data_2_4[row_i_2[i], col_i_2[i]] \u0026lt;- as.numeric(p_2[3200, i]) + delta low_data_2_5[row_i_2[i], col_i_2[i]] \u0026lt;- as.numeric(p_2[4000, i]) + delta } } low_data_mi \u0026lt;- rbind(low_data_1_0, low_data_2_0, low_data_1_1, low_data_2_1, low_data_1_2, low_data_2_2, low_data_1_3, low_data_2_3, low_data_1_4, low_data_2_4, low_data_1_5, low_data_2_5) fit_mi \u0026lt;- with(as.mids(low_data_mi), lm(week8 ~ trt + POOLINV + basval)) summary(pool(fit_mi)) ## estimate std.error statistic df p.value ## (Intercept) 1.3996455 1.90747537 0.73376861 178.5648 4.640521e-01 ## trt2 -1.7184241 0.72878966 -2.35791501 104.6129 2.023692e-02 ## POOLINV121 -0.6247259 1.07890860 -0.57903510 153.8860 5.634121e-01 ## POOLINV131 -2.1714417 0.94558853 -2.29639171 127.5395 2.328443e-02 ## POOLINV141 -0.0533222 0.96836491 -0.05506416 114.9899 9.561829e-01 ## basval -0.5368745 0.08411448 -6.38266384 187.0423 1.336286e-09 We can see that this reduces the treatment effect somewhat. The standard error has not changed much.\nAn alternative to \\(\\delta\\)-based sensitivity analysis is reference-based sensitivity analysis, where more qualitative types of assumption are made about the missing data. See Cro et al. for a disscussion of pros and cons of the two approaches. My very personal opinion is that I prefer the \\(\\delta\\)-based approach. But maybe that’s influenced by its computational simplicity. There’s no way I could do an exercise like this for a reference-based approach in 100 lines of code.\n References Mallinckrodt, Craig, et al. “Recent developments in the prevention and treatment of missing data.” Therapeutic innovation \u0026amp; regulatory science 48.1 (2014): 68-80.\nCro, Suzie, et al. “Sensitivity analysis for clinical trials with missing continuous outcome data using controlled multiple imputation: a practical guide.” Statistics in medicine 39.21 (2020): 2815-2842.\nCro, Suzie, et al. “Reference-based sensitivity analysis via multiple imputation for longitudinal trials with protocol deviation.” The Stata Journal 16.2 (2016): 443-463.\nBürkner, Paul-Christian. “brms: An R package for Bayesian multilevel models using Stan.” Journal of statistical software 80.1 (2017): 1-28.\n Helpful links https://thestatsgeek.com/category/missing-data/\nhttps://discourse.mc-stan.org/t/unstructured-error-covariance-matrix-in-a-multilevel-growth-model/21792/4\nhttps://www.lshtm.ac.uk/research/centres-projects-groups/missing-data#dia-missing-data\n ","date":1630972800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631037846,"objectID":"3a51305530d963fc79b7ba21a4b75a1d","permalink":"/post/multiple-imputation-without-a-specialist-r-package/","publishdate":"2021-09-07T00:00:00Z","relpermalink":"/post/multiple-imputation-without-a-specialist-r-package/","section":"post","summary":"If you’re interested in doing multiple imputation in R, it’s best to use a specialist package. There are many good options out there, including mice (https://www.jstatsoft.org/article/view/v045i03), smcfcs (https://cran.r-project.org/web/packages/smcfcs/vignettes/smcfcs-vignette.html), norm2 (https://usermanual.wiki/Document/norm2UserGuide.911613350/view), and many others (https://cran.r-project.org/web/views/MissingData.html).\nThe aim of this post is for me to deepen my understanding of multiple imputation and not forget it. More specifically, I want a middle way which is somewhere between a black box command (leaves me unsatisfied) and full gory details (ahhhhh!","tags":[],"title":"Multiple imputation without a specialist R package","type":"post"},{"authors":[],"categories":["survival analysis","clinical trials"],"content":" The aim of this post is to demonstrate a landmark/milestone analysis of RCT time-to-event data with a Royston-Parmar flexible parametric survival model. The original reference is:\nRoyston P, Parmar M (2002). “Flexible Parametric Proportional-Hazards and Proportional-Odds Models for Censored Survival Data, with Application to Prognostic Modelling and Estimation of Treatment Effects.” Statistics in Medicine, 21(1), 2175–2197. doi:10.1002/ sim.1203\nThis model has been expertly coded and documented by Chris Jackson in the R package flexsurv (https://www.jstatsoft.org/article/view/v070i08). In this post I’ll be making a big meal out of the same material in an effort to increase my own understanding.\nI need a dataset, so I’ll re-use one from a previous blogpost – this is publically available data from the OAK RCT comparing a checkpoint inhibitor (MPDL3280A) versus chemotherapy (Docetaxel).\nlibrary(survival) library(dplyr) library(flexsurv) dat_oak \u0026lt;- readxl::read_excel(\u0026quot;41591_2018_134_MOESM3_ESM.xlsx\u0026quot;, sheet = 3) %\u0026gt;% select(PtID, ECOGGR, OS, OS.CNSR, TRT01P) %\u0026gt;% mutate(OS.EVENT = -1 * (OS.CNSR - 1)) head(dat_oak) ## # A tibble: 6 x 6 ## PtID ECOGGR OS OS.CNSR TRT01P OS.EVENT ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 318 1 5.19 0 Docetaxel 1 ## 2 1088 0 4.83 0 Docetaxel 1 ## 3 345 1 1.94 0 Docetaxel 1 ## 4 588 0 24.6 1 Docetaxel 0 ## 5 306 1 5.98 0 MPDL3280A 1 ## 6 718 1 19.2 1 Docetaxel 0 ECOG grade is a prognostic covariate. Initially I’m going to ignore this covariate, but I’ll come back to it later in the post.\nI’ll start by plotting the survival curves using the survminer package (https://cran.r-project.org/web/packages/survminer/index.html)…\nkm_oak \u0026lt;- survfit(Surv(OS, OS.EVENT) ~ TRT01P, data = dat_oak) survminer::ggsurvplot(km_oak, data = dat_oak, risk.table = TRUE, break.x.by = 6, legend.title = \u0026quot;\u0026quot;, xlab = \u0026quot;Time (months)\u0026quot;, ylab = \u0026quot;Overall survival\u0026quot;, risk.table.fontsize = 4, legend = c(0.8,0.8)) In another previous post I discussed using flexsurv::flexsurvreg to fit a Weibull model. In brief, we can describe this model as two parallel straight lines on the log cumumlative hazard scale (versus log time).\nHere, I’ll fit a Weibull model to the OAK data and plot the log cumulative hazard versus log time, on top of the non-parametric equivalent. Note that I’m creating functions here because I’ll want to make this graph repeatedly as we add flexibility to the model.\n########################### ### Non-parametric analysis ########################### # Vector of time in months (for plotting): t_seq \u0026lt;- seq(1, 24, length.out = 100) # Extract km estimates km_sum \u0026lt;- summary(km_oak, times = t_seq, extend = TRUE) # Function to plot km estimates (log-cumhaz scale) plot_non_param \u0026lt;- function(){ plot(log(t_seq), log(km_sum$cumhaz[1:100]), type = \u0026quot;l\u0026quot;, xlab = \u0026quot;log(time)\u0026quot;, ylab = \u0026quot;log cumulative hazard\u0026quot;) points(log(t_seq), log(km_sum$cumhaz[101:200]), type = \u0026quot;l\u0026quot;, col = 2) legend(\u0026quot;bottomright\u0026quot;, c(\u0026quot;Docetaxel\u0026quot;, \u0026quot;MPDL3280A\u0026quot;), lty = c(1,1), col = 1:2) } ######################### ### Weibull regression ######################### fit_weibull \u0026lt;- flexsurvreg(Surv(OS, OS.EVENT) ~ TRT01P, data = dat_oak, dist = \u0026quot;weibull\u0026quot;) #Function to add parametric estimate of log-cumulative hazard #to the KM-based plot add_model_estimate \u0026lt;- function(fit){ ### Add Docetaxel estimate summary(fit, newdata = data.frame(TRT01P = \u0026quot;Docetaxel\u0026quot;), t = t_seq, type = \u0026quot;cumhaz\u0026quot;)[[1]][,\u0026quot;est\u0026quot;] %\u0026gt;% log() %\u0026gt;% points(x = log(t_seq), type = \u0026quot;l\u0026quot;, lty = 2) ### Add MPDL3280A estimate summary(fit, newdata = data.frame(TRT01P = \u0026quot;MPDL3280A\u0026quot;), t = t_seq, type = \u0026quot;cumhaz\u0026quot;)[[1]][,\u0026quot;est\u0026quot;] %\u0026gt;% log() %\u0026gt;% points(x = log(t_seq), type = \u0026quot;l\u0026quot;, lty = 2, col = 2) } # plot Weibull regression on top of KM estimates plot_non_param() add_model_estimate(fit_weibull) We can see this is not a good fit at early timepoints. Parallel lines on this scale is the same as saying the hazards are proportional, which is clearly not the case. We therefore need to relax the proportional hazards assumption. One option is to maintain the log-cumulative-hazard-versus-log-time perspective, also maintain straight lines on this scale, but give each treatment its own slope. In other words, this is a Weibull model but with a different shape parameter per treatment. One way I could achieve this is to split the data set and fit two separate models. I won’t do this though. Instead I’ll use the flexsurv::flexsurvspline function. First I’m just going do it. Later I’ll explain the model and syntax.\n# spline model with 0 internal knots and where each treatment has its # own intercept (gamma0) and its own slope (gamma1) fit_spline_0_1 \u0026lt;- flexsurvspline(Surv(OS, OS.EVENT) ~ TRT01P + gamma1(TRT01P), data = dat_oak, k = 0, scale = \u0026quot;hazard\u0026quot;) plot_non_param() add_model_estimate(fit_spline_0_1) This perhaps looks a bit better, but still not great. At this point we might be tempted to introduce more complex splines. That is, we move away from modelling the log-cumulative hazard versus log time as straight lines. In my previous post I talked about this, but still in the context of a proportional hazards model. That is, allowing curved lines, but proportional. For completeness, this went something like this:\n# spline model with 1 internal knot and where each treatment has its # own intercept (gamma0), but common slope (gamma1) and common gamma2 -- I\u0026#39;ll explain later. fit_spline_1_0 \u0026lt;- flexsurvspline(Surv(OS, OS.EVENT) ~ TRT01P, data = dat_oak, k = 1, scale = \u0026quot;hazard\u0026quot;) plot_non_param() add_model_estimate(fit_spline_1_0) As mentioned, this doesn’t capture the non-proportional hazards. We need something a bit more flexible…\n# spline model with 1 internal knot and where each treatment has its # own intercept (gamma0), its own slope (gamma1), but common gamma2 -- I\u0026#39;ll explain later. fit_spline_1_1 \u0026lt;- flexsurvspline(Surv(OS, OS.EVENT) ~ TRT01P + gamma1(TRT01P), data = dat_oak, k = 1, scale = \u0026quot;hazard\u0026quot;) plot_non_param() add_model_estimate(fit_spline_1_1) This is starting to look better. Now to expain a bit more what’s going on. Let’s look at the maximum likelihood estimates from the last model:\nfit_spline_1_1$res ## est L95% U95% se ## gamma0 -3.12264755 -3.44183503 -2.80346006 0.16285375 ## gamma1 1.73312765 1.33785579 2.12839950 0.20167302 ## gamma2 0.03389207 0.01200609 0.05577806 0.01116653 ## TRT01PMPDL3280A -0.10551890 -0.56579074 0.35475295 0.23483689 ## gamma1(TRT01PMPDL3280A) -0.07747115 -0.23538127 0.08043896 0.08056787 The corresponding model is\n\\[\\log H(t;~Docetaxel) = \\gamma_0 + \\gamma_1 \\times \\log(t) + \\gamma_2 \\times f(\\log(t))\\] and\n\\[\\log H(t;~MPDL3280A) = \\gamma_0 + \\text{`TRT01PMPDL3280A`} + \\gamma_1 \\times \\log(t) + \\text{`gamma1(TRT01PMPDL3280A)`} \\times \\log(t) + \\gamma_2 \\times f(\\log(t))\\].\nNow the question is: what is \\(f(\\log(t))\\)?\nThe best way to answer that is probably to plot it, together with the other two basis functions (constant and linear) in this model:\nmy_basis \u0026lt;- flexsurv:::basis(knots = fit_spline_1_1$knots, log(1:24)) my_gamma \u0026lt;- fit_spline_1_1$res[c(\u0026quot;gamma0\u0026quot;, \u0026quot;gamma1\u0026quot;, \u0026quot;gamma2\u0026quot;), \u0026quot;est\u0026quot;] plot(log(1:24),colSums(t(my_basis) * c(0,0,1)) / -50, main = \u0026quot;Basis funs (re-scaled to fit on same plot)\u0026quot;, xlab = \u0026quot;log(time)\u0026quot;, ylab = \u0026quot;Arbitrary scale\u0026quot;, type = \u0026quot;l\u0026quot;, ylim = c(-0.5, 1.5)) points(log(1:24),colSums(t(my_basis) * c(0,1,0)) / 3, type = \u0026quot;l\u0026quot;, col = 2) points(log(1:24),colSums(t(my_basis) * c(1,0,0)) , type = \u0026quot;l\u0026quot;, col = 3) legend(\u0026quot;bottomright\u0026quot;, c(\u0026quot;const\u0026quot;, \u0026quot;log(t)\u0026quot;, \u0026quot;f(log(t))\u0026quot;), lty = c(1,1,1), col = 3:1) The formulas are given in the flexsurv documentation but I won’t repeat it here because I don’t really care. The important thing is that I’m allowing the log-cumulative hazard function to be a linear combination of more complex basis functions in \\(log(t)\\).\nHopefully I’ve done enough examples that you can see what the syntax is doing. Basically, by specifying \\(k\\) internal knots, the baseline log-cumulative hazard is a linear combination of \\(k+2\\) basis functions, including a constant function, a linear function, and \\(k\\) more complex things. By default, if you just write something like formula = Surv(OS, OS.EVENT) ~ TRT01P then this will give you a separate constant (gamma0) for each treatment. For the other basis functions, if you want each treatment to have its own version then you must explicitly specify this by adding e.g. gamma1(TRT01P) into the formula. In principle, I don’t see any reason why you couldn’t fit the following model with a common slope (gamma1) but separate gamma2, to give an alternative to fit_spline_1_1 but with the same number of parameters:\n# spline model with 1 internal knot and where each treatment has its # own intercept (gamma0), its own gamma2, but common slope (gamma1). fit_spline_1_2 \u0026lt;- flexsurvspline(Surv(OS, OS.EVENT) ~ TRT01P + gamma2(TRT01P), data = dat_oak, k = 1, scale = \u0026quot;hazard\u0026quot;) plot_non_param() add_model_estimate(fit_spline_1_2) As a final flourish for this post, I’ll do the landmark/milestone analysis based on the model fit_spline_1_1. Let’s pick 18 months as the landmark time. I’ll take the same approach as the flexsurv package. This simulates from the multivariate normal distribution with mean equal to the maximum likelihood estimates and variance equal to the variance of the maximum likelihood estimates. These random draws are then put through a function outputting the quantity of interest (in our case the difference in survival probabilities at 18 months) and we can then take quantiles to give us an estimate and confidence interval.\n## simulate from MVN distribution sims \u0026lt;- normboot.flexsurvreg(fit_spline_1_1, B = 1e5, raw = TRUE) ## give S(18) on docetaxel for each draw s_18_docetaxel \u0026lt;- 1 - psurvspline(18, gamma = sims[,c(\u0026quot;gamma0\u0026quot;, \u0026quot;gamma1\u0026quot;, \u0026quot;gamma2\u0026quot;)], knots = fit_spline_1_1$knots) ## give S(18) on MPDL3280A for each draw s_18_MPDL3280A \u0026lt;- 1 - psurvspline(18, gamma = cbind(sims[,\u0026quot;gamma0\u0026quot;], sims[,\u0026quot;gamma1\u0026quot;]+sims[,\u0026quot;gamma1(TRT01PMPDL3280A)\u0026quot;], sims[,\u0026quot;gamma2\u0026quot;]), offset = sims[,\u0026quot;TRT01PMPDL3280A\u0026quot;], knots = fit_spline_1_1$knots) ## 95% CI and estimate for diff in survival ## at 18 months (MPDL3280A - docetaxel) quantile(s_18_MPDL3280A - s_18_docetaxel, probs = c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## 0.05959083 0.12016872 0.17982301 p.s. prognostic covariate I promised at one point to discuss the prognostic covariate ECOG status which was available in the data set (in this particular data set it takes values 0 and 1). This post is already longer than I expected, so I won’t dwell on this, but as explained in the flexsurv tutorial it’s possible to do a parametric equivalent of a stratified Cox model by using a spline and allowing each ECOG status to have its own gamma0, gamma1,…,gamma[k+1].\n# Make sure ECOGGR is a factor dat_oak$ECOGGR \u0026lt;- factor(dat_oak$ECOGGR) # spline model with 2 internal knots and where each ECOG grade has its own # complex spline, but where treatment effect within strata is a constant shift fit_spline_ecog_0 \u0026lt;- flexsurvspline(Surv(OS, OS.EVENT) ~ TRT01P + ECOGGR + gamma1(ECOGGR) + gamma2(ECOGGR) + gamma3(ECOGGR), data = dat_oak, k = 2, scale = \u0026quot;hazard\u0026quot;) fit_spline_ecog_0$res ## est L95% U95% se ## gamma0 -4.39991075 -5.1904477 -3.6093738 0.40334259 ## gamma1 2.28385223 0.3406304 4.2270740 0.99145792 ## gamma2 -0.02574156 -0.3626805 0.3111974 0.17191077 ## gamma3 0.11146845 -0.3340140 0.5569509 0.22729116 ## TRT01PMPDL3280A -0.33780623 -0.5033559 -0.1722565 0.08446569 ## ECOGGR1 1.63784166 0.7727434 2.5029399 0.44138477 ## gamma1(ECOGGR1) -0.85035950 -2.8680837 1.1673647 1.02947003 ## gamma2(ECOGGR1) -0.03861271 -0.4030762 0.3258508 0.18595417 ## gamma3(ECOGGR1) 0.02669129 -0.4635331 0.5169156 0.25011906 This is back to being a proportional hazards model which may not be appropriate. We could then make the treatment effect more flexible, e.g.,\n# Make sure ECOGGR is a factor dat_oak$ECOGGR \u0026lt;- factor(dat_oak$ECOGGR) # spline model with 2 internal knots and where each ECOG grade has its own # complex spline, but where treatment effect within strata is handled using a # constant and linear term only. fit_spline_ecog_1 \u0026lt;- flexsurvspline(Surv(OS, OS.EVENT) ~ TRT01P + gamma1(TRT01P) + ECOGGR + gamma1(ECOGGR) + gamma2(ECOGGR) + gamma3(ECOGGR), data = dat_oak, k = 2, scale = \u0026quot;hazard\u0026quot;) fit_spline_ecog_1$res ## est L95% U95% se ## gamma0 -4.524045821 -5.3443397 -3.70375194 0.41852498 ## gamma1 2.296987159 0.3559155 4.23805878 0.99036086 ## gamma2 -0.030850110 -0.3674476 0.30574735 0.17173655 ## gamma3 0.117629309 -0.3273831 0.56264169 0.22705131 ## TRT01PMPDL3280A -0.085983706 -0.5482385 0.37627105 0.23584859 ## ECOGGR1 1.660089705 0.7948724 2.52530704 0.44144553 ## gamma1(TRT01PMPDL3280A) -0.093731710 -0.2545742 0.06711074 0.08206398 ## gamma1(ECOGGR1) -0.799217529 -2.8168457 1.21841060 1.02942102 ## gamma2(ECOGGR1) -0.024330400 -0.3890199 0.34035906 0.18606947 ## gamma3(ECOGGR1) 0.006536356 -0.4839968 0.49706953 0.25027663 It then becomes difficult to summarise the treatment effect succinctly. This is an interesting but complicated topic that I won’t discuss now.\n ","date":1621641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621682255,"objectID":"e92ce6a6ac29bde72ecad82822d269c4","permalink":"/post/landmark-milestone-analysis-under-a-royston-parmar-flexible-survival-model-using-the-r-package-flexsurv/","publishdate":"2021-05-22T00:00:00Z","relpermalink":"/post/landmark-milestone-analysis-under-a-royston-parmar-flexible-survival-model-using-the-r-package-flexsurv/","section":"post","summary":"The aim of this post is to demonstrate a landmark/milestone analysis of RCT time-to-event data with a Royston-Parmar flexible parametric survival model. The original reference is:\nRoyston P, Parmar M (2002). “Flexible Parametric Proportional-Hazards and Proportional-Odds Models for Censored Survival Data, with Application to Prognostic Modelling and Estimation of Treatment Effects.” Statistics in Medicine, 21(1), 2175–2197. doi:10.1002/ sim.1203\nThis model has been expertly coded and documented by Chris Jackson in the R package flexsurv (https://www.","tags":[],"title":"Landmark/Milestone analysis under a Royston-Parmar flexible parametric survival model using the R package flexsurv","type":"post"},{"authors":[],"categories":[],"content":" This is a post for future me. Given my steadily declining maths skills, I can see myself needing to use the generalized simulated annealing function GenSA::GenSA a lot more.\nFor example, suppose I’m helping to design a clinical trial and I want to simulate some data from a Gompertz distribution. I know that the Gompertz distribution allows a rapidly decreasing hazard function (that’s why I want to use it), but I’m not familiar with the details, and don’t have the time/motivation right now to do any maths. I want the survival probability at 1 month to be about 80%, at 3 months to be about 75%, and at 12 months to be about 65%. What are the corresponding parameters of the Gompertz distribution?\nFrom looking at the help pages ?flexsurv::pgompertz, i see that the Gompertz has a shape parameter a and a rate parameter b. I write a quick function to calculate the distance between a Gompertz(a,b) distribution and my desired milestone probabilities:\ngomp_close \u0026lt;- function(a_b, t_1, ## timepoint 1 t_2, ## timepoint 1 t_3, ## timepoint 2 p_1, ## p(event by t_1) p_2, p_3){ a \u0026lt;- a_b[1] b \u0026lt;- a_b[2] distance_1 \u0026lt;- flexsurv::pgompertz(t_1, a, b) - p_1 distance_2 \u0026lt;- flexsurv::pgompertz(t_2, a, b) - p_2 distance_3 \u0026lt;- flexsurv::pgompertz(t_3, a, b) - p_3 distance_1 ^ 2 + distance_2 ^ 2 + distance_3 ^ 2 } and then I let the simulated annealing function find a close enough solution:\nresult \u0026lt;- GenSA::GenSA(par = c(1, 1), # initial values fn = gomp_close, lower = c(-100, 0), # parameter b is positive, so lower lim 0 upper = c(100, 100), t_1 = 1, t_2 = 3, t_3 = 12, p_1 = 0.2, p_2 = 0.25, p_3 = 0.35) result$par ## [1] -0.6307335 0.2536741 Just check this looks ok:\nx \u0026lt;- seq(0, 24, length.out = 100) s \u0026lt;- flexsurv::pgompertz(x, shape = result$par[1], rate = result$par[2], lower.tail = FALSE) plot(x, s, type = \u0026quot;l\u0026quot;, ylim = c(0,1)) points(c(1, 3, 12), c(0.8, 0.75, 0.65)) ","date":1610841600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610898999,"objectID":"38ffe224cd7feef55d68ac6f3db61a96","permalink":"/post/that-ll-do-simulated-annealing-instead-of-maths/","publishdate":"2021-01-17T00:00:00Z","relpermalink":"/post/that-ll-do-simulated-annealing-instead-of-maths/","section":"post","summary":"This is a post for future me. Given my steadily declining maths skills, I can see myself needing to use the generalized simulated annealing function GenSA::GenSA a lot more.\nFor example, suppose I’m helping to design a clinical trial and I want to simulate some data from a Gompertz distribution. I know that the Gompertz distribution allows a rapidly decreasing hazard function (that’s why I want to use it), but I’m not familiar with the details, and don’t have the time/motivation right now to do any maths.","tags":[],"title":"That'll do: simulated annealing instead of maths","type":"post"},{"authors":[],"categories":["survival analysis","clinical trials"],"content":" I’ve written a lot recently about non-proportional hazards in immuno-oncology. One aspect that I have unfortunately overlooked is covariate adjustment. Perhaps this is because it’s so easy to work with extracted data from published Kaplan-Meier plots, where the covariate data is not available. But we know from theoretical and empirical work that covariate adjustment can lead to big increases in power, and perhaps this is equally important or even more important than the power gains from using a weighted log-rank test to match the anticipated non-proportional hazards. To investigate this, we would need access to immuno-oncology RCT data. In general this is not so easy, but fortunately there’s this article containing clinical data from two RCTs (OAK and POPLAR) comparing a checkpoint inhibitor (atezolizumab) versus chemotherapy.\nGandara, D.R., Paul, S.M., Kowanetz, M. et al. Blood-based tumor mutational burden as a predictor of clinical benefit in non-small-cell lung cancer patients treated with atezolizumab. Nat Med 24, 1441–1448 (2018). https://doi.org/10.1038/s41591-018-0134-3\nA number of covariates are provided but I will focus on baseline ECOG grade, which is a performance status indicator – a “0” means fully active; a “1” means restricted in physically strenuous activity. Generally, you would expect patients with lower ECOG status to have longer survival.\nLet’s have a look at the OAK data first…\nlibrary(tidyverse) library(survival) ## Here I\u0026#39;m loading my library locally... devtools::load_all(\u0026quot;~/swlrt/\u0026quot;) ## ...but can also get it from github #devtools::install_github(\u0026quot;dominicmagirr/swlrt\u0026quot;) ## read in the clinical data from the OAK study ## create a new column OS.EVENT which is the opposite ## of OS.CNSR dat_oak \u0026lt;- readxl::read_excel(\u0026quot;41591_2018_134_MOESM3_ESM.xlsx\u0026quot;, sheet = 3) %\u0026gt;% select(PtID, ECOGGR, OS, OS.CNSR, TRT01P) %\u0026gt;% mutate(OS.EVENT = -1 * (OS.CNSR - 1)) ## take a look at first few rows head(dat_oak) ## # A tibble: 6 x 6 ## PtID ECOGGR OS OS.CNSR TRT01P OS.EVENT ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 318 1 5.19 0 Docetaxel 1 ## 2 1088 0 4.83 0 Docetaxel 1 ## 3 345 1 1.94 0 Docetaxel 1 ## 4 588 0 24.6 1 Docetaxel 0 ## 5 306 1 5.98 0 MPDL3280A 1 ## 6 718 1 19.2 1 Docetaxel 0 It’s standard fare for surival analysis. Let’s first make a Kaplan-Meier plot and fit a Cox model based only on treatment…\nkm_oak \u0026lt;- survfit(Surv(OS, OS.EVENT) ~ TRT01P, data = dat_oak) survminer::ggsurvplot(km_oak, data = dat_oak, risk.table = TRUE, break.x.by = 6, legend.title = \u0026quot;\u0026quot;, xlab = \u0026quot;Time (months)\u0026quot;, ylab = \u0026quot;Overall survival\u0026quot;, risk.table.fontsize = 4, legend = c(0.8,0.8)) coxph(Surv(OS, OS.EVENT) ~ TRT01P, data = dat_oak) ## Call: ## coxph(formula = Surv(OS, OS.EVENT) ~ TRT01P, data = dat_oak) ## ## coef exp(coef) se(coef) z p ## TRT01PMPDL3280A -0.31667 0.72857 0.08418 -3.762 0.000169 ## ## Likelihood ratio test=14.16 on 1 df, p=0.0001677 ## n= 850, number of events= 569 The z-value is -3.762 (lower is better). This should be very similar to the z-value of the log-rank test. Let’s check using a function I’ve written for my own R package swlrt.\nswlrt::wlrt(df = dat_oak, trt_colname = \u0026quot;TRT01P\u0026quot;, time_colname = \u0026quot;OS\u0026quot;, event_colname = \u0026quot;OS.EVENT\u0026quot;, wlr = \u0026quot;lr\u0026quot;) ## u v_u z o_minus_e_trt ## 1 -44.59584 139.4881 -3.775946 MPDL3280A Had the non-proportional hazards been anticipated, a weighted log-rank test could have been pre-specified, for example a modestly-weighted log-rank test with \\(t^* = 12\\)…\nswlrt::wlrt(df = dat_oak, trt_colname = \u0026quot;TRT01P\u0026quot;, time_colname = \u0026quot;OS\u0026quot;, event_colname = \u0026quot;OS.EVENT\u0026quot;, wlr = \u0026quot;mw\u0026quot;, t_star = 12) ## u v_u z o_minus_e_trt ## 1 -74.88024 370.4699 -3.890368 MPDL3280A This would have had a lower z-value (-3.89), as you’d expect given the slightly delayed effect,\nBut what about an adjusted analysis? Let’s look at a Cox model including ECOG grade as a covariate…\ncoxph(Surv(OS, OS.EVENT) ~ TRT01P + ECOGGR, data = dat_oak) ## Call: ## coxph(formula = Surv(OS, OS.EVENT) ~ TRT01P + ECOGGR, data = dat_oak) ## ## coef exp(coef) se(coef) z p ## TRT01PMPDL3280A -0.35234 0.70304 0.08445 -4.172 3.02e-05 ## ECOGGR 0.63227 1.88187 0.09072 6.970 3.18e-12 ## ## Likelihood ratio test=65.79 on 2 df, p=5.163e-15 ## n= 850, number of events= 569 Now the z-value is -4.172. So in this example, remembering to adjust for a prognostic covariate appears more important than dealing with the nph issue.\n Can’t we do both? Yes, it’s possible to do the modestly-weighted log-rank test, stratified by ECOG grade:\nswlrt::swlrt(df = dat_oak, trt_colname = \u0026quot;TRT01P\u0026quot;, time_colname = \u0026quot;OS\u0026quot;, event_colname = \u0026quot;OS.EVENT\u0026quot;, strat_colname = \u0026quot;ECOGGR\u0026quot;, wlr = \u0026quot;mw\u0026quot;, t_star = 12) ## $by_strata ## u v_u z o_minus_e_trt ## ECOGGR0 -15.46822 87.47411 -1.653867 MPDL3280A ## ECOGGR1 -71.10216 307.90543 -4.052044 MPDL3280A ## ## $z ## [1] -4.353737 Now the z-value is lower still (-4.35), which seems like the best of both worlds.\n Will covariate adjustment always improve the z-value? No. Let’s go through exactly the same steps for the POPLAR study.\n## read in the clinical data from the POPLAR study ## create a new column OS.EVENT which is the opposite ## of OS.CNSR dat_poplar \u0026lt;- readxl::read_excel(\u0026quot;41591_2018_134_MOESM3_ESM.xlsx\u0026quot;, sheet = 2) %\u0026gt;% select(PtID, ECOGGR, OS, OS.CNSR, TRT01P) %\u0026gt;% mutate(OS.EVENT = -1 * (OS.CNSR - 1)) km_poplar \u0026lt;- survfit(Surv(OS, OS.EVENT) ~ TRT01P, data = dat_poplar) survminer::ggsurvplot(km_poplar, data = dat_poplar, risk.table = TRUE, break.x.by = 6, legend.title = \u0026quot;\u0026quot;, xlab = \u0026quot;Time (months)\u0026quot;, ylab = \u0026quot;Overall survival\u0026quot;, risk.table.fontsize = 4, legend = c(0.8,0.8)) coxph(Surv(OS, OS.EVENT) ~ TRT01P, data = dat_poplar) ## Call: ## coxph(formula = Surv(OS, OS.EVENT) ~ TRT01P, data = dat_poplar) ## ## coef exp(coef) se(coef) z p ## TRT01PMPDL3280A -0.3928 0.6752 0.1426 -2.754 0.00589 ## ## Likelihood ratio test=7.64 on 1 df, p=0.005724 ## n= 287, number of events= 200 coxph(Surv(OS, OS.EVENT) ~ TRT01P + ECOGGR, data = dat_poplar) ## Call: ## coxph(formula = Surv(OS, OS.EVENT) ~ TRT01P + ECOGGR, data = dat_poplar) ## ## coef exp(coef) se(coef) z p ## TRT01PMPDL3280A -0.3770 0.6859 0.1426 -2.644 0.00819 ## ECOGGR 0.4501 1.5684 0.1587 2.836 0.00457 ## ## Likelihood ratio test=16.18 on 2 df, p=0.000306 ## n= 287, number of events= 200 In this case the z-value from the unadjusted Cox model is -2.75, whereas adjusting for ECOGGR it’s -2.644. Let’s look at the un-stratified and stratified modestly-weighted logrank tests (\\(t^* = 12\\))…\n## unstratified swlrt::wlrt(df = dat_poplar, trt_colname = \u0026quot;TRT01P\u0026quot;, time_colname = \u0026quot;OS\u0026quot;, event_colname = \u0026quot;OS.EVENT\u0026quot;, wlr = \u0026quot;mw\u0026quot;, t_star = 12) ## u v_u z o_minus_e_trt ## 1 -36.83455 134.8164 -3.172372 MPDL3280A ## stratified swlrt::swlrt(df = dat_poplar, trt_colname = \u0026quot;TRT01P\u0026quot;, time_colname = \u0026quot;OS\u0026quot;, event_colname = \u0026quot;OS.EVENT\u0026quot;, strat_colname = \u0026quot;ECOGGR\u0026quot;, wlr = \u0026quot;mw\u0026quot;, t_star = 12) ## $by_strata ## u v_u z o_minus_e_trt ## ECOGGR0 -12.98698 27.30791 -2.485215 MPDL3280A ## ECOGGR1 -20.73634 114.79982 -1.935359 MPDL3280A ## ## $z ## [1] -2.828926 Same pattern here, but with lower z-values, as you’d expect given the delayed effect.\n Why the different behaviour: OAK vs POPLAR? This is partly explained from looking at the KM curves by ECOG grade in the two studies. In OAK there is a very big difference in surival (ECOG 1 vs ECOG 0) with the median more-or-less doubling…\nkm_oak_ecog \u0026lt;- survfit(Surv(OS, OS.EVENT) ~ ECOGGR, data = dat_oak) survminer::ggsurvplot(km_oak_ecog, data = dat_oak, risk.table = TRUE, break.x.by = 6, legend.title = \u0026quot;\u0026quot;, xlab = \u0026quot;Time (months)\u0026quot;, ylab = \u0026quot;Overall survival\u0026quot;, risk.table.fontsize = 4, legend = c(0.8,0.8)) …whereas for POPLAR there is still a clear difference but median only increased by about 33%…\nkm_poplar_ecog \u0026lt;- survfit(Surv(OS, OS.EVENT) ~ ECOGGR, data = dat_poplar) survminer::ggsurvplot(km_poplar_ecog, data = dat_poplar, risk.table = TRUE, break.x.by = 6, legend.title = \u0026quot;\u0026quot;, xlab = \u0026quot;Time (months)\u0026quot;, ylab = \u0026quot;Overall survival\u0026quot;, risk.table.fontsize = 4, legend = c(0.8,0.8))  Conclusions This is just a couple of quick examples, but I think it’s safe to say that if you believe you have a strong prognostic covariate and a delayed treatment effect, then a stratified modestly-weighted logrank test is likely to be a good option in terms of type 1 error and power.\nA lot of work has been done on covariate adjustment. Clearly, it’s a complex discussion but I think the consensus is that you are likely to gain more than you lose. A few references I’ve found useful:\nhttps://www.sciencedirect.com/science/article/pii/S0197245697001475\nhttps://www.sciencedirect.com/science/article/pii/S1047279705003248\nhttps://trialsjournal.biomedcentral.com/articles/10.1186/1745-6215-15-139\nhttps://onlinelibrary.wiley.com/doi/full/10.1002/sim.5713\nThe stratified weighted log-rank test has its limits. It can’t handle continuous covariates, and it’s going to break down when the number of strata becomes too large.\n ","date":1599868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599916174,"objectID":"758678bca77a31f01f93f4dde9aaef1c","permalink":"/post/adjusting-for-covariates-under-non-proportional-hazards/","publishdate":"2020-09-12T00:00:00Z","relpermalink":"/post/adjusting-for-covariates-under-non-proportional-hazards/","section":"post","summary":"I’ve written a lot recently about non-proportional hazards in immuno-oncology. One aspect that I have unfortunately overlooked is covariate adjustment. Perhaps this is because it’s so easy to work with extracted data from published Kaplan-Meier plots, where the covariate data is not available. But we know from theoretical and empirical work that covariate adjustment can lead to big increases in power, and perhaps this is equally important or even more important than the power gains from using a weighted log-rank test to match the anticipated non-proportional hazards.","tags":[],"title":"Adjusting for covariates under non-proportional hazards","type":"post"},{"authors":[],"categories":["survival analysis"],"content":" In this blogpost I wanted to explore a Bayesian approach to non-proportional hazards. Take this data set as an example (the data is here).\nlibrary(tidyverse) library(survival) library(brms) ########################## dat \u0026lt;- read_csv(\u0026quot;IPD_both.csv\u0026quot;) %\u0026gt;% mutate(arm = factor(arm)) km_est\u0026lt;-survfit(Surv(time,event)~arm, data=dat) p1 \u0026lt;- survminer::ggsurvplot(km_est, data = dat, risk.table = TRUE, break.x.by = 6, legend.labs = c(\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;), legend.title = \u0026quot;\u0026quot;, xlab = \u0026quot;Time (months)\u0026quot;, ylab = \u0026quot;Overall survival\u0026quot;, risk.table.fontsize = 4, legend = c(0.8,0.8)) p1 It looks like there is a trade-off between short-term survival and long-term survival on the two treatments. But is the apparent long term benefit real? How sure are we? That is my main question of interest here: does treatment “1” improve long term survival?\nProbably the best tool for flexible parametric Bayesian survival analysis is now the rstanarm package (https://arxiv.org/abs/2002.09633). This looks awesome. Unfortunately, I only have access to my work computer at the moment which doesn’t have the latest version installed. Instead I’ll be using the brms package – which is also excellent.\nBefore I can fit a piece-wise exponential model (with changepoints every 6 months), I need to use a little trick (the survSplit function) to change my covariates into time-dependent ones – this kind of thing is explained here.\n## change into time-dependent data set dat_td \u0026lt;- survSplit(Surv(time, event) ~ arm, data = dat, cut = c(6,12,18,24), episode = \u0026quot;period\u0026quot;) %\u0026gt;% mutate(censored = as.numeric(!event), period = factor(period)) ## fit Bayesian model fit1 \u0026lt;- brm(formula = time | cens(censored) + trunc(lb = tstart) ~ arm * period, data = dat_td, family = exponential(), inits = \u0026quot;0\u0026quot;, refresh = 0, seed = 593) summary(fit1) ## Family: exponential ## Links: mu = log ## Formula: time | cens(censored) + trunc(lb = tstart) ~ arm * period ## Data: dat_td (Number of observations: 1813) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 2.89 0.10 2.69 3.10 1.00 3215 3119 ## arm2 0.23 0.15 -0.07 0.53 1.00 2954 3237 ## period2 0.11 0.16 -0.20 0.43 1.00 2821 2525 ## period3 0.68 0.23 0.24 1.14 1.00 3194 2944 ## period4 0.34 0.25 -0.15 0.85 1.00 3074 2596 ## period5 0.06 0.32 -0.52 0.72 1.00 3176 2579 ## arm2:period2 -0.48 0.23 -0.93 -0.04 1.00 2729 2777 ## arm2:period3 -0.92 0.30 -1.53 -0.33 1.00 2949 3065 ## arm2:period4 -0.54 0.35 -1.22 0.14 1.00 2968 2572 ## arm2:period5 -0.08 0.44 -0.95 0.80 1.00 3404 2814 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). To explain where these parameters fit in mathematically, the probability of surviving to time \\(t\\) on arm \\(j=1,2\\) is…\n\\[\\begin{split} S_j(t) = \u0026amp; \\exp\\left(\\frac{-t}{\\exp(\\mu_{j,1})}\\right), \u0026amp; ~~t \\leq 6 \\\\ = \u0026amp; \\exp\\left(\\frac{-6}{\\exp(\\mu_{j,1})}\\right)\\exp\\left(\\frac{-(t-6)}{\\exp(\\mu_{j,2})}\\right), \u0026amp;~~ 6 \u0026lt; t \\leq 12 \\\\ =\u0026amp; \\exp\\left(\\frac{-6}{\\exp(\\mu_{j,1})}\\right)\\exp\\left(\\frac{-(12-6)}{\\exp(\\mu_{j,2})}\\right)\\exp\\left(\\frac{-(t-12)}{\\exp(\\mu_{j,3})}\\right), \u0026amp; ~~12 \u0026lt; t \\leq 18 \\\\ = \u0026amp;...\u0026amp; \\end{split}\\]\nwhere\n\\[\\begin{split} \\mu_{1,1} = \u0026amp; \\text{Intercept}\\\\ \\mu_{1,2} = \u0026amp; \\text{Intercept} + \\text{period2}\\\\ \\mu_{1,3} = \u0026amp; \\text{Intercept} + \\text{period3}\\\\ ... \u0026amp; \\end{split}\\]\nand\n\\[\\begin{split} \\mu_{2,1} = \u0026amp; \\text{Intercept} + \\text{arm2}\\\\ \\mu_{2,2} = \u0026amp; \\text{Intercept} + \\text{arm2} + \\text{period2} + \\text{arm2:period2}\\\\ \\mu_{2,3} = \u0026amp; \\text{Intercept} + \\text{arm2} + \\text{period3} + \\text{arm2:period3}\\\\ ... \u0026amp; \\end{split}\\]\nTo generate posterior samples of these survival probabilities, I need to take the posterior samples of the model parameters, and then perform these transformations. Apologies for the ugly piece of code here.\n## function to turn posterior samples ## of model parameters (\u0026quot;ps = posterior_samples(fit1)\u0026quot;) ## into posterior samples of S(t) get_s \u0026lt;- function(t, ps, arm = \u0026quot;1\u0026quot;, changepoints = c(6,12,18,24)){ ### Extract the scale parameters from posterior samples: log_scales_1 \u0026lt;- matrix(NA, nrow = length(ps[[1]]), ncol = length(changepoints) + 1) log_scales_2 \u0026lt;- matrix(NA, nrow = length(ps[[1]]), ncol = length(changepoints) + 1) log_scales_1[,1] \u0026lt;- ps$b_Intercept log_scales_2[,1] \u0026lt;- ps$b_Intercept + ps$b_arm2 for (i in (1 + seq_along(changepoints))){ log_scales_1[,i] \u0026lt;- ps$b_Intercept + ps[[paste0(\u0026quot;b_period\u0026quot;,i)]] log_scales_2[,i] \u0026lt;- ps$b_Intercept + ps[[paste0(\u0026quot;b_period\u0026quot;,i)]] + ps$b_arm2 + ps[[paste0(\u0026quot;b_arm2:period\u0026quot;,i)]] } scales_1 \u0026lt;- exp(log_scales_1) scales_2 \u0026lt;- exp(log_scales_2) ### Piece-wise exponential survival function: changepoints_Inf \u0026lt;- c(changepoints, Inf) if(arm == 1){ p \u0026lt;- exp(-min(t, changepoints[1]) / scales_1[,1]) for (i in which(changepoints \u0026lt; t)){ p \u0026lt;- p * exp(-(min(t, changepoints_Inf[i + 1]) - changepoints[i]) / scales_1[,i + 1]) } return(p) } else { p \u0026lt;- exp(-min(t, changepoints[1]) / scales_2[,1]) for (i in which(changepoints \u0026lt; t)){ p \u0026lt;- p * exp(-(min(t, changepoints_Inf[i + 1]) - changepoints[i]) / scales_2[,i + 1]) } return(p) } } For example, I can take the posterior samples for the survival probabilities at each month, calculate the posterior means, and see how well this matches the K-M plot:\nps \u0026lt;- posterior_samples(fit1) t_seq \u0026lt;- seq(0, 36, 1) s_1 \u0026lt;- purrr::map(t_seq, get_s, ps = ps) s_2 \u0026lt;- purrr::map(t_seq, get_s, ps = ps, arm = \u0026quot;2\u0026quot;) df_sims \u0026lt;- data.frame(time = t_seq, mean_1 = purrr::map_dbl(s_1, mean), mean_2 = purrr::map_dbl(s_2, mean)) p1$plot + geom_line(data = df_sims, mapping = aes(x = time, y = mean_1), colour = \u0026quot;red\u0026quot;) + geom_line(data = df_sims, mapping = aes(x = time, y = mean_2), colour = \u0026quot;blue\u0026quot;) Now suppose I’m interested in the difference in survival probabilities at 24 months, \\(S_1(24) - S_2(24)\\). I can make a 95% credible interval:\ndiff_24 \u0026lt;- get_s(24, ps) - get_s(24, ps, arm = \u0026quot;2\u0026quot;) quantile(diff_24, probs = c(0.025, 0.975)) %\u0026gt;% round(2) ## 2.5% 97.5% ## 0.01 0.16 But I hadn’t pre-specified 24 months. I might just as well have been interested in the difference at 12,18 or 30 months:\ndiff_12 \u0026lt;- get_s(12, ps) - get_s(12, ps, arm = \u0026quot;2\u0026quot;) quantile(diff_12, probs = c(0.025, 0.975)) %\u0026gt;% round(2) ## 2.5% 97.5% ## -0.07 0.08 diff_18 \u0026lt;- get_s(18, ps) - get_s(18, ps, arm = \u0026quot;2\u0026quot;) quantile(diff_18, probs = c(0.025, 0.975)) %\u0026gt;% round(2) ## 2.5% 97.5% ## 0.00 0.15 diff_30 \u0026lt;- get_s(30, ps) - get_s(30, ps, arm = \u0026quot;2\u0026quot;) quantile(diff_30, probs = c(0.025, 0.975)) %\u0026gt;% round(2) ## 2.5% 97.5% ## -0.03 0.13 How should we interpret these 95% credible intervals, where 2 out of 4 just about exclude 0? Borderline convincing? But hang on… when I view \\((-0.07,0.08)\\times (0,0.15)\\times (0.01,0.16) \\times(-0.03, 0.13)\\) as a credible region for the differences at 12,18,24 and 30 months, this has far less than 95% posterior probability:\nmean(diff_12 \u0026gt; -0.07 \u0026amp; diff_12 \u0026lt; 0.08 \u0026amp; diff_18 \u0026gt; 0 \u0026amp; diff_18 \u0026lt; 0.15 \u0026amp; diff_24 \u0026gt; 0.01 \u0026amp; diff_24 \u0026lt; 0.16 \u0026amp; diff_30 \u0026gt; -0.03 \u0026amp; diff_30 \u0026lt; 0.13) ## [1] 0.86025 To get a 95% credible region, I have to expand the individual credible intervals a bit (via trial and error)…\nquantile(diff_12, probs = c(0.008, 0.992)) %\u0026gt;% round(2) ## 0.8% 99.2% ## -0.08 0.10 quantile(diff_18, probs = c(0.008, 0.992)) %\u0026gt;% round(2) ## 0.8% 99.2% ## -0.02 0.17 quantile(diff_24, probs = c(0.008, 0.992)) %\u0026gt;% round(2) ## 0.8% 99.2% ## -0.01 0.18 quantile(diff_30, probs = c(0.008, 0.992)) %\u0026gt;% round(2) ## 0.8% 99.2% ## -0.04 0.15 mean(diff_12 \u0026gt; -0.08 \u0026amp; diff_12 \u0026lt; 0.10 \u0026amp; diff_18 \u0026gt; -0.02 \u0026amp; diff_18 \u0026lt; 0.17 \u0026amp; diff_24 \u0026gt; -0.01 \u0026amp; diff_24 \u0026lt; 0.18 \u0026amp; diff_30 \u0026gt; -0.04 \u0026amp; diff_30 \u0026lt; 0.15) ## [1] 0.9495 How should we interpret this 95% credible region? None of timepoints quite manage to exclude zero. Borderline unconvincing?\nAnother perspective is that treatment “1” is efficacious if there is at least one timepoint where the survival probability is higher than on treatment “2”. The probability that this is the case is:\nmean(diff_12 \u0026gt; 0 | diff_18 \u0026gt; 0 | diff_24 \u0026gt; 0 | diff_30 \u0026gt; 0) ## [1] 0.99625 Well over a 99% chance. Highly convincing evidence!\nConclusion Maybe I’m overthinking things here, but for me fitting a nice Bayesian model is only half the job done. We also need a good way to describe the (multivariate) posterior distribution. Of course, all three of these interpretations are valid given the prior distribution and model assumptions (I skipped over discussing the prior distribution here). But are these three summaries not superficially quite similar, yet yielding slightly different (perhaps importantly so) conclusions? Are we really prepared to explain these differences to our clinical colleagues, patients, regulators, payers? If not, is this still intellectually superior to a frequentist analysis? I don’t know the answers to these questions.\n ","date":1595808000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595853296,"objectID":"b73349c7cf4ccf5acdc0ed73185399ad","permalink":"/post/a-bayesian-approach-to-non-proportional-hazards/","publishdate":"2020-07-27T00:00:00Z","relpermalink":"/post/a-bayesian-approach-to-non-proportional-hazards/","section":"post","summary":"In this blogpost I wanted to explore a Bayesian approach to non-proportional hazards. Take this data set as an example (the data is here).\nlibrary(tidyverse) library(survival) library(brms) ########################## dat \u0026lt;- read_csv(\u0026quot;IPD_both.csv\u0026quot;) %\u0026gt;% mutate(arm = factor(arm)) km_est\u0026lt;-survfit(Surv(time,event)~arm, data=dat) p1 \u0026lt;- survminer::ggsurvplot(km_est, data = dat, risk.table = TRUE, break.x.by = 6, legend.labs = c(\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;), legend.title = \u0026quot;\u0026quot;, xlab = \u0026quot;Time (months)\u0026quot;, ylab = \u0026quot;Overall survival\u0026quot;, risk.table.fontsize = 4, legend = c(0.","tags":[],"title":"A Bayesian approach to non-proportional hazards","type":"post"},{"authors":[],"categories":["clinical trials","survival analysis"],"content":" In my opinion, many phase III trials in immuno-oncology are 10–20 % larger than they need (ought) to be.\nThis is because the method we use for the primary analysis doesn’t match what we know about how these drugs work.\nFixing this doesn’t require anything fancy, just old-school stats from the 1960s.\nIn this new preprint I try to explain how I think it should be done.\n","date":1594339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594384351,"objectID":"bfe36dc818859ce1e0d0af51cc82531d","permalink":"/post/non-proportional-hazards-in-immuno-oncology-is-an-old-perspective-needed/","publishdate":"2020-07-10T00:00:00Z","relpermalink":"/post/non-proportional-hazards-in-immuno-oncology-is-an-old-perspective-needed/","section":"post","summary":"In my opinion, many phase III trials in immuno-oncology are 10–20 % larger than they need (ought) to be.\nThis is because the method we use for the primary analysis doesn’t match what we know about how these drugs work.\nFixing this doesn’t require anything fancy, just old-school stats from the 1960s.\nIn this new preprint I try to explain how I think it should be done.","tags":[],"title":"Non-proportional hazards in immuno-oncology: is an old perspective needed?","type":"post"},{"authors":[],"categories":[],"content":" Building on my last post, I decided to build a shiny app to make it much easier to extract (approximate) patient-level data from a Kaplan-Meier curve. Video here and code here.\n","date":1591488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591535820,"objectID":"67a5c65111e1886c5fcf36d4da8a8879","permalink":"/post/shiny-app-for-enhancing-published-kaplan-meier-plots/","publishdate":"2020-06-07T00:00:00Z","relpermalink":"/post/shiny-app-for-enhancing-published-kaplan-meier-plots/","section":"post","summary":"Building on my last post, I decided to build a shiny app to make it much easier to extract (approximate) patient-level data from a Kaplan-Meier curve. Video here and code here.","tags":[],"title":"Shiny app for enhancing published Kaplan-Meier plots","type":"post"},{"authors":[],"categories":[],"content":" I made a video with step-by-step instructions for extracting (approximate) patient-level data from a Kaplan-Meier curve. It’s a useful thing to be able to do, and while there are plenty of references already (see below) I thought perhaps a full guide using completely free software was missing.\nReferences Guyot, P., Ades, A., Ouwens, M.J. et al. Enhanced secondary analysis of survival data: reconstructing the data from published Kaplan-Meier survival curves. BMC Med Res Methodol 12, 9 (2012). https://doi.org/10.1186/1471-2288-12-9\nWei, Y., \u0026amp; Royston, P. (2017). Reconstructing Time-to-event Data from Published Kaplan–Meier Curves. The Stata Journal, 17(4), 786–802. https://doi.org/10.1177/1536867X1801700402\nAuthor: Ankit Rohatgi Title: WebPlotDigitizer Website: https://automeris.io/WebPlotDigitizer Version: 4.2 Date: April, 2019 E-Mail: ankitrohatgi@hotmail.com Location: San Francisco, California, USA\nhttps://github.com/giabaio/survHE/blob/master/R/digitise.R\nSatagopan, Jaya M., Alexia Iasonos, and Joseph G. Kanik. “A reconstructed melanoma data set for evaluating differential treatment benefit according to biomarker subgroups.” Data in brief 12 (2017): 667-675. https://doi.org/10.1016/j.dib.2017.05.005\n ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591035901,"objectID":"6d2b143df58f019337b441a5ac384d26","permalink":"/post/extract-patient-level-data-from-a-kaplan-meier-plot/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/post/extract-patient-level-data-from-a-kaplan-meier-plot/","section":"post","summary":"I made a video with step-by-step instructions for extracting (approximate) patient-level data from a Kaplan-Meier curve. It’s a useful thing to be able to do, and while there are plenty of references already (see below) I thought perhaps a full guide using completely free software was missing.\nReferences Guyot, P., Ades, A., Ouwens, M.J. et al. Enhanced secondary analysis of survival data: reconstructing the data from published Kaplan-Meier survival curves.","tags":[],"title":"Extract patient-level data from a Kaplan-Meier plot","type":"post"},{"authors":[],"categories":["clinical trials"],"content":" I’ve spent a lot of time thinking about hypothesis tests in clinical trials recently. Periodically, it’s good to question the fundamentals of whether this is a good idea in general. I don’t think my views have changed. I was considering writing a blog post but then I came across this article by the editors of the Clinical Trials journal which pretty much sums it up. The article can also be found with DOI: 10.1177/1740774519846504.\n","date":1587859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587898833,"objectID":"ab08f612d9b6f35d97cdab92cde5204b","permalink":"/post/is-there-still-a-place-for-significance-testing-in-clinical-trials/","publishdate":"2020-04-26T00:00:00Z","relpermalink":"/post/is-there-still-a-place-for-significance-testing-in-clinical-trials/","section":"post","summary":"I’ve spent a lot of time thinking about hypothesis tests in clinical trials recently. Periodically, it’s good to question the fundamentals of whether this is a good idea in general. I don’t think my views have changed. I was considering writing a blog post but then I came across this article by the editors of the Clinical Trials journal which pretty much sums it up. The article can also be found with DOI: 10.","tags":[],"title":"Is there still a place for significance testing in clinical trials?","type":"post"},{"authors":null,"categories":[],"content":" Previously, I started discussing the flexsurv package. I used it to fit a Weibull model. This is implemented as an accelerated failure time model. It is also a proportional hazards model (although, as I found previously, converting between the two is not so straightforward, but it can be done by SurvRegCensCov).\nNow let’s compare Weibull regression with Cox regression. Firstly, Weibull regression:\n assumes proportional hazards; the number of parameters is equal to \\(k + 2\\), where \\(k\\) is the number of covariates; we can estimate things like the median, \\(P(S\u0026gt;s^*)\\), etc. from the model… but the model might be too restrictive – we won’t estimate these things very well.  Cox regression:\n assumes proportional hazards; there are \\(k\\) parameters (one for each covariate); we don’t estimate the baseline hazard… which means we don’t get estimates for things like the median, \\(P(S\u0026gt;s^*)\\), etc.  flexsurv has a function flexsurvspline which allows one to bridge the gap between Weibull regression and Cox regresssion.\nFrom Weibull regression to Cox regression. For a Weibull distribution with shape \\(a\\), scale \\(b\\), covariates \\(x\\), and regression coefficents \\(\\gamma\\), the survival probability is\n\\[S(t; x) = \\exp\\left\\lbrace - \\left( \\frac{t}{b \\cdot \\exp(x^T\\gamma)} \\right) ^ a\\right\\rbrace\\]\nSince survival is related to the log-cumulative hazard via \\(S=\\exp(-H)\\), this means that\n\\[\\log H(t;x) = a\\log t - a\\log(b) - a x^T\\gamma\\]\nIn words, the log-cumulative hazard has a linear relationship with (log-) time, with the intercept depending on the value of \\(x\\). For a given data set, we can check if this is reasonable by looking at non-paramteric estimates, \\(\\log \\hat{H}(t; x)\\).\nlibrary(survival) library(flexsurv) library(ggplot2) ### Non-parametric analysis fit_km \u0026lt;- survfit(Surv(futime, death) ~ trt, data = myeloid) t_seq \u0026lt;- seq(1, 2500, length.out = 1000) km_sum \u0026lt;- summary(fit_km, times = t_seq, extend = TRUE) ### Weibull regression fit_weibull \u0026lt;- flexsurvreg(Surv(futime, death) ~ trt, dist = \u0026quot;weibull\u0026quot;, data = myeloid) a \u0026lt;- fit_weibull$res[\u0026quot;shape\u0026quot;, \u0026quot;est\u0026quot;] b \u0026lt;- fit_weibull$res[\u0026quot;scale\u0026quot;, \u0026quot;est\u0026quot;] trtB \u0026lt;- fit_weibull$res[\u0026quot;trtB\u0026quot;, \u0026quot;est\u0026quot;] ### plot log-cumulative hazard against log-time df \u0026lt;- data.frame(log_time = log(rep(t_seq, 2)), logcumhaz = log(-log(km_sum$surv)), trt = rep(c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;), each = 1000), logcumhaz_w = c(a * (log(t_seq) - log(b)), a * (log(t_seq) - log(b) - trtB))) ggplot(data = df, mapping = aes(x = log_time, y = logcumhaz, colour = trt)) + geom_line() + geom_line(mapping = aes(x = log_time, y = logcumhaz_w, colour = trt), linetype = 2) + theme_bw() + scale_x_continuous(limits = c(2,8)) + scale_y_continuous(limits = c(-6,0)) What’s going on here? Well, the first thing to acknowledge is that the hazards only appear to be proportional after about 150 (\\(e^5\\)) days. I’m not sure I would immediately abandon a proportional-hazards model, though, as most of the events happen when the hazards are proportional (only 10-15% of the events happen before day 150), so the right-hand-side of the plot is far more important. Looking to the right then: the relationship between the log-cumulative hazard and log-time is not really linear. The distance between the two lines is roughly the same for the two models (Weibull and non-parametric), suggesting that the Weibull model does ok at estimating the hazard ratio. However, the lack of linearity will lead to poor estimates for the medians, \\(P(S\u0026gt;s^*)\\), etc., as can be confirmed by plotting the survival curves:\nggplot(data = df, mapping = aes(x = exp(log_time), y = exp(-exp(logcumhaz)), colour = trt)) + geom_line() + geom_line(mapping = aes(x = exp(log_time), y = exp(-exp(logcumhaz_w)), colour = trt), linetype = 2) + theme_bw() To improve the model, given this lack of linearity, it seems quite natural to change from\n\\[\\log H(t;x) = a\\log t - a\\log(b) - a x^T\\gamma\\]\nto\n\\[\\log H(t;x) = s(\\log t) + x^T\\beta\\]\nwhere \\(s(\\log t)\\) is a natural cubic spline function of (log) time. One can make the model more/less flexible by choosing a large/small number of knots. By default, the knots are placed at quantiles of the uncensored event times. How many knots are required? I don’t really have a good answer for this: one or two. At most, three? In this example, I’m using two inner knots, placed at 33% and 66% of the uncensored event times (indicated by vertical lines):\nfit_spline_2 \u0026lt;- flexsurvspline(Surv(futime, death) ~ trt, data = myeloid, k = 2, scale = \u0026quot;hazard\u0026quot;) spline_2_sum \u0026lt;- summary(fit_spline_2, t = t_seq, type = \u0026quot;cumhaz\u0026quot;) df2 \u0026lt;- cbind(df, data.frame(logcumhaz_s2 = log(c(spline_2_sum$`trt=A`[\u0026quot;est\u0026quot;]$est, spline_2_sum$`trt=B`[\u0026quot;est\u0026quot;]$est)))) ggplot(data = df2, mapping = aes(x = log_time, y = logcumhaz, colour = trt)) + geom_line() + geom_line(mapping = aes(x = log_time, y = logcumhaz_s2, colour = trt), linetype = 2) + theme_bw() + scale_x_continuous(limits = c(2,8)) + scale_y_continuous(limits = c(-6,0)) + geom_vline(xintercept = fit_spline_2$knots) This looks a lot better, and we can see the improvement in the survival curves:\nggplot(data = df2, mapping = aes(x = exp(log_time), y = exp(-exp(logcumhaz)), colour = trt)) + geom_line() + geom_line(mapping = aes(x = exp(log_time), y = exp(-exp(logcumhaz_s2)), colour = trt), linetype = 2) + theme_bw()  From Cox regression to Weibull regression. If we start out from Cox regression\n\\[h(t;x)=h_0(t)\\exp(x^T\\beta)\\]\nthis means that\n\\[\\log H(t;x) = \\log H_0(t;x) + x^T\\beta\\]\nWe estimate the parameters \\(\\beta\\) from the partial likelihood, and don’t estimate \\(\\log H_0(t;x)\\). So \\(\\log H_0(t;x)\\) can be anything. However, with the flexsurvspline function, as long as we use enough knots, \\(s(\\log(t))\\) can be more-or-less anything (smooth), so the two methods will give the same information about \\(\\beta\\).\n Conclusions I can’t really see any reason not to switch from a Cox model to flexsurvspline. You don’t lose anything in terms of inference on \\(\\beta\\), only gain a nice estimate for the baseline hazard. Also, inference is all based on maximum likelihood. No special theory required.\nFrom the other side, if you start out from Weibull regression, and then realise that Weibull is the wrong model, you don’t have to think too hard about how to choose a better model, you know that flexsurvspline will be good (assuming proportional hazards is correct: for non-proportional hazards you may have to think harder).\nWhat can go wrong? In small sample sizes, I guess there could be issues with over-fitting if too many knots are chosen. But given a decent sample size, I can’t see any problems. I would be interested to see a \\(\\log H_0(t;x)\\) that is highly wiggly – doesn’t seem likely in practice.\n ","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"4e99202e9d8bdf2882886313032b1cea","permalink":"/post/flexsurv-2/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/post/flexsurv-2/","section":"post","summary":"Previously, I started discussing the flexsurv package. I used it to fit a Weibull model. This is implemented as an accelerated failure time model. It is also a proportional hazards model (although, as I found previously, converting between the two is not so straightforward, but it can be done by SurvRegCensCov).\nNow let’s compare Weibull regression with Cox regression. Firstly, Weibull regression:\n assumes proportional hazards; the number of parameters is equal to \\(k + 2\\), where \\(k\\) is the number of covariates; we can estimate things like the median, \\(P(S\u0026gt;s^*)\\), etc.","tags":["R Packages","Technical"],"title":"flexsurv 2","type":"post"},{"authors":null,"categories":[],"content":" I’m going to write about some of my favourite R packages. I’ll start with flexsurv (https://github.com/chjackson/flexsurv-dev) by Chris Jackson, which can be used to fit all kinds of parametric models to survival data. It can really do a lot, but I’ll pick out just 2 cool things I like about it:\nFit a standard survival model, but where it’s slightly easier to work out what the parameters mean. Fit a proportional hazards model, which is a lot like a Cox model, but where you also model the baseline hazard using a spline.  1. Consistent parameter values As mentioned in the tutorial (https://www.jstatsoft.org/article/view/v070i08), for simple models flexsurvreg acts as a wrapper for survival::survreg, but where the parameters in the output match those of dweibull.\nWith survival::survreg I would do, e.g.:\ndat_ovarian \u0026lt;- survival::ovarian dat_ovarian$rx \u0026lt;- factor(dat_ovarian$rx) library(survival) fit_survreg = survreg(Surv(futime, fustat) ~ rx, dist = \u0026quot;weibull\u0026quot;, data = dat_ovarian) summary(fit_survreg) ## ## Call: ## survreg(formula = Surv(futime, fustat) ~ rx, data = dat_ovarian, ## dist = \u0026quot;weibull\u0026quot;) ## Value Std. Error z p ## (Intercept) 6.825 0.344 19.84 \u0026lt;2e-16 ## rx2 0.559 0.529 1.06 0.29 ## Log(scale) -0.121 0.251 -0.48 0.63 ## ## Scale= 0.886 ## ## Weibull distribution ## Loglik(model)= -97.4 Loglik(intercept only)= -98 ## Chisq= 1.18 on 1 degrees of freedom, p= 0.28 ## Number of Newton-Raphson Iterations: 5 ## n= 26 Then I would look around online for an explanation of the output e.g. (https://stats.stackexchange.com/questions/159044/weibull-survival-model-in-r). There is also an explanation in ?survreg.\nOn the other hand, using flexsurv:\nlibrary(flexsurv) fit_flexsurvreg = flexsurvreg(Surv(futime, fustat) ~ rx, dist = \u0026quot;weibull\u0026quot;, data = dat_ovarian) fit_flexsurvreg ## Call: ## flexsurvreg(formula = Surv(futime, fustat) ~ rx, data = dat_ovarian, ## dist = \u0026quot;weibull\u0026quot;) ## ## Estimates: ## data mean est L95% U95% se exp(est) ## shape NA 1.129 0.690 1.848 0.284 NA ## scale NA 920.128 468.868 1805.704 316.508 NA ## rx2 0.500 0.559 -0.478 1.597 0.529 1.749 ## L95% U95% ## shape NA NA ## scale NA NA ## rx2 0.620 4.936 ## ## N = 26, Events: 12, Censored: 14 ## Total time at risk: 15588 ## Log-likelihood = -97.36415, df = 3 ## AIC = 200.7283 The parameters shape and scale correspond to dweibull. So I don’t have to think any further? Not quite: I still have to work out what the estimate of rx2 is doing. I might look at exp(est) = 1.749 and somehow expect this to be a hazard ratio. It’s not. It’s a multiplicative effect on the scale parameter. So when rx = 1 the scale is 920.1, and when rx = 2 the scale is 920.1 * 1.749. The hazard ratio (treatment 2 vs treatment 1) is\n\\[\\begin{align} \\frac{h_2(x)}{h_1(x)} \u0026amp; = \\left( \\frac{b_1}{b_2} \\right)^a \\\\ \u0026amp; = \\left( \\frac{920.1}{920.1 \\times 1.749} \\right)^{1.129}\\\\ \u0026amp; = 0.53 \\end{align} \\]\nwhere \\(a\\) is the common shape parameter, and \\(b_1\\) and \\(b_2\\) are the scale parameters.\nOnce I had started writing this post, I realized that it’s actually not straightforward to make inference on the hazard ratio using flexsurv. For working out variances/covariances, the survreg parameterization is indeed better. I looked around for other R packages in this space, and found SurvRegCensCov, which can do this conversion automatically for you:\nlibrary(SurvRegCensCov) ConvertWeibull(fit_survreg)$HR ## HR LB UB ## rx2 0.5318051 0.1683444 1.679989 For completeness, using flexsurv, the log-hazard ratio is\n\\[\\begin{align} \\log\\left( \\frac{h_2(x)}{h_1(x) }\\right) \u0026amp; = a\\left\\lbrace \\log(b_1) - \\log(b_2) \\right\\rbrace \\\\ \u0026amp; = -\\exp(\\log(a)) \\times \\left\\lbrace \\log(b_2) - \\log(b_1) \\right\\rbrace \\end{align}\\]\nI can extract the terms \\(\\alpha:=\\log(a)\\) and \\(\\beta:=\\log(b_2) - \\log(b_1)\\) from the fit_flexsurvreg object, as well as their (co)variance.\nalpha \u0026lt;- fit_flexsurvreg$res.t[\u0026quot;shape\u0026quot;, \u0026quot;est\u0026quot;] beta \u0026lt;- fit_flexsurvreg$res.t[\u0026quot;rx2\u0026quot;, \u0026quot;est\u0026quot;] cov_alpha_beta \u0026lt;- vcov(fit_flexsurvreg)[c(\u0026quot;shape\u0026quot;, \u0026quot;rx2\u0026quot;), c(\u0026quot;shape\u0026quot;, \u0026quot;rx2\u0026quot;)] Then work out the variance of the log-hazard ratio using the delta method.\n\\[\\text{var}\\left\\lbrace -\\beta\\exp(\\alpha) \\right\\rbrace = (-\\beta\\exp(\\alpha), -\\exp(\\alpha)) \\text{Cov}(\\alpha, \\beta) \\left( \\begin{array}{c} -\\beta\\exp(\\alpha) \\\\ -\\exp(\\alpha) \\end{array}\\right)\\]\ngrad \u0026lt;- matrix(c(-beta*exp(alpha), -exp(alpha)), ncol = 1) var_lhr = t(grad) %*% cov_alpha_beta %*% grad se_lhr = sqrt(var_lhr) se_lhr ## [,1] ## [1,] 0.5868807 And to get a 95% confidence interval for the hazard ratio…\nlog_hr = -exp(alpha) * beta log_hr_upr = log_hr + qnorm(0.975) * se_lhr log_hr_lwr = log_hr - qnorm(0.975) * se_lhr data.frame(HR = exp(log_hr), LB = exp(log_hr_lwr), UB = exp(log_hr_upr)) ## HR LB UB ## 1 0.5318051 0.1683444 1.679988  Splines The second thing I really like about flexsurv is the proportional hazards model with a spline for the baseline hazard. I’ll explore this in another post.\n ","date":1572220800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572220800,"objectID":"e943e878f08e699d8fc8f60da43195a1","permalink":"/post/flexsurv/","publishdate":"2019-10-28T00:00:00Z","relpermalink":"/post/flexsurv/","section":"post","summary":"I’m going to write about some of my favourite R packages. I’ll start with flexsurv (https://github.com/chjackson/flexsurv-dev) by Chris Jackson, which can be used to fit all kinds of parametric models to survival data. It can really do a lot, but I’ll pick out just 2 cool things I like about it:\nFit a standard survival model, but where it’s slightly easier to work out what the parameters mean. Fit a proportional hazards model, which is a lot like a Cox model, but where you also model the baseline hazard using a spline.","tags":["R Packages","Technical"],"title":"flexsurv","type":"post"},{"authors":null,"categories":[],"content":" In the last post on longitudinal hurdle models, I had just taken samples from the marginal mean\n\\[\\begin{align} g(\\theta; x) \u0026amp; = E(Y \\mid \\theta; x) \\\\ \u0026amp; = \\int E(Y \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}E(Y \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\\\ \u0026amp;= L^{-1}\\sum_{l = 1}^{L}\\left\\lbrace 1 - \\text{logit}^{-1} ( x^T \\gamma + u_{i\u0026#39;}^{(l)}) \\right\\rbrace \\exp(x^T\\beta + v_{i\u0026#39;}^{(l)} + \\frac{\\sigma^2}{2}).\\end{align}\\]\nOne of the issues with lognormal data is that it is highly skewed, so the mean can be very large. In a small sample, the sample mean can change a lot based on just 1 or 2 large observations. For this reason I would like to sample from other summary measures of \\(Y\\).\nSamples from \\(p(Y \\leq k \\mid \\theta; x)\\) This is quite similar to taking samples from the marginal mean.\n\\[\\begin{align} r(\\theta; x, k) \u0026amp; = p(Y \u0026lt; k \\mid \\theta; x) \\\\ \u0026amp; = \\int p(Y \u0026lt; k \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}p(Y \u0026lt; k \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\\\ \u0026amp;= L^{-1}\\sum_{l = 1}^{L}\\ \\left[ \\pi^{(l)}(x) + \\left\\lbrace 1 - \\pi^{(l)}(x)\\right\\rbrace \\Phi \\left\\lbrace \\frac{\\log(k) - x^T\\beta - v_{i\u0026#39;}^{(l)}}{\\sigma} \\right\\rbrace \\right].\\end{align}\\]\nwhere \\(\\pi^{(l)}(x):= \\text{logit}^{-1} ( x^T \\gamma + u_{i\u0026#39;}^{(l)})\\).\nAgain, I don’t know of any functions for doing this, so I built my own.\ndevtools::install_github(\u0026quot;dominicmagirr/hurlong\u0026quot;) library(brms) x \u0026lt;- data.frame(id = NA, time = \u0026quot;2\u0026quot;) hurlong::marg_pyk_q(k = 0.5, newdata = x, nsims = 1000, fit = fit_hurdle) ## id time 2.5% 50% 97.5% ## 1 NA 2 0.421836 0.5018633 0.5792536  Samples from \\(\\text{median}(Y \\mid \\theta; x)\\) Finally, I am interested in samples from the marginal median.\n\\[\\begin{align} s(\\theta; x) \u0026amp; = \\text{median}(Y \\mid \\theta; x) \\\\ \u0026amp; = \\int \\text{median}(Y \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}\\text{median}(Y\\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x) \\end{align}\\]\nTo evaluate \\(\\text{median}(Y\\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\) at each \\(l\\), I do a search for \\(m^{(l)}\\) such that\n\\[p(Y \u0026lt; m^{(l)} \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x) = 0.5.\\]\nAgain, I’ve made a function that can do this (for this specific longitudinal hurdle model).\nhurlong::marg_med(newdata = x, nsims = 1000, ks = exp(seq(log(0.01), log(100), length.out = 15)), # where to evaluate p(Y\u0026lt;k) fit = fit_hurdle) ## id time 2.5% 50% 97.5% ## 1 NA 2 0.2332964 0.4894882 0.8856152  Predictions To round off this series on longitudinal hurdle models, I want to show how to simulate draws (and find quantiles) from the posterior predictive distribution for a new observation (\\(\\tilde{Y}\\)). Firstly for a patient \\(i\\) already in the data set, where we draw from\n\\[ f(\\tilde{y} \\mid \\mathbf{y} ; x) = f(\\tilde{y} \\mid u_i, v_i, \\theta, \\mathbf{y} ; x)f(u_i, v_i, \\theta \\mid \\mathbf{y}) \\]\nlibrary(brms) predict(fit_hurdle, newdata = data.frame(id = 1, time = \u0026quot;2\u0026quot;), robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.2448615 0.3224153 0 2.145083 and, secondly, for a new patient \\(i\u0026#39;\\), where we draw from\n\\[ f(\\tilde{y} \\mid \\mathbf{y} ; x) = f(\\tilde{y} \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta, \\mathbf{y} ; x)f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) f(\\theta \\mid \\mathbf{y}) \\]\npredict(fit_hurdle, newdata = data.frame(id = NA, time = \u0026quot;2\u0026quot;), allow_new_levels = TRUE, sample_new_levels = \u0026quot;gaussian\u0026quot;, robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.4614108 0.6840876 0 52.57643  ","date":1571961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571961600,"objectID":"667624a716e015e1dfc0b086abb2e1e4","permalink":"/post/longitudinal-hurdle-models-3/","publishdate":"2019-10-25T00:00:00Z","relpermalink":"/post/longitudinal-hurdle-models-3/","section":"post","summary":"In the last post on longitudinal hurdle models, I had just taken samples from the marginal mean\n\\[\\begin{align} g(\\theta; x) \u0026amp; = E(Y \\mid \\theta; x) \\\\ \u0026amp; = \\int E(Y \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}E(Y \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\\\ \u0026amp;= L^{-1}\\sum_{l = 1}^{L}\\left\\lbrace 1 - \\text{logit}^{-1} ( x^T \\gamma + u_{i\u0026#39;}^{(l)}) \\right\\rbrace \\exp(x^T\\beta + v_{i\u0026#39;}^{(l)} + \\frac{\\sigma^2}{2}).","tags":["Technical"],"title":"Longitudinal hurdle models 3","type":"post"},{"authors":null,"categories":[],"content":" In a previous post I fit a longitudinal hurdle model using the brms package.\nlibrary(brms) summary(fit_hurdle) ## Family: hurdle_lognormal ## Links: mu = identity; sigma = identity; hu = logit ## Formula: y ~ time + (1 | q | id) ## hu ~ time + (1 | q | id) ## Data: dat (Number of observations: 800) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~id (Number of levels: 100) ## Estimate Est.Error l-95% CI u-95% CI Rhat ## sd(Intercept) 1.96 0.16 1.67 2.30 1.00 ## sd(hu_Intercept) 2.46 0.30 1.94 3.10 1.00 ## cor(Intercept,hu_Intercept) -0.90 0.04 -0.97 -0.80 1.00 ## Bulk_ESS Tail_ESS ## sd(Intercept) 1475 2467 ## sd(hu_Intercept) 3265 4837 ## cor(Intercept,hu_Intercept) 1949 3570 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.07 0.21 -0.34 0.48 1.01 770 1487 ## hu_Intercept -2.73 0.35 -3.44 -2.07 1.00 1484 3476 ## time2 -0.33 0.08 -0.48 -0.18 1.00 11586 5606 ## hu_time2 1.27 0.24 0.81 1.73 1.00 11668 5524 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.94 0.03 0.88 1.00 1.00 8823 5708 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). I’d now like to do some inference on the model, combining its zero and non-zero parts.\nThe model is: for observation \\(j\\) from patient \\(i\\),\n\\[Y_{i,j} = Z_{i,j}Y^*_{i,j},\\] \\[\\text{logit}\\left\\lbrace P(Z_{i,j} = 0) \\right\\rbrace = x_{i,j}^T\\gamma + u_i,\\]\n\\[\\log (Y^*_{i,j}) \\sim N(x_{i,j}^T\\beta + v_i, ~\\sigma^2), \\]\n\\[\\left( \\begin{array}{c} u_i \\\\ v_i \\\\ \\end{array} \\right) \\sim N\\left(\\left( \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right), \\left( \\begin{array}{c c} \\sigma_u^2 \u0026amp; \\rho \\sigma_u\\sigma_v \\\\ \\rho \\sigma_u \\sigma_v \u0026amp; \\sigma_v ^ 2\\end{array}\\right)\\right),\\] where \\(x_{i,j}^T = (1, t_{i,j})\\). Also let \\(\\theta = (\\gamma, \\beta, \\sigma, \\sigma_u, \\sigma_v, \\rho)\\).\nHow to use fitted.brmsfit Having obtained posterior samples from \\((\\theta, u_1,\\ldots,u_n,v_1,\\ldots,v_n)\\), we might want to look at samples from:\n\\[\\begin{align} h(u_i, v_i, \\theta; x) \u0026amp; = E(Y \\mid u_i, v_i, \\theta; x) \\\\ \u0026amp;= \\left\\lbrace 1 - \\text{logit}^{-1} ( x^T \\gamma + u_i) \\right\\rbrace \\exp(x^T\\beta + v_i + \\frac{\\sigma^2}{2}).\\end{align}\\]\nThis is some kind of patient-specific expectation of \\(Y\\), conditional on the random effects. If patient \\(i\\) is already in the model, then we can just take samples directly from the posterior. This can be achieved with the fitted method:\nfitted(fit_hurdle, newdata = data.frame(id = 1, time = \u0026quot;2\u0026quot;), robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.4034682 0.150417 0.1796645 0.8248473 will give the median and (2.5, 97.5) quantiles of \\(h(u_1, v_1, \\theta ; x)\\) at timepoint “2”.\nAlternatively, we might be more interested in \\(h(0, 0, \\theta ; x)\\), which in some sense is the patient-specific expectation of \\(Y\\) for an “average” patient with random effects fixed at zero. We can get this by setting re_formula = NA:\nfitted(fit_hurdle, newdata = data.frame(time = \u0026quot;2\u0026quot;), robust = TRUE, re_formula = NA) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.9615294 0.2401395 0.5689127 1.569304 Or, we might be more interested in \\(h(u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta ; x)\\) for a new patient \\(i\u0026#39;\\). Now we need to generate samples from the posterior distribution\n\\[f(u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta \\mid \\mathbf{y}) = f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) f(\\theta \\mid \\mathbf{y})\\]\nwe can do this by going through our posterior samples \\(\\theta^{(k)}\\) for \\(k = 1,\\ldots,K\\) and each time simulating\n\\[\\left( \\begin{array}{c} u_{i\u0026#39;}^{(k)} \\\\ v_{i\u0026#39;}^{(k)} \\\\ \\end{array} \\right) \\sim N\\left(\\left( \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right), \\left( \\begin{array}{c c} (\\sigma^{(k)}_u)^2 \u0026amp; \\rho \\sigma^{(k)}_u\\sigma^{(k)}_v \\\\ \\rho \\sigma^{(k)}_u \\sigma^{(k)}_v \u0026amp; (\\sigma^{(k)}_v)^2\\end{array}\\right)\\right).\\]\nThe way I expected this to be done is\nfitted(fit_hurdle, newdata = data.frame(id = NA, time = \u0026quot;2\u0026quot;), allow_new_levels = TRUE, sample_new_levels = \u0026quot;gaussian\u0026quot;, robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.9095811 1.315207 0.0009475099 58.14658 and this is indeed what’s happening, as can be seen by going through the steps manually\nps \u0026lt;- posterior_samples(fit_hurdle) sigma_error \u0026lt;- ps[,\u0026quot;sigma\u0026quot;] sigma_u \u0026lt;- ps[,\u0026quot;sd_id__hu_Intercept\u0026quot;] sigma_v \u0026lt;- ps[,\u0026quot;sd_id__Intercept\u0026quot;] rho \u0026lt;- ps[,\u0026quot;cor_id__Intercept__hu_Intercept\u0026quot;] n_mcmc \u0026lt;- length(rho) x \u0026lt;- data.frame(id = NA, time = \u0026quot;2\u0026quot;) ### simulate u_i\u0026#39; and v_i\u0026#39; u \u0026lt;- rnorm(n_mcmc, sd = sigma_u) ### include correlation v \u0026lt;- rnorm(n_mcmc, mean = u * sigma_v / sigma_u * rho, sd = sqrt((1 - rho^2) * sigma_v ^ 2)) ### extract draws from xi = x*gamma xi \u0026lt;- qlogis(fitted(fit_hurdle, newdata = x, re_formula = NA, dpar = \u0026quot;hu\u0026quot;, summary = FALSE)) ### extract draws from eta = x*beta eta \u0026lt;- fitted(fit_hurdle, newdata = x, re_formula = NA, dpar = \u0026quot;mu\u0026quot;, summary = FALSE) ey \u0026lt;- (1 - plogis(xi + u)) * exp(eta + v + sigma_error ^ 2 / 2) round(quantile(ey, probs = c(0.025, 0.5, 0.975)), 3) ## 2.5% 50% 97.5% ## 0.001 0.927 52.369  Unconditional expectation Instead of a patient-specific expectation, conditional on random effects, we might be more interested in targeting an overall expectation (for a new patient) where we integrate out the random effects. In other words, we want to take samples from\n\\[\\begin{align} g(\\theta; x) \u0026amp; = E(Y \\mid \\theta; x) \\\\ \u0026amp; = \\int E(Y \\mid u_{i\u0026#39;}, v_{i\u0026#39;}, \\theta; x) f(u_{i\u0026#39;}, v_{i\u0026#39;} \\mid \\theta, \\mathbf{y}) du_{i\u0026#39;}dv_{i\u0026#39;} \\\\ \u0026amp;\\approx L^{-1}\\sum_{l = 1}^{L}E(Y \\mid u_{i\u0026#39;}^{(l)}, v_{i\u0026#39;}^{(l)}, \\theta; x)\\\\ \u0026amp;= L^{-1}\\sum_{l = 1}^{L}\\left\\lbrace 1 - \\text{logit}^{-1} ( x^T \\gamma + u_{i\u0026#39;}^{(l)}) \\right\\rbrace \\exp(x^T\\beta + v_{i\u0026#39;}^{(l)} + \\frac{\\sigma^2}{2}).\\end{align}\\]\nThat is, I take \\(g(\\theta^{(k)}; x)\\) for \\(k = 1,\\ldots,K\\). At each \\(k\\), for \\(l= 1 \\ldots,L\\), I take indpendent draws\n\\[\\left( \\begin{array}{c} u_{i\u0026#39;}^{(l)} \\\\ v_{i\u0026#39;}^{(l)} \\\\ \\end{array} \\right) \\sim N\\left(\\left( \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right), \\left( \\begin{array}{c c} (\\sigma^{(k)}_u)^2 \u0026amp; \\rho \\sigma^{(k)}_u\\sigma^{(k)}_v \\\\ \\rho \\sigma^{(k)}_u \\sigma^{(k)}_v \u0026amp; (\\sigma^{(k)}_v)^2\\end{array}\\right)\\right).\\]\nto perform the inner Monte Carlo integration (probably not the best method for this 2-d example). I don’t think it’s possible to do this with brms so I’ve written my own code (which only works for this specific model).\ndevtools::install_github(\u0026quot;dominicmagirr/hurlong\u0026quot;) hurlong::marg_mean_q(newdata = x, nsims = 1000, fit = fit_hurdle) ## id time 2.5% 50% 97.5% ## 1 NA 2 3.845286 7.511835 19.58231  ","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"a90dd793d0d1ae36d7d656c84f4e2229","permalink":"/post/longitudinal-hurdle-models-2/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/post/longitudinal-hurdle-models-2/","section":"post","summary":"In a previous post I fit a longitudinal hurdle model using the brms package.\nlibrary(brms) summary(fit_hurdle) ## Family: hurdle_lognormal ## Links: mu = identity; sigma = identity; hu = logit ## Formula: y ~ time + (1 | q | id) ## hu ~ time + (1 | q | id) ## Data: dat (Number of observations: 800) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~id (Number of levels: 100) ## Estimate Est.","tags":["Technical"],"title":"Longitudinal hurdle models 2","type":"post"},{"authors":null,"categories":[],"content":" Data Recently, I have been modelling data that is longitudinal, contains excess zeros, and where the non-zero data is right-skewed and measured on a continuous scale, rather than being count data.\nI’ll simulate a semi-realistic example data set from a lognormal hurdle model. The “random effects” for the pr(zero) and non-zero parts of the model are negatively correlated.\nset.seed(180) ## 100 patients id \u0026lt;- 1:100 ## 2 timepoints time \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;) ## random effects u \u0026lt;- rnorm(100, sd = 2) v \u0026lt;- rnorm(100, mean = -0.95 * u, sd = sqrt((1 - 0.95^2) * 4)) # p(zero) is negatively correlated with Y* ## non-zero data (4 obs per id, at two timepoints) ystar1 \u0026lt;- exp(rnorm(400, mean = u, sd = 1)) ystar2 \u0026lt;- exp(rnorm(400, mean = -0.5 + u, sd = 1)) ## z = 1 if \u0026quot;cross hurdle\u0026quot;, i.e. if not zero z1 \u0026lt;- rbinom(400, size = 1, prob = 1 - plogis(-2 + v)) # p(cross hurdle) = 1 - p(zero) z2 \u0026lt;- rbinom(400, size = 1, prob = 1 - plogis(-1 + v)) dat \u0026lt;- data.frame(y = c(z1 * ystar1, z2 * ystar2), time = rep(time, each = 400), id = rep(id, 8)) In this data set there are 100 patients and two timepoints. For each patient, at each timepoint, I have simulated 4 independent observations (I’ve only done this to make model convergence a bit easier). The important point is that the data is correlated within patient, and also z (hurdle part) and ystar (non-zero part) are correlated, so that patients who start with a smaller (non-zero) y at the first timepoint are more likely to have y = 0 at the second timepoint. This can be clearly seen in the plot below.\nlibrary(tidyverse) ggplot(data = dat, mapping = aes(x = time, y = y, group = id)) + geom_point() + geom_line() + scale_y_log10()  Fit the model To fit the model I’m using the excellent brms package (https://github.com/paul-buerkner/brms)\nBürkner P. C. (2018). Advanced Bayesian Multilevel Modeling with the R Package brms. The R Journal. 10(1), 395-411. doi.org/10.32614/RJ-2018-017\nlibrary(brms) fit_hurdle \u0026lt;- brm(bf(y ~ time + (1 | q | id), hu ~ time + (1 | q | id)), data = dat, iter = 4000, family = hurdle_lognormal(), refresh = 0) summary(fit_hurdle) ## Family: hurdle_lognormal ## Links: mu = identity; sigma = identity; hu = logit ## Formula: y ~ time + (1 | q | id) ## hu ~ time + (1 | q | id) ## Data: dat (Number of observations: 800) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~id (Number of levels: 100) ## Estimate Est.Error l-95% CI u-95% CI Rhat ## sd(Intercept) 1.96 0.16 1.69 2.29 1.00 ## sd(hu_Intercept) 2.47 0.30 1.93 3.11 1.00 ## cor(Intercept,hu_Intercept) -0.90 0.04 -0.97 -0.81 1.00 ## Bulk_ESS Tail_ESS ## sd(Intercept) 1452 2688 ## sd(hu_Intercept) 2979 5484 ## cor(Intercept,hu_Intercept) 2539 4394 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.07 0.20 -0.34 0.45 1.00 885 1970 ## hu_Intercept -2.72 0.34 -3.43 -2.07 1.00 1662 3089 ## time2 -0.33 0.08 -0.49 -0.18 1.00 14798 5985 ## hu_time2 1.26 0.24 0.80 1.73 1.00 12452 5627 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.94 0.03 0.88 1.00 1.00 11293 6175 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1).  Inference From the output I can see that I have more-or-less recovered the parameters from my model. In practice, I could use this to make inference on the two parts of the model separately. In future posts I’ll discuss how to make inference/predictions when combining the two parts of the model.\n ","date":1571184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571184000,"objectID":"d054a2492f9c45c5d7b2d2c1bf228558","permalink":"/post/longitudinal-hurdle-models/","publishdate":"2019-10-16T00:00:00Z","relpermalink":"/post/longitudinal-hurdle-models/","section":"post","summary":"Data Recently, I have been modelling data that is longitudinal, contains excess zeros, and where the non-zero data is right-skewed and measured on a continuous scale, rather than being count data.\nI’ll simulate a semi-realistic example data set from a lognormal hurdle model. The “random effects” for the pr(zero) and non-zero parts of the model are negatively correlated.\nset.seed(180) ## 100 patients id \u0026lt;- 1:100 ## 2 timepoints time \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;) ## random effects u \u0026lt;- rnorm(100, sd = 2) v \u0026lt;- rnorm(100, mean = -0.","tags":["Technical"],"title":"Longitudinal hurdle models","type":"post"},{"authors":null,"categories":[],"content":" Suppose we have the following data.\ndf \u0026lt;- dplyr::tibble(patient_id = as.character(1:12), treatment = rep(c(\u0026quot;C\u0026quot;, \u0026quot;E\u0026quot;), each = 6), survival_time = survival::Surv(time = c(2,6,8,11,17,24,7,9,13,22,23,25), event = c(1,1,1,1,1,0,1,1,1,0,0,0))) knitr::kable(df)   patient_id treatment survival_time    1 C 2  2 C 6  3 C 8  4 C 11  5 C 17  6 C 24+  7 E 7  8 E 9  9 E 13  10 E 22+  11 E 23+  12 E 25+    Let’s arrange the data in increasing order of survival time.\ndf_ordered \u0026lt;- dplyr::arrange(df, survival_time[,\u0026quot;time\u0026quot;]) knitr::kable(df_ordered)   patient_id treatment survival_time    1 C 2  2 C 6  7 E 7  3 C 8  8 E 9  4 C 11  9 E 13  5 C 17  10 E 22+  11 E 23+  6 C 24+  12 E 25+    Method 1: scores The first step is to estimate the survival probability from the pooled data using the Nelson-Aalen estimator. Then, to get the log-rank scores, we add 1 to the logarithm of the pooled survival estimate for all observed events. For censored events we do not add 1.\npooled_fit \u0026lt;- survival::survfit(survival_time ~ 1, data = df_ordered) df_logrank \u0026lt;- dplyr::mutate(df_ordered, pooled_s = exp(-cumsum(pooled_fit$n.event / pooled_fit$n.risk)), logrank_score = log(pooled_s) + survival_time[,\u0026quot;status\u0026quot;]) knitr::kable(df_logrank, digits = 3)   patient_id treatment survival_time pooled_s logrank_score    1 C 2 0.920 0.917  2 C 6 0.840 0.826  7 E 7 0.760 0.726  3 C 8 0.680 0.615  8 E 9 0.600 0.490  4 C 11 0.520 0.347  9 E 13 0.440 0.180  5 C 17 0.361 -0.020  10 E 22+ 0.361 -1.020  11 E 23+ 0.361 -1.020  6 C 24+ 0.361 -1.020  12 E 25+ 0.361 -1.020    To calculate the logrank score statistic we add up all the scores on the control arm.\nlogrank_score \u0026lt;- dplyr::summarise(dplyr::group_by(df_logrank, treatment), sum(logrank_score)) logrank_score ## # A tibble: 2 x 2 ## treatment `sum(logrank_score)` ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 C 1.66 ## 2 E -1.66  Method 2: observed - expected The score statistic is equivalent to the sum of “observed” - “expected” events at each event time. This is implemented in the survival package.\nlogrank_fit \u0026lt;- survival::survdiff(survival_time ~ treatment, data = df) rbind(logrank_fit$n, logrank_fit$obs - logrank_fit$exp)[2,] ## treatment=C treatment=E ## 1.664105 -1.664105  ","date":1559520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559520000,"objectID":"ccfb07f82da43dae6cf664dc84f3d19e","permalink":"/post/log-rank-test/","publishdate":"2019-06-03T00:00:00Z","relpermalink":"/post/log-rank-test/","section":"post","summary":"Suppose we have the following data.\ndf \u0026lt;- dplyr::tibble(patient_id = as.character(1:12), treatment = rep(c(\u0026quot;C\u0026quot;, \u0026quot;E\u0026quot;), each = 6), survival_time = survival::Surv(time = c(2,6,8,11,17,24,7,9,13,22,23,25), event = c(1,1,1,1,1,0,1,1,1,0,0,0))) knitr::kable(df)   patient_id treatment survival_time    1 C 2  2 C 6  3 C 8  4 C 11  5 C 17  6 C 24+  7 E 7  8 E 9  9 E 13  10 E 22+  11 E 23+  12 E 25+    Let’s arrange the data in increasing order of survival time.","tags":[],"title":"How to calculate the log-rank statistic","type":"post"},{"authors":[],"categories":[],"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1537963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537963200,"objectID":"1038d14884db9cb42dadbba4efd1b86b","permalink":"/talk/group-sequential/","publishdate":"2018-09-26T12:00:00Z","relpermalink":"/talk/group-sequential/","section":"talk","summary":"In this talk I delved into some of the history of John Whitehead's Triangular Test. The context was a mulit-arm trial of several treatments for visceral leishmaniasis, designed by Neal Alexander and colleagues from LSHTM. Together with Annabel Allison (as part of her MSc dissertation), we tried to make several improvements to the design.","tags":null,"title":"Generalizing boundaries for triangular designs, and efficacy estimation at extended follow-ups","type":"talk"},{"authors":null,"categories":null,"content":"","date":1528120800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528120800,"objectID":"596b358e2eb987ca44822814107af0e9","permalink":"/talk/non-proportional-hazards/","publishdate":"2018-06-04T14:00:00Z","relpermalink":"/talk/non-proportional-hazards/","section":"talk","summary":"Here I talk about designing a clinical trial for an immuno-oncology endpoint where we expect a delay in the seperation of the survival curves. Starting with the properties of a standard design under non-proportional hazards, I believed I had made a great improvement by adapting the sample size based on a \"ratio of hazard ratios\". However, a well-chosen group-sequential design made an equal (slightly better) improvement to the operating characteristics, and is much simpler.","tags":null,"title":"Group-sequential and adaptive designs in immuno-oncology","type":"talk"},{"authors":null,"categories":null,"content":"","date":1504085400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504085400,"objectID":"d43c5d4bc26318eecdfaa14e216c503e","permalink":"/talk/vienna-cen-isbs/","publishdate":"2017-08-30T09:30:00Z","relpermalink":"/talk/vienna-cen-isbs/","section":"talk","summary":"Quite often, single-arm studies in early-phase oncology are criticized because they lead to false positives. Here, I wanted to point out that they also have a high risk of false negatives.","tags":null,"title":"Small trials, historical data. Methods and software for decision making.","type":"talk"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]