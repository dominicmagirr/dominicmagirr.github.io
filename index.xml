<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dominic Magirr</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Dominic Magirr</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 20 Nov 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Dominic Magirr</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Example Page 1</title>
      <link>/courses/example/example1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/example/example1/</guid>
      <description>

&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;

&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;

&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;

&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;

&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page 2</title>
      <link>/courses/example/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/example/example2/</guid>
      <description>

&lt;p&gt;Here are some more tips for getting started with Academic:&lt;/p&gt;

&lt;h2 id=&#34;tip-3&#34;&gt;Tip 3&lt;/h2&gt;

&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;

&lt;h2 id=&#34;tip-4&#34;&gt;Tip 4&lt;/h2&gt;

&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian model averaging: struggles with interaction</title>
      <link>/post/bayesian-model-averaging-struggles-with-interaction/</link>
      <pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/post/bayesian-model-averaging-struggles-with-interaction/</guid>
      <description>


&lt;p&gt;This post records my unsuccessful attempts to use Bayesian model averaging when some models contain interaction terms.&lt;/p&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;Data comes from a phase 3 study of an immuno-oncology agent (MPDL3280A) versus chemotherapy (Docetaxel) in lung cancer. The endpoint I’m interested in is progression-free survival (PFS), and there are a number of other covariates (10) that could be prognostic and/or predictive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(survival)
library(BMA)
dat = rio::import(&amp;quot;https://static-content.springer.com/esm/art%3A10.1038%2Fs41591-018-0134-3/MediaObjects/41591_2018_134_MOESM3_ESM.xlsx&amp;quot;,
                  setclass = &amp;quot;tibble&amp;quot;,
                  which = 3,
                  na = c(&amp;quot;&amp;quot;, &amp;quot;.&amp;quot;)) %&amp;gt;% 
  mutate(PFS.EVENT = -1 * (PFS.CNSR - 1),
         high_btmb = ifelse(btmb &amp;gt;= 16, &amp;quot;YES&amp;quot;, &amp;quot;NO&amp;quot;)) %&amp;gt;% 
  select(TRT01P, PFS, PFS.EVENT, 
         high_btmb, BAGE, race2, 
         SEX, HIST, ECOGGR, PRIORTXC, 
         TOBHX, blSLD, METSITES) %&amp;gt;% 
  mutate_if(is.character, as.factor) 


head(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 13
##   TRT01P   PFS PFS.EVENT high_btmb  BAGE race2 SEX   HIST  ECOGGR PRIORTXC
##   &amp;lt;fct&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 Docet…  1.28         1 NO           64 WHITE M     NON-…      1        1
## 2 Docet…  2.33         1 NO           65 WHITE M     NON-…      0        1
## 3 Docet…  1.94         1 YES          75 WHITE M     SQUA…      1        1
## 4 Docet… 12.3          1 NO           61 WHITE F     NON-…      0        2
## 5 MPDL3…  1.41         1 YES          53 WHITE F     NON-…      1        1
## 6 Docet…  8.54         1 &amp;lt;NA&amp;gt;         80 WHITE M     NON-…      1        1
## # … with 3 more variables: TOBHX &amp;lt;fct&amp;gt;, blSLD &amp;lt;dbl&amp;gt;, METSITES &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;goal-of-the-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Goal of the analysis&lt;/h3&gt;
&lt;p&gt;In an ideal world, I want to see which (if any) variables have an interaction with treatment, and, simultaneously, I want to produce reasonable estimates of the magnitudes of such effects. This is probably asking too much, but in a naive first attempt I plugged in all variables (along with their interaction with treatment) into &lt;code&gt;BMA::bic.surv&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- dat[complete.cases(dat),]
x$ECOGGR &amp;lt;- factor(x$ECOGGR)
x$PRIORTXC &amp;lt;- factor(x$PRIORTXC)
fit &amp;lt;- bic.surv(Surv(PFS, PFS.EVENT) ~ .*TRT01P, data = x) 
data.frame(variable = names(fit$output.names),
           probability_non_zero = fit$probne0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    variable probability_non_zero
## 1                    TRT01P                  4.0
## 2                 high_btmb                  0.0
## 3                      BAGE                  0.0
## 4                     race2                  0.0
## 5                       SEX                  0.9
## 6                      HIST                 15.6
## 7                    ECOGGR                 30.9
## 8                  PRIORTXC                 83.2
## 9                     TOBHX                  0.0
## 10                    blSLD                 38.9
## 11                 METSITES                 98.9
## 12       TRT01P.high_btmb..                 17.6
## 13     TRT01PMPDL3280A.BAGE                  4.3
## 14           TRT01P.race2..                  0.0
## 15             TRT01P.SEX..                  0.0
## 16            TRT01P.HIST..                  0.0
## 17          TRT01P.ECOGGR..                  0.8
## 18        TRT01P.PRIORTXC..                 20.2
## 19           TRT01P.TOBHX..                  0.0
## 20    TRT01PMPDL3280A.blSLD                  0.0
## 21 TRT01PMPDL3280A.METSITES                  1.9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gave a posterior probability of 4% of the main effect of treatment being non-zero. I thought this was a bit low. In a univariable Cox model the effect of treatment is modest, but not tiny:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coxph(Surv(PFS, PFS.EVENT) ~ TRT01P, data = x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## coxph(formula = Surv(PFS, PFS.EVENT) ~ TRT01P, data = x)
## 
##                     coef exp(coef) se(coef)     z     p
## TRT01PMPDL3280A -0.11178   0.89425  0.08404 -1.33 0.184
## 
## Likelihood ratio test=1.77  on 1 df, p=0.1835
## n= 641, number of events= 584&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I looked more closely at the interaction terms. Take, for example, the fourth most likely model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sf &amp;lt;- summary(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m4 &amp;lt;- cbind(names(sf[, &amp;quot;model 4&amp;quot;]), 
            as.numeric(unname(sf[,&amp;quot;model 4&amp;quot;])))
m4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1]                                              [,2]       
##  [1,] &amp;quot;TRT01P&amp;quot;                                          NA         
##  [2,] &amp;quot;      .MPDL3280A&amp;quot;                                NA         
##  [3,] &amp;quot;high_btmb&amp;quot;                                       NA         
##  [4,] &amp;quot;         .YES&amp;quot;                                   NA         
##  [5,] &amp;quot;BAGE&amp;quot;                                            NA         
##  [6,] &amp;quot;race2&amp;quot;                                           NA         
##  [7,] &amp;quot;     .OTHER&amp;quot;                                     NA         
##  [8,] &amp;quot;     .WHITE&amp;quot;                                     NA         
##  [9,] &amp;quot;SEX&amp;quot;                                             NA         
## [10,] &amp;quot;   .M&amp;quot;                                           NA         
## [11,] &amp;quot;HIST&amp;quot;                                            NA         
## [12,] &amp;quot;    .SQUAMOUS&amp;quot;                                   NA         
## [13,] &amp;quot;ECOGGR&amp;quot;                                          NA         
## [14,] &amp;quot;      .1&amp;quot;                                        NA         
## [15,] &amp;quot;PRIORTXC&amp;quot;                                        NA         
## [16,] &amp;quot;        .2&amp;quot;                                      &amp;quot;-0.36197&amp;quot; 
## [17,] &amp;quot;TOBHX&amp;quot;                                           NA         
## [18,] &amp;quot;     .NEVER&amp;quot;                                     NA         
## [19,] &amp;quot;     .PREVIOUS&amp;quot;                                  NA         
## [20,] &amp;quot;blSLD&amp;quot;                                           &amp;quot;0.00243&amp;quot;  
## [21,] &amp;quot;METSITES&amp;quot;                                        &amp;quot;0.11115&amp;quot;  
## [22,] &amp;quot;TRT01P.high_btmb..&amp;quot;                              NA         
## [23,] &amp;quot;                  .TRT01PMPDL3280A:high_btmbYES&amp;quot; &amp;quot;-0.268703&amp;quot;
## [24,] &amp;quot;TRT01PMPDL3280A.BAGE&amp;quot;                            NA         
## [25,] &amp;quot;TRT01P.race2..&amp;quot;                                  NA         
## [26,] &amp;quot;              .TRT01PMPDL3280A:race2OTHER&amp;quot;       NA         
## [27,] &amp;quot;              .TRT01PMPDL3280A:race2WHITE&amp;quot;       NA         
## [28,] &amp;quot;TRT01P.SEX..&amp;quot;                                    NA         
## [29,] &amp;quot;            .TRT01PMPDL3280A:SEXM&amp;quot;               NA         
## [30,] &amp;quot;TRT01P.HIST..&amp;quot;                                   NA         
## [31,] &amp;quot;             .TRT01PMPDL3280A:HISTSQUAMOUS&amp;quot;      NA         
## [32,] &amp;quot;TRT01P.ECOGGR..&amp;quot;                                 NA         
## [33,] &amp;quot;               .TRT01PMPDL3280A:ECOGGR1&amp;quot;         NA         
## [34,] &amp;quot;TRT01P.PRIORTXC..&amp;quot;                               NA         
## [35,] &amp;quot;                 .TRT01PMPDL3280A:PRIORTXC2&amp;quot;     NA         
## [36,] &amp;quot;TRT01P.TOBHX..&amp;quot;                                  NA         
## [37,] &amp;quot;              .TRT01PMPDL3280A:TOBHXNEVER&amp;quot;       NA         
## [38,] &amp;quot;              .TRT01PMPDL3280A:TOBHXPREVIOUS&amp;quot;    NA         
## [39,] &amp;quot;TRT01PMPDL3280A.blSLD&amp;quot;                           NA         
## [40,] &amp;quot;TRT01PMPDL3280A.METSITES&amp;quot;                        NA         
## [41,] &amp;quot;&amp;quot;                                                NA         
## [42,] &amp;quot;nVar&amp;quot;                                            &amp;quot;4&amp;quot;        
## [43,] &amp;quot;BIC&amp;quot;                                             &amp;quot;-17.33416&amp;quot;
## [44,] &amp;quot;post prob&amp;quot;                                       &amp;quot;0.049&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;…this model contains 4 parameters: main effects for prior treatment, baseline sum of diameters, number of metastases, and an effect of being &lt;em&gt;both&lt;/em&gt; &lt;code&gt;TRT01P == MPDL3280A&lt;/code&gt; and &lt;code&gt;high_btmb == YES&lt;/code&gt;. Note that there is no variable for the main effect of treatment (nor TMB) in this model. That is, we are fitting models with interaction terms where their main effects are not included. This is, in general, &lt;a href=&#34;https://stats.stackexchange.com/questions/11009/including-the-interaction-but-not-the-main-effects-in-a-model/11080#11080&#34;&gt;a bad thing&lt;/a&gt;. So bad, in fact, that R will not even allow me to fit this model using standard syntax. If I try to do it…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coxph(Surv(PFS, PFS.EVENT) ~ PRIORTXC + blSLD + METSITES + TRT01P:high_btmb, 
               data = x, 
               method = &amp;quot;breslow&amp;quot;, 
               iter.max = 30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## coxph(formula = Surv(PFS, PFS.EVENT) ~ PRIORTXC + blSLD + METSITES + 
##     TRT01P:high_btmb, data = x, method = &amp;quot;breslow&amp;quot;, iter.max = 30)
## 
##                                    coef  exp(coef)   se(coef)      z
## PRIORTXC2                    -0.3612339  0.6968160  0.0987439 -3.658
## blSLD                         0.0022519  1.0022544  0.0009478  2.376
## METSITES                      0.1109506  1.1173397  0.0318342  3.485
## TRT01PDocetaxel:high_btmbNO   0.2579332  1.2942524  0.1415469  1.822
## TRT01PMPDL3280A:high_btmbNO   0.2402823  1.2716080  0.1405986  1.709
## TRT01PDocetaxel:high_btmbYES  0.3815722  1.4645855  0.1688867  2.259
## TRT01PMPDL3280A:high_btmbYES         NA         NA  0.0000000     NA
##                                     p
## PRIORTXC2                    0.000254
## blSLD                        0.017509
## METSITES                     0.000492
## TRT01PDocetaxel:high_btmbNO  0.068418
## TRT01PMPDL3280A:high_btmbNO  0.087452
## TRT01PDocetaxel:high_btmbYES 0.023862
## TRT01PMPDL3280A:high_btmbYES       NA
## 
## Likelihood ratio test=43.91  on 6 df, p=7.704e-08
## n= 641, number of events= 584&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;…then rather than receiving a model with 4 variables, I have been given a model with 6 variables. The main effects of &lt;code&gt;TRT01P&lt;/code&gt; and &lt;code&gt;high_btmb&lt;/code&gt; have been included as well. (This is the behaviour for factors; you are allowed to include a continous variable in an interaction term without including it as a main effect).&lt;/p&gt;
&lt;p&gt;In conclusion, the output from the model averaging is more-or-less uninterpretable when including interactions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predictive-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predictive distributions&lt;/h2&gt;
&lt;p&gt;Although inference on individual parameters is not possible, it’s still likely that &lt;code&gt;BMA&lt;/code&gt; will produce good predictions when I take the whole (mixed) model fit into account. At this point, I had a half-formed plan of re-fitting all of the ‘best’ models using &lt;code&gt;flexsurv::flexsurvspline&lt;/code&gt; to give me the baseline hazards as well, and then combining the output to give me personalized predictive survival curves.&lt;/p&gt;
&lt;p&gt;But then I realized how much work that would involve, especially without access to R’s standard model syntax. It’s possible with some effort to reproduce the “4th best model”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mm_full &amp;lt;- model.matrix(Surv(PFS, PFS.EVENT) ~ .*TRT01P, data = x)
mm_4 &amp;lt;- mm_full[ ,attr(mm_full, &amp;quot;assign&amp;quot;) %in% which(fit$which[4,])]

coxph(Surv(PFS, PFS.EVENT) ~ .,
      data = cbind(x[,c(&amp;quot;PFS&amp;quot;, &amp;quot;PFS.EVENT&amp;quot;)], 
                   as.data.frame(mm_4)), 
      method = &amp;quot;breslow&amp;quot;, 
      iter.max = 30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## coxph(formula = Surv(PFS, PFS.EVENT) ~ ., data = cbind(x[, c(&amp;quot;PFS&amp;quot;, 
##     &amp;quot;PFS.EVENT&amp;quot;)], as.data.frame(mm_4)), method = &amp;quot;breslow&amp;quot;, 
##     iter.max = 30)
## 
##                                      coef  exp(coef)   se(coef)      z
## PRIORTXC2                      -0.3619700  0.6963033  0.0987112 -3.667
## blSLD                           0.0024295  1.0024325  0.0009301  2.612
## METSITES                        0.1111500  1.1175626  0.0317456  3.501
## `TRT01PMPDL3280A:high_btmbYES` -0.2687027  0.7643705  0.1310337 -2.051
##                                       p
## PRIORTXC2                      0.000245
## blSLD                          0.008997
## METSITES                       0.000463
## `TRT01PMPDL3280A:high_btmbYES` 0.040302
## 
## Likelihood ratio test=42.81  on 4 df, p=1.131e-08
## n= 641, number of events= 584&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, functions like &lt;code&gt;predict&lt;/code&gt; etc also wouldn’t work, and I’d have to do everying manually, and it probably wasn’t worth the effort.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;force-all-main-effects-to-be-included-in-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Force all main effects to be included in the model&lt;/h2&gt;
&lt;p&gt;It’s possible to force all main effects to be included in the model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_2 &amp;lt;- bic.surv(Surv(PFS, PFS.EVENT) ~ .*TRT01P, 
                  data = x, 
                  prior.param = c(rep(1, 11),  # main effects
                                  rep(0.5, 10))) # interactions with trt&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I think I’m back to a situation where I can safely look at the effect of individual parameters…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(fit_2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-20-bayesian-model-averaging-struggles-with-interaction_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-11-20-bayesian-model-averaging-struggles-with-interaction_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-11-20-bayesian-model-averaging-struggles-with-interaction_files/figure-html/unnamed-chunk-10-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;…but there’s not a lot of regularization going on now. Doesn’t look a lot different to a backward elimination, or something similar.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;alternative-ways-forward&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Alternative ways forward&lt;/h2&gt;
&lt;p&gt;Abandon model averaging and use something else: ridge/lasso regression or their Bayesian alternatives. This is what I intend to explore next.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>BMA-2</title>
      <link>/post/bma-2/</link>
      <pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/post/bma-2/</guid>
      <description>


&lt;p&gt;In my &lt;a href=&#34;/post/bma&#34;&gt;last post&lt;/a&gt; I had just used the &lt;code&gt;BMA&lt;/code&gt; package to do a quick analysis with Bayesian model averaging. The method rests on a lovely piece of theory. Posterior model probabilities are calculated via a simple application of Bayes rule:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(M_1 | D) = \frac{p(D | M_1)p(M_1)}{\sum_k p(D | M_k)p(M_k)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;but it’s a challenge calculating the terms&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(D | M_k) = \int p(D | \theta_k, M_k)p(\theta_k | M_k) d\theta_k.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.stat.washington.edu/raftery/Research/PDF/socmeth1995.pdf&#34;&gt;Rafferty (Section 4)&lt;/a&gt; gives a top-class explanation for why&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log p(D | M_k) \approx \log p(D | \hat{\theta},M_k) - (d/2)\log n\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; are the number of parameters and “observations”, respectively, in model &lt;span class=&#34;math inline&#34;&gt;\(M_k\)&lt;/span&gt;. This is the same as saying&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(D | M_k) \approx \exp\left\lbrace -0.5 \times \text{BIC}(M_k)\right\rbrace,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(BIC\)&lt;/span&gt; is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayesian_information_criterion&#34;&gt;Bayesian Information Criteria&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;reproduce-bma-results-manually&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reproduce &lt;code&gt;BMA&lt;/code&gt; results manually&lt;/h2&gt;
&lt;p&gt;I find it helpful sometimes to do things the long way round. We started with the &lt;code&gt;survival::veterans&lt;/code&gt; data set, changing some covariates from numeric to categorical&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(survival)
library(BMA)
data(veteran)
veteran$trt &amp;lt;- factor(veteran$trt)
veteran$prior &amp;lt;- factor(veteran$prior)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can fit a Cox model to each combination of covariates&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Extract names of covariates
x_names &amp;lt;- setdiff(names(veteran), 
                   c(&amp;quot;time&amp;quot;, &amp;quot;status&amp;quot;))

## Find all combinations of covariates
xs &amp;lt;- c(&amp;quot;1&amp;quot;,
        unlist(purrr::map(seq_along(x_names),
                          combn,
                          x = x_names,
                          FUN = function(x) paste(x, 
                                                  collapse = &amp;quot; + &amp;quot;))))

## Turn into model formulas
fs &amp;lt;- paste(&amp;quot;Surv(time, status)&amp;quot;, xs, sep = &amp;quot; ~ &amp;quot;)

## Fit models
ms &amp;lt;- purrr::map(fs, 
                 function(f) coxph(as.formula(f),
                                   data = veteran, 
                                   method = &amp;quot;breslow&amp;quot;, 
                                   iter.max = 30))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the models, we can extract the BICs, turn them into (approximate) marginal probabilites of the data, and then into (approximate) posterior model probabilities (where we assume equal prior probabilities for each model)…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_bic &amp;lt;- function(m){
   log(m$nevent) * length(coef(m)) - 2 * rev(m$loglik)[1]
}

bics &amp;lt;- purrr::map_dbl(ms, get_bic)

p_ds &amp;lt;- exp(-0.5 * bics)

p_ms &amp;lt;- p_ds / (sum(p_ds))

## calculate cut-off for model inclusion
odds_ms &amp;lt;- p_ms / (1 - p_ms)
cut_off &amp;lt;- 0.05 * max(odds_ms) / (1 + 0.05 * max(odds_ms))

plot(seq_along(p_ms), p_ms,
     xlab = &amp;quot;Model index&amp;quot;,
     ylab = &amp;quot;p(M|D)&amp;quot;)

abline(h = cut_off, col = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-05-bma-2_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most of the models have very low posterior probability. Only 6 cross the default threshold for inclusion in the final model (odds ratio less than 20 compared to the best model). I can extract those models and re-standardise:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;include_indices &amp;lt;- which(p_ms &amp;gt;= cut_off)
ms_include &amp;lt;- ms[include_indices]
p_ms_include &amp;lt;- p_ms[include_indices] / sum(p_ms[include_indices])
data.frame(x = xs[include_indices],
           p = round(p_ms_include,3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                             x     p
## 1                       karno 0.154
## 2            celltype + karno 0.562
## 3      trt + celltype + karno 0.113
## 4 celltype + karno + diagtime 0.054
## 5      celltype + karno + age 0.061
## 6    celltype + karno + prior 0.056&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can &lt;a href=&#34;/post/bma&#34;&gt;check&lt;/a&gt; that this matches the output from the &lt;code&gt;BMA::bic.surv&lt;/code&gt; summary.&lt;/p&gt;
&lt;div id=&#34;marginal-posterior-distributions-of-coefficients&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Marginal posterior distributions of coefficients&lt;/h3&gt;
&lt;p&gt;This part is messy, to be honest, but I wanted to finish the job. To reproduce the posterior density plots, I start by putting posterior means and variances into a table, with corresponding posterior model probabilities:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_df &amp;lt;- purrr::map2_dfr(ms_include, p_ms_include,
                           function(m, p) data.frame(mean = unname(coef(m)),
                                                     var = unname(diag(vcov(m))), 
                                                     p = p,
                                                     covariate = names(coef(m)),
                                                     stringsAsFactors = FALSE))

head(post_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mean          var         p         covariate
## 1 -0.03324294 2.573811e-05 0.1540161             karno
## 2  0.71214811 6.387773e-02 0.5618671 celltypesmallcell
## 3  1.15080136 8.576748e-02 0.5618671     celltypeadeno
## 4  0.32514265 7.655965e-02 0.5618671     celltypelarge
## 5 -0.03090393 2.681814e-05 0.5618671             karno
## 6  0.25731308 4.025205e-02 0.1130516              trt2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can turn this into a function to find the non-zero densities:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;non_zero_density &amp;lt;- function(x, select_cov){
  
  df &amp;lt;- post_df[post_df$covariate == select_cov,] 
  ds &amp;lt;- purrr::pmap_dfc(list(p = df$p,
                             m = df$mean,
                             v = df$var),
                        function(p,m,v) p * dnorm(x, m, sqrt(v)))
  
  rowSums(ds)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example (I’ve cheated here by looking ahead for a sensible range of x for &lt;code&gt;trt2&lt;/code&gt;)…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x_trt2 &amp;lt;- seq(-0.5, 1, length.out = 100)
nzd_trt2 &amp;lt;- non_zero_density(x = x_trt2,
                      select_cov = &amp;quot;trt2&amp;quot;)

plot(x_trt2,
     nzd_trt2,
     type = &amp;quot;l&amp;quot;,
     xlab = &amp;quot;coefficient for trt2&amp;quot;,
     ylab = &amp;quot;non-zero density&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-05-bma-2_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is looking good now, &lt;a href=&#34;/post/bma&#34;&gt;compared to last time&lt;/a&gt;, but we still need to find the posterior probability that &lt;code&gt;trt2&lt;/code&gt; is excluded from the model, and sort out the y-axis scale (in &lt;code&gt;?plot.bic.surv&lt;/code&gt; it says “The nonzero part of the distribution is scaled so that the maximum height is equal to the probability that the coefficient is nonzero.”)…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prob_nonzero &amp;lt;- function(select_cov){
  
  df &amp;lt;- post_df[post_df$covariate == select_cov,] 
  sum(df$p)
  
}
rescale_nzd &amp;lt;- function(nzd, pnz){
  nzd / max(nzd) * pnz
}

pnz_trt2 &amp;lt;- prob_nonzero(&amp;quot;trt2&amp;quot;)
rnzd_trt2 &amp;lt;- rescale_nzd(nzd_trt2, pnz_trt2)

plot(x_trt2,
     rnzd_trt2,
     type = &amp;quot;l&amp;quot;,
     xlab = &amp;quot;coefficient for trt2&amp;quot;,
     ylab = &amp;quot; &amp;quot;,
     ylim = c(0,1))

points(c(0,0), c(0, 1 - pnz_trt2), type = &amp;#39;l&amp;#39;, lwd = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-05-bma-2_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yep, there we go! I could now repeat this for the other covariates if I wanted.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>BMA</title>
      <link>/post/bma/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/post/bma/</guid>
      <description>


&lt;p&gt;I’ve used the &lt;code&gt;BMA&lt;/code&gt; package for Bayesian model averaging a couple of times this year and think it’s great.&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://www.r-project.org/doc/Rnews/Rnews_2005-2.pdf&#34; class=&#34;uri&#34;&gt;https://www.r-project.org/doc/Rnews/Rnews_2005-2.pdf&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The situation is this: you have a data set with a response variable and multiple explanatory variables (the goal could be prediction or inference). It’s crying out for a regression analysis. But you don’t want to use a naive variable selection method because you &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/pdf/10.1111/tri.12895&#34;&gt;know that’s bad&lt;/a&gt;. Is there something smarter you can do, and do &lt;em&gt;quickly&lt;/em&gt;?&lt;/p&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Let’s take the &lt;code&gt;veteran&lt;/code&gt; data set from the &lt;code&gt;survival&lt;/code&gt; package. This data comes from a randomized trial of two chemotherapies with a primary endpoint of overall survival. There are several covariates, which we can take a look at…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(survival)
library(BMA)
data(veteran)
str(veteran)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    137 obs. of  8 variables:
##  $ trt     : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ celltype: Factor w/ 4 levels &amp;quot;squamous&amp;quot;,&amp;quot;smallcell&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ time    : num  72 411 228 126 118 10 82 110 314 100 ...
##  $ status  : num  1 1 1 1 1 1 1 1 1 0 ...
##  $ karno   : num  60 70 60 60 70 20 40 80 50 70 ...
##  $ diagtime: num  7 5 3 9 11 5 10 29 18 6 ...
##  $ age     : num  69 64 38 63 65 49 69 68 43 70 ...
##  $ prior   : num  0 10 0 10 10 0 10 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;karno&lt;/code&gt; refers to Karnofsky performance score (from 0-100 where higher is better). Something’s gone wrong with the &lt;code&gt;prior&lt;/code&gt; treatment variable: this should be a yes/no. Variable &lt;code&gt;trt&lt;/code&gt; should also be categorical:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;veteran$trt &amp;lt;- factor(veteran$trt)
veteran$prior &amp;lt;- factor(veteran$prior)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the &lt;code&gt;bic.surv&lt;/code&gt; function to fit a Cox model for every possible combination of the explanatory variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test.bic.surv&amp;lt;- bic.surv(Surv(time,status) ~ ., 
                         data = veteran, 
                         factor.type = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The summary gives us the posterior probability for each model (by default each model is given the same prior probability)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(test.bic.surv, 
        conditional=FALSE, 
        digits=2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## bic.surv.formula(f = Surv(time, status) ~ ., data = veteran,     factor.type = TRUE)
## 
## 
##   6  models were selected
##  Best  5  models (cumulative posterior probability =  0.95 ): 
## 
##                     p!=0    EV       SD      model 1   model 2   model 3 
## trt                  11.3                                                
##    .2                       0.02909  0.1058      .         .       0.2573
## celltype             84.6                                                
##         .smallcell          0.61564  0.3538    0.7121      .       0.8196
##         .adeno              0.97619  0.4965    1.1508      .       1.1477
##         .large              0.28260  0.2830    0.3251      .       0.3930
## karno               100.0  -0.03135  0.0052   -0.0309   -0.0332   -0.0311
## diagtime              5.4   0.00018  0.0020      .         .         .   
## age                   6.1  -0.00036  0.0026      .         .         .   
## prior                 5.6                                                
##      .10                    0.00576  0.0542      .         .         .   
##                                                                          
## nVar                                             2         1         3   
## BIC                                          -39.3626  -36.7742  -36.1558
## post prob                                      0.562     0.154     0.113 
##                     model 4   model 5 
## trt                                   
##    .2                   .         .   
## celltype                              
##         .smallcell    0.7208    0.7264
##         .adeno        1.1643    1.1765
##         .large        0.3215    0.3276
## karno                -0.0318   -0.0311
## diagtime                .         .   
## age                  -0.0059      .   
## prior                                 
##      .10                .       0.1026
##                                       
## nVar                    3         3   
## BIC                 -34.9292  -34.7553
## post prob             0.061     0.056&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s possible to plot the marginal posterior distribution for each covariate (averaged according to posterior model probabilities):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(test.bic.surv)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-04-bma_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/2019-11-04-bma_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I’ve found these graphs really useful for showing the boss. When you want to show-off potentially interesting relationships, but prevent them (to some extent) from getting carried away. This is an intuitive way to do it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A drawback of &lt;code&gt;BMA&lt;/code&gt; is that it doesn’t handle missing data. Any row with &lt;code&gt;NA&lt;/code&gt;s will be removed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In a &lt;a href=&#34;/post/bma-2&#34;&gt;future post&lt;/a&gt; I’ll go more into the details of what &lt;code&gt;BMA&lt;/code&gt; is doing.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>flexsurv 2</title>
      <link>/post/flexsurv-2/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/post/flexsurv-2/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;/post/flexsurv&#34;&gt;Previously,&lt;/a&gt; I started discussing the &lt;code&gt;flexsurv&lt;/code&gt; package. I used it to fit a Weibull model. This is implemented as an accelerated failure time model. It is also a proportional hazards model (although, as I found previously, converting between the two is not so straightforward, but it can be done by &lt;code&gt;SurvRegCensCov&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Now let’s compare Weibull regression with Cox regression. Firstly, Weibull regression:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assumes proportional hazards;&lt;/li&gt;
&lt;li&gt;the number of parameters is equal to &lt;span class=&#34;math inline&#34;&gt;\(k + 2\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the number of covariates;&lt;/li&gt;
&lt;li&gt;we can estimate things like the median, &lt;span class=&#34;math inline&#34;&gt;\(P(S&amp;gt;s^*)\)&lt;/span&gt;, etc. from the model…&lt;/li&gt;
&lt;li&gt;but the model might be too restrictive – we won’t estimate these things very well.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cox regression:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assumes proportional hazards;&lt;/li&gt;
&lt;li&gt;there are &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; parameters (one for each covariate);&lt;/li&gt;
&lt;li&gt;we don’t estimate the baseline hazard…&lt;/li&gt;
&lt;li&gt;which means we don’t get estimates for things like the median, &lt;span class=&#34;math inline&#34;&gt;\(P(S&amp;gt;s^*)\)&lt;/span&gt;, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;flexsurv&lt;/code&gt; has a function &lt;code&gt;flexsurvspline&lt;/code&gt; which allows one to bridge the gap between Weibull regression and Cox regresssion.&lt;/p&gt;
&lt;div id=&#34;from-weibull-regression-to-cox-regression.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;From Weibull regression to Cox regression.&lt;/h2&gt;
&lt;p&gt;For a Weibull distribution with shape &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, scale &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, covariates &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, and regression coefficents &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;, the survival probability is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S(t; x) = \exp\left\lbrace - \left( \frac{t}{b \cdot \exp(x^T\gamma)} \right) ^ a\right\rbrace\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since survival is related to the log-cumulative hazard via &lt;span class=&#34;math inline&#34;&gt;\(S=\exp(-H)\)&lt;/span&gt;, this means that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log H(t;x) = a\log t - a\log(b) - a x^T\gamma\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In words, the log-cumulative hazard has a linear relationship with (log-) time, with the intercept depending on the value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. For a given data set, we can check if this is reasonable by looking at non-paramteric estimates, &lt;span class=&#34;math inline&#34;&gt;\(\log \hat{H}(t; x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(survival)
library(flexsurv)
library(ggplot2)

### Non-parametric analysis
fit_km &amp;lt;- survfit(Surv(futime, death) ~ trt,
                  data = myeloid)
t_seq &amp;lt;- seq(1, 2500, length.out = 1000)
km_sum &amp;lt;- summary(fit_km, times = t_seq, extend = TRUE)

### Weibull regression
fit_weibull &amp;lt;- flexsurvreg(Surv(futime, death) ~ trt, 
                           dist = &amp;quot;weibull&amp;quot;, 
                           data = myeloid)

a &amp;lt;- fit_weibull$res[&amp;quot;shape&amp;quot;, &amp;quot;est&amp;quot;]
b &amp;lt;- fit_weibull$res[&amp;quot;scale&amp;quot;, &amp;quot;est&amp;quot;]
trtB &amp;lt;- fit_weibull$res[&amp;quot;trtB&amp;quot;, &amp;quot;est&amp;quot;]


### plot log-cumulative hazard against log-time
df &amp;lt;- data.frame(log_time = log(rep(t_seq, 2)),
                 logcumhaz = log(-log(km_sum$surv)),
                 trt = rep(c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;), each = 1000),
                 logcumhaz_w = c(a * (log(t_seq) - log(b)),
                                 a * (log(t_seq) - log(b) - trtB)))

ggplot(data = df,
       mapping = aes(x = log_time,
                     y = logcumhaz,
                     colour = trt)) +
  geom_line() +
  geom_line(mapping = aes(x = log_time,
                          y = logcumhaz_w,
                          colour = trt),
            linetype = 2) +
  theme_bw() +
  scale_x_continuous(limits = c(2,8)) +
  scale_y_continuous(limits = c(-6,0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-01-flexsurv-2_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What’s going on here? Well, the first thing to acknowledge is that the hazards only appear to be proportional after about 150 (&lt;span class=&#34;math inline&#34;&gt;\(e^5\)&lt;/span&gt;) days. I’m not sure I would immediately abandon a proportional-hazards model, though, as most of the events happen when the hazards are proportional (only 10-15% of the events happen before day 150), so the right-hand-side of the plot is far more important. Looking to the right then: the relationship between the log-cumulative hazard and log-time is not really linear. The distance between the two lines is roughly the same for the two models (Weibull and non-parametric), suggesting that the Weibull model does ok at estimating the hazard ratio. However, the lack of linearity will lead to poor estimates for the medians, &lt;span class=&#34;math inline&#34;&gt;\(P(S&amp;gt;s^*)\)&lt;/span&gt;, etc., as can be confirmed by plotting the survival curves:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = df,
       mapping = aes(x = exp(log_time),
                     y = exp(-exp(logcumhaz)),
                     colour = trt)) +
  geom_line() +
  geom_line(mapping = aes(x = exp(log_time),
                          y = exp(-exp(logcumhaz_w)),
                          colour = trt),
            linetype = 2) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-01-flexsurv-2_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To improve the model, given this lack of linearity, it seems quite natural to change from&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log H(t;x) = a\log t - a\log(b) - a x^T\gamma\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log H(t;x) = s(\log t) + x^T\beta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s(\log t)\)&lt;/span&gt; is a natural cubic spline function of (log) time. One can make the model more/less flexible by choosing a large/small number of knots. By default, the knots are placed at quantiles of the uncensored event times. How many knots are required? I don’t really have a good answer for this: one or two. At most, three? In this example, I’m using two inner knots, placed at 33% and 66% of the uncensored event times (indicated by vertical lines):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_spline_2 &amp;lt;- flexsurvspline(Surv(futime, death) ~ trt, 
                               data = myeloid,
                               k = 2,
                               scale = &amp;quot;hazard&amp;quot;)

spline_2_sum &amp;lt;- summary(fit_spline_2, t = t_seq, type = &amp;quot;cumhaz&amp;quot;)

df2 &amp;lt;- cbind(df,
      data.frame(logcumhaz_s2 = log(c(spline_2_sum$`trt=A`[&amp;quot;est&amp;quot;]$est,
                                  spline_2_sum$`trt=B`[&amp;quot;est&amp;quot;]$est))))

ggplot(data = df2,
       mapping = aes(x = log_time,
                     y = logcumhaz,
                     colour = trt)) +
  geom_line() +
  geom_line(mapping = aes(x = log_time,
                          y = logcumhaz_s2,
                          colour = trt),
            linetype = 2) +
  theme_bw() +
  scale_x_continuous(limits = c(2,8)) +
  scale_y_continuous(limits = c(-6,0)) +
  geom_vline(xintercept = fit_spline_2$knots)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-01-flexsurv-2_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks a lot better, and we can see the improvement in the survival curves:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = df2,
       mapping = aes(x = exp(log_time),
                     y = exp(-exp(logcumhaz)),
                     colour = trt)) +
  geom_line() +
  geom_line(mapping = aes(x = exp(log_time),
                          y = exp(-exp(logcumhaz_s2)),
                          colour = trt),
            linetype = 2) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-11-01-flexsurv-2_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;from-cox-regression-to-weibull-regression.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;From Cox regression to Weibull regression.&lt;/h2&gt;
&lt;p&gt;If we start out from Cox regression&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[h(t;x)=h_0(t)\exp(x^T\beta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;this means that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log H(t;x) = \log H_0(t;x) + x^T\beta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We estimate the parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; from the partial likelihood, and don’t estimate &lt;span class=&#34;math inline&#34;&gt;\(\log H_0(t;x)\)&lt;/span&gt;. So &lt;span class=&#34;math inline&#34;&gt;\(\log H_0(t;x)\)&lt;/span&gt; can be anything. However, with the &lt;code&gt;flexsurvspline&lt;/code&gt; function, as long as we use enough knots, &lt;span class=&#34;math inline&#34;&gt;\(s(\log(t))\)&lt;/span&gt; can be more-or-less anything (smooth), so the two methods will give the same information about &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;I can’t really see any reason not to switch from a Cox model to &lt;code&gt;flexsurvspline&lt;/code&gt;. You don’t lose anything in terms of inference on &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, only gain a nice estimate for the baseline hazard. Also, inference is all based on maximum likelihood. No special theory required.&lt;/p&gt;
&lt;p&gt;From the other side, if you start out from Weibull regression, and then realise that Weibull is the wrong model, you don’t have to think too hard about how to choose a better model, you &lt;em&gt;know&lt;/em&gt; that &lt;code&gt;flexsurvspline&lt;/code&gt; will be good (assuming proportional hazards is correct: for non-proportional hazards you may have to think harder).&lt;/p&gt;
&lt;p&gt;What can go wrong? In small sample sizes, I guess there could be issues with over-fitting if too many knots are chosen. But given a decent sample size, I can’t see any problems. I would be interested to see a &lt;span class=&#34;math inline&#34;&gt;\(\log H_0(t;x)\)&lt;/span&gt; that is highly wiggly – doesn’t seem likely in practice.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>flexsurv</title>
      <link>/post/flexsurv/</link>
      <pubDate>Mon, 28 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/flexsurv/</guid>
      <description>


&lt;p&gt;I’m going to write about some of my favourite R packages. I’ll start with &lt;code&gt;flexsurv&lt;/code&gt; (&lt;a href=&#34;https://github.com/chjackson/flexsurv-dev&#34; class=&#34;uri&#34;&gt;https://github.com/chjackson/flexsurv-dev&lt;/a&gt;) by Chris Jackson, which can be used to fit all kinds of parametric models to survival data. It can really do a lot, but I’ll pick out just 2 cool things I like about it:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fit a standard survival model, but where it’s &lt;em&gt;slightly&lt;/em&gt; easier to work out what the parameters mean.&lt;/li&gt;
&lt;li&gt;Fit a proportional hazards model, which is a lot like a Cox model, but where you also model the baseline hazard using a spline.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;consistent-parameter-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Consistent parameter values&lt;/h2&gt;
&lt;p&gt;As mentioned in the tutorial (&lt;a href=&#34;https://www.jstatsoft.org/article/view/v070i08&#34; class=&#34;uri&#34;&gt;https://www.jstatsoft.org/article/view/v070i08&lt;/a&gt;), for simple models &lt;code&gt;flexsurvreg&lt;/code&gt; acts as a wrapper for &lt;code&gt;survival::survreg&lt;/code&gt;, but where the parameters in the output match those of &lt;code&gt;dweibull&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;survival::survreg&lt;/code&gt; I would do, e.g.:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_ovarian &amp;lt;- survival::ovarian
dat_ovarian$rx &amp;lt;- factor(dat_ovarian$rx)

library(survival)
fit_survreg = survreg(Surv(futime, fustat) ~ rx, 
                      dist = &amp;quot;weibull&amp;quot;,
                      data = dat_ovarian)

summary(fit_survreg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## survreg(formula = Surv(futime, fustat) ~ rx, data = dat_ovarian, 
##     dist = &amp;quot;weibull&amp;quot;)
##              Value Std. Error     z      p
## (Intercept)  6.825      0.344 19.84 &amp;lt;2e-16
## rx2          0.559      0.529  1.06   0.29
## Log(scale)  -0.121      0.251 -0.48   0.63
## 
## Scale= 0.886 
## 
## Weibull distribution
## Loglik(model)= -97.4   Loglik(intercept only)= -98
##  Chisq= 1.18 on 1 degrees of freedom, p= 0.28 
## Number of Newton-Raphson Iterations: 5 
## n= 26&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I would look around online for an explanation of the output e.g. (&lt;a href=&#34;https://stats.stackexchange.com/questions/159044/weibull-survival-model-in-r&#34; class=&#34;uri&#34;&gt;https://stats.stackexchange.com/questions/159044/weibull-survival-model-in-r&lt;/a&gt;). There is also an explanation in &lt;code&gt;?survreg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand, using &lt;code&gt;flexsurv&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(flexsurv)
fit_flexsurvreg = flexsurvreg(Surv(futime, fustat) ~ rx, 
                              dist = &amp;quot;weibull&amp;quot;,
                              data = dat_ovarian)

fit_flexsurvreg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## flexsurvreg(formula = Surv(futime, fustat) ~ rx, data = dat_ovarian, 
##     dist = &amp;quot;weibull&amp;quot;)
## 
## Estimates: 
##        data mean  est       L95%      U95%      se        exp(est)
## shape        NA      1.129     0.690     1.848     0.284        NA
## scale        NA    920.128   468.868  1805.704   316.508        NA
## rx2       0.500      0.559    -0.478     1.597     0.529     1.749
##        L95%      U95%    
## shape        NA        NA
## scale        NA        NA
## rx2       0.620     4.936
## 
## N = 26,  Events: 12,  Censored: 14
## Total time at risk: 15588
## Log-likelihood = -97.36415, df = 3
## AIC = 200.7283&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The parameters &lt;code&gt;shape&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt; correspond to &lt;code&gt;dweibull&lt;/code&gt;. So I don’t have to think any further? Not quite: I still have to work out what the estimate of &lt;code&gt;rx2&lt;/code&gt; is doing. I might look at &lt;code&gt;exp(est) = 1.749&lt;/code&gt; and somehow expect this to be a hazard ratio. It’s not. It’s a multiplicative effect on the scale parameter. So when &lt;code&gt;rx = 1&lt;/code&gt; the scale is &lt;code&gt;920.1&lt;/code&gt;, and when &lt;code&gt;rx = 2&lt;/code&gt; the scale is &lt;code&gt;920.1 * 1.749&lt;/code&gt;. The hazard ratio (treatment 2 vs treatment 1) is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} \frac{h_2(x)}{h_1(x)} &amp;amp; = \left( \frac{b_1}{b_2} \right)^a \\
                                      &amp;amp; = \left( \frac{920.1}{920.1 \times 1.749} \right)^{1.129}\\
                                      &amp;amp; = 0.53 \end{align} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the common shape parameter, and &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt; are the scale parameters.&lt;/p&gt;
&lt;p&gt;Once I had started writing this post, I realized that it’s actually not straightforward to make inference on the hazard ratio using &lt;code&gt;flexsurv&lt;/code&gt;. For working out variances/covariances, the &lt;code&gt;survreg&lt;/code&gt; parameterization is indeed better. I looked around for other R packages in this space, and found &lt;code&gt;SurvRegCensCov&lt;/code&gt;, which can do this conversion automatically for you:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(SurvRegCensCov)
ConvertWeibull(fit_survreg)$HR&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            HR        LB       UB
## rx2 0.5318051 0.1683444 1.679989&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For completeness, using &lt;code&gt;flexsurv&lt;/code&gt;, the log-hazard ratio is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} \log\left( \frac{h_2(x)}{h_1(x) }\right) &amp;amp; = a\left\lbrace \log(b_1) - \log(b_2) \right\rbrace \\
                                                         &amp;amp; = -\exp(\log(a)) \times \left\lbrace \log(b_2) - \log(b_1)  \right\rbrace \end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I can extract the terms &lt;span class=&#34;math inline&#34;&gt;\(\alpha:=\log(a)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta:=\log(b_2) - \log(b_1)\)&lt;/span&gt; from the &lt;code&gt;fit_flexsurvreg&lt;/code&gt; object, as well as their (co)variance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alpha &amp;lt;- fit_flexsurvreg$res.t[&amp;quot;shape&amp;quot;, &amp;quot;est&amp;quot;]
beta &amp;lt;- fit_flexsurvreg$res.t[&amp;quot;rx2&amp;quot;, &amp;quot;est&amp;quot;]
cov_alpha_beta &amp;lt;- vcov(fit_flexsurvreg)[c(&amp;quot;shape&amp;quot;, &amp;quot;rx2&amp;quot;), c(&amp;quot;shape&amp;quot;, &amp;quot;rx2&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then work out the variance of the log-hazard ratio using the delta method.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{var}\left\lbrace -\beta\exp(\alpha) \right\rbrace = (-\beta\exp(\alpha), -\exp(\alpha)) \text{Cov}(\alpha, \beta) \left( \begin{array}{c} -\beta\exp(\alpha) \\ -\exp(\alpha) \end{array}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grad &amp;lt;- matrix(c(-beta*exp(alpha),
                 -exp(alpha)), 
               ncol = 1)
var_lhr = t(grad) %*% cov_alpha_beta %*% grad
se_lhr = sqrt(var_lhr)
se_lhr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]
## [1,] 0.5868807&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And to get a 95% confidence interval for the hazard ratio…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_hr = -exp(alpha) * beta
log_hr_upr = log_hr + qnorm(0.975) * se_lhr
log_hr_lwr = log_hr - qnorm(0.975) * se_lhr

data.frame(HR = exp(log_hr),
           LB = exp(log_hr_lwr),
           UB = exp(log_hr_upr))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          HR        LB       UB
## 1 0.5318051 0.1683444 1.679988&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;splines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Splines&lt;/h2&gt;
&lt;p&gt;The second thing I really like about &lt;code&gt;flexsurv&lt;/code&gt; is the proportional hazards model with a spline for the baseline hazard. I’ll explore this in &lt;a href=&#34;/post/flexsurv-2&#34;&gt;another post&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal hurdle models 3</title>
      <link>/post/longitudinal-hurdle-models-3/</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/longitudinal-hurdle-models-3/</guid>
      <description>


&lt;p&gt;In the &lt;a href=&#34;/post/longitudinal-hurdle-models-2&#34;&gt;last&lt;/a&gt; post on longitudinal hurdle models, I had just taken samples from the marginal mean&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} g(\theta; x) &amp;amp; = E(Y \mid  \theta; x) \\
&amp;amp; = \int   E(Y \mid u_{i&amp;#39;}, v_{i&amp;#39;}, \theta; x) f(u_{i&amp;#39;}, v_{i&amp;#39;} \mid \theta, \mathbf{y}) du_{i&amp;#39;}dv_{i&amp;#39;} \\ 
&amp;amp;\approx  L^{-1}\sum_{l = 1}^{L}E(Y \mid u_{i&amp;#39;}^{(l)}, v_{i&amp;#39;}^{(l)}, \theta; x)\\
&amp;amp;=  L^{-1}\sum_{l = 1}^{L}\left\lbrace 1 - \text{logit}^{-1} ( x^T \gamma + u_{i&amp;#39;}^{(l)}) \right\rbrace \exp(x^T\beta + v_{i&amp;#39;}^{(l)} + \frac{\sigma^2}{2}).\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;One of the issues with lognormal data is that it is highly skewed, so the mean can be very large. In a small sample, the sample mean can change a lot based on just 1 or 2 large observations. For this reason I would like to sample from other summary measures of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;samples-from-py-leq-k-mid-theta-x&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Samples from &lt;span class=&#34;math inline&#34;&gt;\(p(Y \leq k \mid \theta; x)\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;This is quite similar to taking samples from the marginal mean.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} r(\theta; x, k) &amp;amp; = p(Y &amp;lt; k \mid  \theta; x) \\
&amp;amp; = \int   p(Y &amp;lt; k \mid u_{i&amp;#39;}, v_{i&amp;#39;}, \theta; x) f(u_{i&amp;#39;}, v_{i&amp;#39;} \mid \theta, \mathbf{y}) du_{i&amp;#39;}dv_{i&amp;#39;} \\ 
&amp;amp;\approx  L^{-1}\sum_{l = 1}^{L}p(Y &amp;lt; k \mid u_{i&amp;#39;}^{(l)}, v_{i&amp;#39;}^{(l)}, \theta; x)\\
&amp;amp;=  L^{-1}\sum_{l = 1}^{L}\ \left[ \pi^{(l)}(x) + \left\lbrace 1 - \pi^{(l)}(x)\right\rbrace \Phi \left\lbrace \frac{\log(k) - x^T\beta - v_{i&amp;#39;}^{(l)}}{\sigma} \right\rbrace \right].\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\pi^{(l)}(x):= \text{logit}^{-1} ( x^T \gamma + u_{i&amp;#39;}^{(l)})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Again, I don’t know of any functions for doing this, so I built my own.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;dominicmagirr/hurlong&amp;quot;)
library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- data.frame(id = NA, time = &amp;quot;2&amp;quot;)
hurlong::marg_pyk_q(k = 0.5,
                    newdata = x, 
                    nsims = 1000, 
                    fit = fit_hurdle)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   id time     2.5%       50%     97.5%
## 1 NA    2 0.421836 0.5018633 0.5792536&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;samples-from-textmediany-mid-theta-x&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Samples from &lt;span class=&#34;math inline&#34;&gt;\(\text{median}(Y \mid \theta; x)\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Finally, I am interested in samples from the marginal median.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} s(\theta; x) &amp;amp; = \text{median}(Y  \mid  \theta; x) \\
&amp;amp; = \int   \text{median}(Y \mid u_{i&amp;#39;}, v_{i&amp;#39;}, \theta; x) f(u_{i&amp;#39;}, v_{i&amp;#39;} \mid \theta, \mathbf{y}) du_{i&amp;#39;}dv_{i&amp;#39;} \\ 
&amp;amp;\approx  L^{-1}\sum_{l = 1}^{L}\text{median}(Y\mid u_{i&amp;#39;}^{(l)}, v_{i&amp;#39;}^{(l)}, \theta; x) \end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To evaluate &lt;span class=&#34;math inline&#34;&gt;\(\text{median}(Y\mid u_{i&amp;#39;}^{(l)}, v_{i&amp;#39;}^{(l)}, \theta; x)\)&lt;/span&gt; at each &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;, I do a search for &lt;span class=&#34;math inline&#34;&gt;\(m^{(l)}\)&lt;/span&gt; such that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(Y &amp;lt; m^{(l)} \mid u_{i&amp;#39;}^{(l)}, v_{i&amp;#39;}^{(l)}, \theta; x) = 0.5.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, I’ve made a function that can do this (for this specific longitudinal hurdle model).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hurlong::marg_med(newdata = x, 
                  nsims = 1000, 
                  ks = exp(seq(log(0.01), 
                               log(100), 
                               length.out = 15)), # where to evaluate p(Y&amp;lt;k)
                  fit = fit_hurdle)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   id time      2.5%       50%     97.5%
## 1 NA    2 0.2332964 0.4894882 0.8856152&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predictions&lt;/h2&gt;
&lt;p&gt;To round off this series on longitudinal hurdle models, I want to show how to simulate draws (and find quantiles) from the posterior predictive distribution for a new observation (&lt;span class=&#34;math inline&#34;&gt;\(\tilde{Y}\)&lt;/span&gt;). Firstly for a patient &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; already in the data set, where we draw from&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(\tilde{y} \mid \mathbf{y} ; x) = f(\tilde{y} \mid u_i, v_i, \theta, \mathbf{y} ; x)f(u_i, v_i, \theta \mid \mathbf{y}) \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(fit_hurdle,
        newdata = data.frame(id = 1, time = &amp;quot;2&amp;quot;),
        robust = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate Est.Error Q2.5    Q97.5
## [1,] 0.2448615 0.3224153    0 2.145083&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and, secondly, for a new patient &lt;span class=&#34;math inline&#34;&gt;\(i&amp;#39;\)&lt;/span&gt;, where we draw from&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(\tilde{y} \mid \mathbf{y} ; x) = f(\tilde{y} \mid u_{i&amp;#39;}, v_{i&amp;#39;}, \theta, \mathbf{y} ; x)f(u_{i&amp;#39;}, v_{i&amp;#39;} \mid \theta,  \mathbf{y}) f(\theta \mid \mathbf{y}) \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(fit_hurdle,
        newdata = data.frame(id = NA, time = &amp;quot;2&amp;quot;),
        allow_new_levels = TRUE,
        sample_new_levels = &amp;quot;gaussian&amp;quot;,
        robust = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate Est.Error Q2.5    Q97.5
## [1,] 0.4614108 0.6840876    0 52.57643&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal hurdle models 2</title>
      <link>/post/longitudinal-hurdle-models-2/</link>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/longitudinal-hurdle-models-2/</guid>
      <description>


&lt;p&gt;In a &lt;a href=&#34;/post/longitudinal-hurdle-models&#34;&gt;previous&lt;/a&gt; post I fit a longitudinal hurdle model using the &lt;code&gt;brms&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)
summary(fit_hurdle)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: hurdle_lognormal 
##   Links: mu = identity; sigma = identity; hu = logit 
## Formula: y ~ time + (1 | q | id) 
##          hu ~ time + (1 | q | id)
##    Data: dat (Number of observations: 800) 
## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;
##          total post-warmup samples = 8000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##                             Estimate Est.Error l-95% CI u-95% CI Rhat
## sd(Intercept)                   1.96      0.16     1.67     2.30 1.00
## sd(hu_Intercept)                2.46      0.30     1.94     3.10 1.00
## cor(Intercept,hu_Intercept)    -0.90      0.04    -0.97    -0.80 1.00
##                             Bulk_ESS Tail_ESS
## sd(Intercept)                   1475     2467
## sd(hu_Intercept)                3265     4837
## cor(Intercept,hu_Intercept)     1949     3570
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept        0.07      0.21    -0.34     0.48 1.01      770     1487
## hu_Intercept    -2.73      0.35    -3.44    -2.07 1.00     1484     3476
## time2           -0.33      0.08    -0.48    -0.18 1.00    11586     5606
## hu_time2         1.27      0.24     0.81     1.73 1.00    11668     5524
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.94      0.03     0.88     1.00 1.00     8823     5708
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’d now like to do some inference on the model, combining its zero and non-zero parts.&lt;/p&gt;
&lt;p&gt;The model is: for observation &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; from patient &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_{i,j} = Z_{i,j}Y^*_{i,j},\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\text{logit}\left\lbrace P(Z_{i,j} = 0) \right\rbrace = x_{i,j}^T\gamma + u_i,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log (Y^*_{i,j})  \sim N(x_{i,j}^T\beta + v_i, ~\sigma^2), \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left( \begin{array}{c}  u_i \\ v_i \\ \end{array} \right) \sim N\left(\left( \begin{array}{c}  0 \\ 0 \\ \end{array} \right), \left( \begin{array}{c c} \sigma_u^2 &amp;amp; \rho \sigma_u\sigma_v \\ \rho \sigma_u \sigma_v &amp;amp; \sigma_v ^ 2\end{array}\right)\right),\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(x_{i,j}^T = (1, t_{i,j})\)&lt;/span&gt;. Also let &lt;span class=&#34;math inline&#34;&gt;\(\theta = (\gamma, \beta, \sigma, \sigma_u, \sigma_v, \rho)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;how-to-use-fitted.brmsfit&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to use &lt;code&gt;fitted.brmsfit&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Having obtained posterior samples from &lt;span class=&#34;math inline&#34;&gt;\((\theta, u_1,\ldots,u_n,v_1,\ldots,v_n)\)&lt;/span&gt;, we &lt;em&gt;might&lt;/em&gt; want to look at samples from:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} h(u_i, v_i, \theta; x) &amp;amp; = E(Y \mid u_i, v_i, \theta; x) \\ &amp;amp;= \left\lbrace 1 - \text{logit}^{-1} ( x^T \gamma + u_i) \right\rbrace \exp(x^T\beta + v_i + \frac{\sigma^2}{2}).\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is some kind of patient-specific expectation of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, conditional on the random effects. If patient &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is already in the model, then we can just take samples directly from the posterior. This can be achieved with the &lt;code&gt;fitted&lt;/code&gt; method:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(fit_hurdle, 
       newdata = data.frame(id = 1, time = &amp;quot;2&amp;quot;), 
       robust = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate Est.Error      Q2.5     Q97.5
## [1,] 0.4034682  0.150417 0.1796645 0.8248473&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will give the median and (2.5, 97.5) quantiles of &lt;span class=&#34;math inline&#34;&gt;\(h(u_1, v_1, \theta ; x)\)&lt;/span&gt; at timepoint “2”.&lt;/p&gt;
&lt;p&gt;Alternatively, we &lt;em&gt;might&lt;/em&gt; be more interested in &lt;span class=&#34;math inline&#34;&gt;\(h(0, 0, \theta ; x)\)&lt;/span&gt;, which in some sense is the patient-specific expectation of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for an “average” patient with random effects fixed at zero. We can get this by setting &lt;code&gt;re_formula = NA&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(fit_hurdle, 
       newdata = data.frame(time = &amp;quot;2&amp;quot;), 
       robust = TRUE, 
       re_formula = NA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate Est.Error      Q2.5    Q97.5
## [1,] 0.9615294 0.2401395 0.5689127 1.569304&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or, we &lt;em&gt;might&lt;/em&gt; be more interested in &lt;span class=&#34;math inline&#34;&gt;\(h(u_{i&amp;#39;}, v_{i&amp;#39;}, \theta ; x)\)&lt;/span&gt; for a new patient &lt;span class=&#34;math inline&#34;&gt;\(i&amp;#39;\)&lt;/span&gt;. Now we need to generate samples from the posterior distribution&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(u_{i&amp;#39;}, v_{i&amp;#39;}, \theta \mid \mathbf{y}) = f(u_{i&amp;#39;}, v_{i&amp;#39;} \mid \theta, \mathbf{y}) f(\theta \mid \mathbf{y})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we can do this by going through our posterior samples &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(k)}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(k = 1,\ldots,K\)&lt;/span&gt; and each time simulating&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left( \begin{array}{c}  u_{i&amp;#39;}^{(k)} \\ v_{i&amp;#39;}^{(k)} \\ \end{array} \right) \sim N\left(\left( \begin{array}{c}  0 \\ 0 \\ \end{array} \right), \left( \begin{array}{c c} (\sigma^{(k)}_u)^2 &amp;amp; \rho \sigma^{(k)}_u\sigma^{(k)}_v \\ \rho \sigma^{(k)}_u \sigma^{(k)}_v &amp;amp; (\sigma^{(k)}_v)^2\end{array}\right)\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The way I expected this to be done is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(fit_hurdle, newdata = data.frame(id = NA, time = &amp;quot;2&amp;quot;), 
       allow_new_levels = TRUE,
       sample_new_levels = &amp;quot;gaussian&amp;quot;,
       robust = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate Est.Error         Q2.5    Q97.5
## [1,] 0.9095811  1.315207 0.0009475099 58.14658&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and this is indeed what’s happening, as can be seen by going through the steps manually&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ps &amp;lt;- posterior_samples(fit_hurdle)

sigma_error &amp;lt;- ps[,&amp;quot;sigma&amp;quot;]
sigma_u &amp;lt;- ps[,&amp;quot;sd_id__hu_Intercept&amp;quot;]
sigma_v &amp;lt;- ps[,&amp;quot;sd_id__Intercept&amp;quot;]
rho &amp;lt;- ps[,&amp;quot;cor_id__Intercept__hu_Intercept&amp;quot;]

n_mcmc &amp;lt;- length(rho)

x &amp;lt;- data.frame(id = NA, time = &amp;quot;2&amp;quot;)

### simulate u_i&amp;#39; and v_i&amp;#39; 
u &amp;lt;- rnorm(n_mcmc, sd = sigma_u)
### include correlation 
v &amp;lt;- rnorm(n_mcmc, 
           mean = u * sigma_v / sigma_u * rho,
           sd = sqrt((1 - rho^2) * sigma_v ^ 2))


### extract draws from xi = x*gamma
xi &amp;lt;- qlogis(fitted(fit_hurdle, 
                    newdata = x, 
                    re_formula = NA, 
                    dpar = &amp;quot;hu&amp;quot;,
                    summary = FALSE))

### extract draws from eta = x*beta
eta &amp;lt;- fitted(fit_hurdle, 
              newdata = x, 
              re_formula = NA, 
              dpar = &amp;quot;mu&amp;quot;,
              summary = FALSE)

ey &amp;lt;- (1 - plogis(xi + u)) * exp(eta + v + sigma_error ^ 2 / 2)

round(quantile(ey, probs = c(0.025, 0.5, 0.975)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   2.5%    50%  97.5% 
##  0.001  0.927 52.369&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;unconditional-expectation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unconditional expectation&lt;/h3&gt;
&lt;p&gt;Instead of a patient-specific expectation, conditional on random effects, we might be more interested in targeting an overall expectation (for a new patient) where we integrate out the random effects. In other words, we want to take samples from&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} g(\theta; x) &amp;amp; = E(Y \mid  \theta; x) \\
&amp;amp; = \int   E(Y \mid u_{i&amp;#39;}, v_{i&amp;#39;}, \theta; x) f(u_{i&amp;#39;}, v_{i&amp;#39;} \mid \theta, \mathbf{y}) du_{i&amp;#39;}dv_{i&amp;#39;} \\ 
&amp;amp;\approx  L^{-1}\sum_{l = 1}^{L}E(Y \mid u_{i&amp;#39;}^{(l)}, v_{i&amp;#39;}^{(l)}, \theta; x)\\
&amp;amp;=  L^{-1}\sum_{l = 1}^{L}\left\lbrace 1 - \text{logit}^{-1} ( x^T \gamma + u_{i&amp;#39;}^{(l)}) \right\rbrace \exp(x^T\beta + v_{i&amp;#39;}^{(l)} + \frac{\sigma^2}{2}).\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;That is, I take &lt;span class=&#34;math inline&#34;&gt;\(g(\theta^{(k)}; x)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(k = 1,\ldots,K\)&lt;/span&gt;. At each &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(l= 1 \ldots,L\)&lt;/span&gt;, I take indpendent draws&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left( \begin{array}{c}  u_{i&amp;#39;}^{(l)} \\ v_{i&amp;#39;}^{(l)} \\ \end{array} \right) \sim N\left(\left( \begin{array}{c}  0 \\ 0 \\ \end{array} \right), \left( \begin{array}{c c} (\sigma^{(k)}_u)^2 &amp;amp; \rho \sigma^{(k)}_u\sigma^{(k)}_v \\ \rho \sigma^{(k)}_u \sigma^{(k)}_v &amp;amp; (\sigma^{(k)}_v)^2\end{array}\right)\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to perform the inner Monte Carlo integration (probably not the best method for this 2-d example). I don’t think it’s possible to do this with &lt;code&gt;brms&lt;/code&gt; so I’ve written my own code (which only works for this specific model).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;dominicmagirr/hurlong&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hurlong::marg_mean_q(newdata = x, 
                     nsims = 1000, 
                     fit = fit_hurdle)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   id time     2.5%      50%    97.5%
## 1 NA    2 3.845286 7.511835 19.58231&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal hurdle models</title>
      <link>/post/longitudinal-hurdle-models/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/longitudinal-hurdle-models/</guid>
      <description>


&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;Recently, I have been modelling data that is longitudinal, contains excess zeros, and where the non-zero data is right-skewed and measured on a continuous scale, rather than being count data.&lt;/p&gt;
&lt;p&gt;I’ll simulate a semi-realistic example data set from a lognormal hurdle model. The “random effects” for the pr(zero) and non-zero parts of the model are negatively correlated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(180)

## 100 patients
id &amp;lt;- 1:100

## 2 timepoints
time &amp;lt;- c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;)

## random effects
u &amp;lt;- rnorm(100, sd = 2)
v &amp;lt;- rnorm(100, mean = -0.95 * u, sd = sqrt((1 - 0.95^2) * 4)) # p(zero) is negatively correlated with Y*

## non-zero data (4 obs per id, at two timepoints)
ystar1 &amp;lt;- exp(rnorm(400, mean = u, sd = 1))
ystar2 &amp;lt;- exp(rnorm(400, mean = -0.5 + u, sd = 1))

## z = 1 if &amp;quot;cross hurdle&amp;quot;, i.e. if not zero
z1 &amp;lt;- rbinom(400, size = 1, prob = 1 - plogis(-2 + v)) # p(cross hurdle) = 1 - p(zero)
z2 &amp;lt;- rbinom(400, size = 1, prob = 1 - plogis(-1 + v))

dat &amp;lt;- data.frame(y = c(z1 * ystar1, z2 * ystar2),
                  time = rep(time, each = 400),
                  id = rep(id, 8))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this data set there are 100 patients and two timepoints. For each patient, at each timepoint, I have simulated 4 independent observations (I’ve only done this to make model convergence a bit easier). The important point is that the data is correlated within patient, and also z (hurdle part) and ystar (non-zero part) are correlated, so that patients who start with a smaller (non-zero) y at the first timepoint are more likely to have y = 0 at the second timepoint. This can be clearly seen in the plot below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
ggplot(data = dat,
                mapping = aes(x = time, y = y, group = id)) +
  geom_point() +
  geom_line() +
  scale_y_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-16-longitudinal-hurdle-models_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit the model&lt;/h2&gt;
&lt;p&gt;To fit the model I’m using the excellent &lt;code&gt;brms&lt;/code&gt; package (&lt;a href=&#34;https://github.com/paul-buerkner/brms&#34; class=&#34;uri&#34;&gt;https://github.com/paul-buerkner/brms&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Bürkner P. C. (2018). Advanced Bayesian Multilevel Modeling with the R Package brms. The R Journal. 10(1), 395-411. doi.org/10.32614/RJ-2018-017&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)

fit_hurdle &amp;lt;- brm(bf(y ~  time + (1 | q | id),
                     hu ~ time + (1 | q | id)),
                  data = dat,
                  iter = 4000,
                  family = hurdle_lognormal(),
                  refresh = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit_hurdle)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: hurdle_lognormal 
##   Links: mu = identity; sigma = identity; hu = logit 
## Formula: y ~ time + (1 | q | id) 
##          hu ~ time + (1 | q | id)
##    Data: dat (Number of observations: 800) 
## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;
##          total post-warmup samples = 8000
## 
## Group-Level Effects: 
## ~id (Number of levels: 100) 
##                             Estimate Est.Error l-95% CI u-95% CI Rhat
## sd(Intercept)                   1.96      0.16     1.69     2.29 1.00
## sd(hu_Intercept)                2.47      0.30     1.93     3.11 1.00
## cor(Intercept,hu_Intercept)    -0.90      0.04    -0.97    -0.81 1.00
##                             Bulk_ESS Tail_ESS
## sd(Intercept)                   1452     2688
## sd(hu_Intercept)                2979     5484
## cor(Intercept,hu_Intercept)     2539     4394
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept        0.07      0.20    -0.34     0.45 1.00      885     1970
## hu_Intercept    -2.72      0.34    -3.43    -2.07 1.00     1662     3089
## time2           -0.33      0.08    -0.49    -0.18 1.00    14798     5985
## hu_time2         1.26      0.24     0.80     1.73 1.00    12452     5627
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.94      0.03     0.88     1.00 1.00    11293     6175
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inference&lt;/h2&gt;
&lt;p&gt;From the output I can see that I have more-or-less recovered the parameters from my model. In practice, I could use this to make inference on the two parts of the model separately. In &lt;a href=&#34;/post/longitudinal-hurdle-models-2&#34;&gt;future&lt;/a&gt; posts I’ll discuss how to make inference/predictions when combining the two parts of the model.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multi-arm multi-stage designs for early phase oncology trials</title>
      <link>/talk/example/</link>
      <pubDate>Mon, 03 Jun 2019 11:00:00 +0000</pubDate>
      <guid>/talk/example/</guid>
      <description></description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/about/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m a medical statistician. Quite simply, I&amp;rsquo;m interested in clinical trial design, analysis and interpretation.&lt;/p&gt;

&lt;p&gt;When I was around 16 or 17, I planned to study medicine. However, after doing some work experience and realizing how difficult that path would be, I decided to do a maths degree. Towards the end of my studies, I was fortunate to see a careers talk on Medical Statistics that led me to Lancaster University to do their MSc course &amp;ndash; a fantastic year, and I carried on to do a PhD. My research was on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;group sequential trials&lt;/li&gt;
&lt;li&gt;multiple hypothesis testing&lt;/li&gt;
&lt;li&gt;adaptive designs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://scholar.google.co.uk/citations?user=obri2xMAAAAJ&amp;hl=en&#34;&gt;Google scholar&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I stayed in Lancaster a little longer, and also had a wonderful year postdoccing at the Medical University of Vienna. This time allowed me to publish some more papers, teach, and start my statistical consultancy career.&lt;/p&gt;

&lt;p&gt;I came back to the UK in 2015 to work at AstraZeneca. For two years, I worked in the early-phase oncology group, as part of clinical teams, contributing to the design of trials, and overseeing the statistical analyses conducted by CRO partners. For a further year, I worked as an internal statistical consultant as part of AstraZeneca&amp;rsquo;s Statistical Innovation team. This was an intense, but rewarding, experience that allowed me to work on trials in many different therapy areas. A highlight was working on &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8186&#34;&gt;new methods for dealing with non-proportional hazards in immuno-oncology&lt;/a&gt;. (Preprint &lt;a href=&#34;https://arxiv.org/pdf/1807.11097.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Since January 2019 I have been working at a start-up company &lt;a href=&#34;https://www.ccg.ai&#34; target=&#34;_blank&#34;&gt;Cambridge Cancer Genomics&lt;/a&gt;, where I have become very interested in statistical modelling of longitudinal circulating-tumour DNA measurements.&lt;/p&gt;

&lt;p&gt;I built this website with the &lt;a href=&#34;https://github.com/rstudio/blogdown&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;blogdown&lt;/strong&gt;&lt;/a&gt; package.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to calculate the log-rank statistic</title>
      <link>/post/log-rank-test/</link>
      <pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/post/log-rank-test/</guid>
      <description>


&lt;p&gt;Suppose we have the following data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- dplyr::tibble(patient_id = as.character(1:12), 
                    treatment = rep(c(&amp;quot;C&amp;quot;, &amp;quot;E&amp;quot;), each = 6),
                    survival_time = survival::Surv(time = c(2,6,8,11,17,24,7,9,13,22,23,25),
                                                   event = c(1,1,1,1,1,0,1,1,1,0,0,0)))

knitr::kable(df)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;patient_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;treatment&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;survival_time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;24+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25+&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let’s arrange the data in increasing order of survival time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_ordered &amp;lt;- dplyr::arrange(df, survival_time[,&amp;quot;time&amp;quot;])
knitr::kable(df_ordered)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;patient_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;treatment&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;survival_time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;24+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25+&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;method-1-scores&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Method 1: scores&lt;/h1&gt;
&lt;p&gt;The first step is to estimate the survival probability from the pooled data using the Nelson-Aalen estimator. Then, to get the log-rank scores, we add 1 to the logarithm of the pooled survival estimate for all observed events. For censored events we do not add 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pooled_fit &amp;lt;- survival::survfit(survival_time ~ 1, data = df_ordered)
df_logrank &amp;lt;- dplyr::mutate(df_ordered, 
                            pooled_s = exp(-cumsum(pooled_fit$n.event / pooled_fit$n.risk)),
                            logrank_score = log(pooled_s) + survival_time[,&amp;quot;status&amp;quot;])

knitr::kable(df_logrank, digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;patient_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;treatment&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;survival_time&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;pooled_s&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;logrank_score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.920&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.917&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.840&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.826&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.760&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.726&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.680&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.615&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.600&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.490&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.520&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.347&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.440&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.180&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.361&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22+&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.361&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23+&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.361&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;24+&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.361&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25+&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.361&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.020&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To calculate the logrank score statistic we add up all the scores on the control arm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logrank_score &amp;lt;- dplyr::summarise(dplyr::group_by(df_logrank, treatment), sum(logrank_score))
logrank_score&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   treatment `sum(logrank_score)`
##   &amp;lt;chr&amp;gt;                    &amp;lt;dbl&amp;gt;
## 1 C                         1.66
## 2 E                        -1.66&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;method-2-observed---expected&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Method 2: observed - expected&lt;/h1&gt;
&lt;p&gt;The score statistic is equivalent to the sum of “observed” - “expected” events at each event time. This is implemented in the survival package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logrank_fit &amp;lt;- survival::survdiff(survival_time ~ treatment, data = df)
rbind(logrank_fit$n, logrank_fit$obs - logrank_fit$exp)[2,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## treatment=C treatment=E 
##    1.664105   -1.664105&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
   One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generalizing boundaries for triangular designs, and efficacy estimation at extended follow-ups</title>
      <link>/talk/group-sequential/</link>
      <pubDate>Wed, 26 Sep 2018 12:00:00 +0000</pubDate>
      <guid>/talk/group-sequential/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
