<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>flexsurv 2 - Dominic Magirr</title>
<meta property="og:title" content="flexsurv 2 - Dominic Magirr">


  <link href='/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/dominicmagirr">GitHub</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">5 min read</span>
    

    <h1 class="article-title">flexsurv 2</h1>

    
    <span class="article-date">2019-11-01</span>
    

    <div class="article-content">
      


<p>Previously, I started discussing the <code>flexsurv</code> package. I used it to fit a Weibull model. This is implemented as an accelerated failure time model. It is also a proportional hazards model (although, as I found previously, converting between the two is not so straightforward, but it can be done by <code>SurvRegCensCov</code>).</p>
<p>Now let’s compare Weibull regression with Cox regression. Firstly, Weibull regression:</p>
<ul>
<li>assumes proportional hazards;</li>
<li>the number of parameters is equal to <span class="math inline">\(k + 2\)</span>, where <span class="math inline">\(k\)</span> is the number of covariates;</li>
<li>we can estimate things like the median, <span class="math inline">\(P(S&gt;s^*)\)</span>, etc. from the model…</li>
<li>but the model might be too restrictive – we won’t estimate these things very well.</li>
</ul>
<p>Cox regression:</p>
<ul>
<li>assumes proportional hazards;</li>
<li>there are <span class="math inline">\(k\)</span> parameters (one for each covariate);</li>
<li>we don’t estimate the baseline hazard…</li>
<li>which means we don’t get estimates for things like the median, <span class="math inline">\(P(S&gt;s^*)\)</span>, etc.</li>
</ul>
<p><code>flexsurv</code> has a function <code>flexsurvspline</code> which allows one to bridge the gap between Weibull regression and Cox regresssion.</p>
<div id="from-weibull-regression-to-cox-regression." class="section level2">
<h2>From Weibull regression to Cox regression.</h2>
<p>For a Weibull distribution with shape <span class="math inline">\(a\)</span>, scale <span class="math inline">\(b\)</span>, covariates <span class="math inline">\(x\)</span>, and regression coefficents <span class="math inline">\(\gamma\)</span>, the survival probability is</p>
<p><span class="math display">\[S(t; x) = \exp\left\lbrace - \left( \frac{t}{b \cdot \exp(x^T\gamma)} \right) ^ a\right\rbrace\]</span></p>
<p>Since survival is related to the log-cumulative hazard via <span class="math inline">\(S=\exp(-H)\)</span>, this means that</p>
<p><span class="math display">\[\log H(t;x) = a\log t - a\log(b) - a x^T\gamma\]</span></p>
<p>In words, the log-cumulative hazard has a linear relationship with (log-) time, with the intercept depending on the value of <span class="math inline">\(x\)</span>. For a given data set, we can check if this is reasonable by looking at non-paramteric estimates, <span class="math inline">\(\log \hat{H}(t; x)\)</span>.</p>
<pre class="r"><code>library(survival)
library(flexsurv)
library(ggplot2)

### Non-parametric analysis
fit_km &lt;- survfit(Surv(futime, death) ~ trt,
                  data = myeloid)
t_seq &lt;- seq(1, 2500, length.out = 1000)
km_sum &lt;- summary(fit_km, times = t_seq, extend = TRUE)

### Weibull regression
fit_weibull &lt;- flexsurvreg(Surv(futime, death) ~ trt, 
                           dist = &quot;weibull&quot;, 
                           data = myeloid)

a &lt;- fit_weibull$res[&quot;shape&quot;, &quot;est&quot;]
b &lt;- fit_weibull$res[&quot;scale&quot;, &quot;est&quot;]
trtB &lt;- fit_weibull$res[&quot;trtB&quot;, &quot;est&quot;]


### plot log-cumulative hazard against log-time
df &lt;- data.frame(log_time = log(rep(t_seq, 2)),
                 logcumhaz = log(-log(km_sum$surv)),
                 trt = rep(c(&quot;A&quot;, &quot;B&quot;), each = 1000),
                 logcumhaz_w = c(a * (log(t_seq) - log(b)),
                                 a * (log(t_seq) - log(b) - trtB)))

ggplot(data = df,
       mapping = aes(x = log_time,
                     y = logcumhaz,
                     colour = trt)) +
  geom_line() +
  geom_line(mapping = aes(x = log_time,
                          y = logcumhaz_w,
                          colour = trt),
            linetype = 2) +
  theme_bw() +
  scale_x_continuous(limits = c(2,8)) +
  scale_y_continuous(limits = c(-6,0))</code></pre>
<p><img src="/post/2019-11-01-flexsurv-2_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>What’s going on here? Well, the first thing to acknowledge is that the hazards only appear to be proportional after about 150 (<span class="math inline">\(e^5\)</span>) days. I’m not sure I would immediately abandon a proportional-hazards model, though, as most of the events happen when the hazards are proportional (only 10-15% of the events happen before day 150), so the right-hand-side of the plot is far more important. Looking to the right then: the relationship between the log-cumulative hazard and log-time is not really linear. The distance between the two lines is roughly the same for the two models (Weibull and non-parametric), suggesting that the Weibull model does ok at estimating the hazard ratio. However, the lack of linearity will lead to poor estimates for the medians, <span class="math inline">\(P(S&gt;s^*)\)</span>, etc., as can be confirmed by plotting the survival curves:</p>
<pre class="r"><code>ggplot(data = df,
       mapping = aes(x = exp(log_time),
                     y = exp(-exp(logcumhaz)),
                     colour = trt)) +
  geom_line() +
  geom_line(mapping = aes(x = exp(log_time),
                          y = exp(-exp(logcumhaz_w)),
                          colour = trt),
            linetype = 2) +
  theme_bw()</code></pre>
<p><img src="/post/2019-11-01-flexsurv-2_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>To improve the model, given this lack of linearity, it seems quite natural to change from</p>
<p><span class="math display">\[\log H(t;x) = a\log t - a\log(b) - a x^T\gamma\]</span></p>
<p>to</p>
<p><span class="math display">\[\log H(t;x) = s(\log t) + x^T\beta\]</span></p>
<p>where <span class="math inline">\(s(\log t)\)</span> is a natural cubic spline function of (log) time. One can make the model more/less flexible by choosing a large/small number of knots. By default, the knots are placed at quantiles of the uncensored event times. How many knots are required? I don’t really have a good answer for this: one or two. At most, three? In this example, I’m using two inner knots, placed at 33% and 66% of the uncensored event times (indicated by vertical lines):</p>
<pre class="r"><code>fit_spline_2 &lt;- flexsurvspline(Surv(futime, death) ~ trt, 
                               data = myeloid,
                               k = 2,
                               scale = &quot;hazard&quot;)

spline_2_sum &lt;- summary(fit_spline_2, t = t_seq, type = &quot;cumhaz&quot;)

df2 &lt;- cbind(df,
      data.frame(logcumhaz_s2 = log(c(spline_2_sum$`trt=A`[&quot;est&quot;]$est,
                                  spline_2_sum$`trt=B`[&quot;est&quot;]$est))))

ggplot(data = df2,
       mapping = aes(x = log_time,
                     y = logcumhaz,
                     colour = trt)) +
  geom_line() +
  geom_line(mapping = aes(x = log_time,
                          y = logcumhaz_s2,
                          colour = trt),
            linetype = 2) +
  theme_bw() +
  scale_x_continuous(limits = c(2,8)) +
  scale_y_continuous(limits = c(-6,0)) +
  geom_vline(xintercept = fit_spline_2$knots)</code></pre>
<p><img src="/post/2019-11-01-flexsurv-2_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>This looks a lot better, and we can see the improvement in the survival curves:</p>
<pre class="r"><code>ggplot(data = df2,
       mapping = aes(x = exp(log_time),
                     y = exp(-exp(logcumhaz)),
                     colour = trt)) +
  geom_line() +
  geom_line(mapping = aes(x = exp(log_time),
                          y = exp(-exp(logcumhaz_s2)),
                          colour = trt),
            linetype = 2) +
  theme_bw()</code></pre>
<p><img src="/post/2019-11-01-flexsurv-2_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="from-cox-regression-to-weibull-regression." class="section level2">
<h2>From Cox regression to Weibull regression.</h2>
<p>If we start out from Cox regression</p>
<p><span class="math display">\[h(t;x)=h_0(t)\exp(x^T\beta)\]</span></p>
<p>this means that</p>
<p><span class="math display">\[\log H(t;x) = \log H_0(t;x) + x^T\beta\]</span></p>
<p>We estimate the parameters <span class="math inline">\(\beta\)</span> from the partial likelihood, and don’t estimate <span class="math inline">\(\log H_0(t;x)\)</span>. So <span class="math inline">\(\log H_0(t;x)\)</span> can be anything. However, with the <code>flexsurvspline</code> function, as long as we use enough knots, <span class="math inline">\(s(\log(t))\)</span> can be more-or-less anything (smooth), so the two methods will give the same information about <span class="math inline">\(\beta\)</span>.</p>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>I can’t really see any reason not to switch from a Cox model to <code>flexsurvspline</code>. You don’t lose anything in terms of inference on <span class="math inline">\(\beta\)</span>, only gain a nice estimate for the baseline hazard. Also, inference is all based on maximum likelihood. No special theory required.</p>
<p>From the other side, if you start out from Weibull regression, and then realise that Weibull is the wrong model, you don’t have to think too hard about how to choose a better model, you <em>know</em> that <code>flexsurvspline</code> will be good (assuming proportional hazards is correct: for non-proportional hazards you may have to think harder).</p>
<p>What can go wrong? In small sample sizes, I guess there could be issues with over-fitting if too many knots are chosen. But given a decent sample size, I can’t see any problems. I would be interested to see a <span class="math inline">\(\log H_0(t;x)\)</span> that is highly wiggly – doesn’t seem likely in practice.</p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

